{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from jax import nn, random, lax, vmap, ops\n",
    "from scipy.special import binom\n",
    "\n",
    "from time import time_ns\n",
    "\n",
    "from opt_einsum import contract, contract_expression\n",
    "\n",
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "sys.path.append(cwd[:-len('befit/examples/temp_rev_learn')])\n",
    "\n",
    "def einsum(equation, *args, optimize=True):\n",
    "    return contract(equation, *args, backend='jax', optimize=optimize)\n",
    "\n",
    "sns.set(context='talk', style='white', color_codes=True, font_scale=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from scipy import io\n",
    "# load experiment and define generator for observations\n",
    "data = io.loadmat('main/states_and_rewards.mat')\n",
    "Sirr = data['irregular']['S'][0, 0][:, 0] - 1\n",
    "Oirr = data['irregular']['R'][0, 0]\n",
    "Sreg = data['regular']['S'][0, 0][:, 0] - 1\n",
    "Oreg = data['regular']['R'][0, 0]\n",
    "\n",
    "runs = 1\n",
    "\n",
    "outcomes1 = jnp.concatenate([(Oreg[:, None].repeat(runs, -2) + 1)//2, \n",
    "                            (Oirr[:, None].repeat(runs, -2) + 1)//2], -2)\n",
    "outcomes2 = jnp.concatenate([Sreg[:, None].repeat(runs, -1) + 2, \n",
    "                             Sirr[:, None].repeat(runs, -1) + 2], -1)[..., None]\n",
    "\n",
    "outcomes = jnp.concatenate([outcomes1, outcomes2], -1)\n",
    "\n",
    "subs = jnp.array(range(2 * runs))\n",
    "def process(t, responses):\n",
    "    return outcomes[t, subs, responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define negative binomial distribution in a discrete phase-type representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdiag = vmap(lambda v: jnp.diag(v, k=0))\n",
    "voffd = vmap(lambda v: jnp.diag(v, k=1))\n",
    "\n",
    "# max possible precision\n",
    "n = 10\n",
    "nu = jnp.arange(1, n + 1)\n",
    "\n",
    "# discretisation of mean values\n",
    "mu = jnp.arange(4., 45., 1)\n",
    "p = mu[..., None]/(nu[None] + mu[..., None])\n",
    "\n",
    "# return probabilities\n",
    "j = jnp.tril(jnp.arange(1, n + 1)[None].repeat(n, 0))\n",
    "bnm = binom(nu[..., None], jnp.tril(j-1))\n",
    "p1 = p[..., None] ** jnp.tril((nu[..., None] - j + 1))\n",
    "p2 = (1 - p[..., None])**jnp.tril(j - 1)\n",
    "pi =  jnp.tril(bnm * p1 * p2)\n",
    "pi = jnp.concatenate([pi, 1 - pi.sum(-1, keepdims=True)], -1)\n",
    "\n",
    "# phase transition matrix for different models m, f_t| f_{t+1}\n",
    "p_ff = []\n",
    "for i in range(n):\n",
    "    vp = p[..., i:i + 1].repeat(i + 1, -1)\n",
    "    tmp = jnp.concatenate([vdiag(vp) + voffd(1 - vp[..., :-1]), jnp.zeros((p.shape[0], i + 1, n - i - 1))], -1)\n",
    "    tmp = jnp.concatenate([tmp, 1 - tmp.sum(-1, keepdims=True)], -1)\n",
    "    tmp = jnp.concatenate([tmp, jnp.ones((p.shape[0], n - i - 1, n + 1))/(n + 1)], -2)\n",
    "    tmp = jnp.concatenate([tmp, pi[:, i:i+1]], -2)\n",
    "    p_ff.append(tmp)\n",
    "\n",
    "p_mff = jnp.stack(p_ff, -3).reshape(-1, n + 1, n + 1)\n",
    "\n",
    "def nb_dist(mu, nu, d_max):\n",
    "    d = jnp.expand_dims(jnp.arange(1., d_max + 1.), -1)[..., None]\n",
    "    k = d - 1.\n",
    "    p = mu / (nu + mu)\n",
    "    \n",
    "    return binom(k + nu - 1, k) * (1 - p)**nu * p ** k\n",
    "\n",
    "d_max = 50\n",
    "dist = nb_dist(mu[:, None], nu[None], d_max)\n",
    "\n",
    "# state transition matrix f_t, j_t| j_{t+1}\n",
    "p_fjj = jnp.zeros((n+1, 2, 2))\n",
    "p_fjj = ops.index_add(p_fjj, ops.index[-1, :, 1], 1.)\n",
    "p_fjj = ops.index_add(p_fjj, ops.index[:-1, :, 0], 1.)\n",
    "\n",
    "# state transition matrix j_t, c_t | c_{t+1}\n",
    "p_jcc = jnp.stack([jnp.eye(2), (jnp.ones((2, 2)) - jnp.eye(2))], 0)\n",
    "p_fcc = einsum('jcz,fj->fcz', p_jcc, p_fjj[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import digamma\n",
    "vdiag = vmap(lambda v: jnp.diag(v, k=0))\n",
    "voffd = vmap(lambda v: jnp.diag(v, k=1))\n",
    "def log(x, minimum=-1e10):\n",
    "    return jnp.where(x > 0, jnp.log(x), minimum) \n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, N, nu_max=1, nu_min=0):\n",
    "        self.N = N\n",
    "        self.nu_max = nu_max\n",
    "        self.nu_min = nu_min\n",
    "        \n",
    "        self.__make_transition_matrices()\n",
    "        self.__set_prior()\n",
    "        \n",
    "    def __make_transition_matrices(self):\n",
    "        nu = jnp.arange(1, self.nu_max + 1)\n",
    "        mu = jnp.arange(4., 45., 1.)\n",
    "        p = mu[..., None]/(nu[None] + mu[..., None])\n",
    "        \n",
    "        j = jnp.tril(jnp.arange(1, self.nu_max + 1)[None].repeat(self.nu_max, 0))\n",
    "        bnm = binom(nu[..., None], jnp.tril(j-1))\n",
    "        p1 = p[..., None] ** jnp.tril((nu[..., None] - j + 1))\n",
    "        p2 = (1 - p[..., None])**jnp.tril(j - 1)\n",
    "        pi =  jnp.tril(bnm * p1 * p2)\n",
    "        pi = jnp.concatenate([pi, 1 - pi.sum(-1, keepdims=True)], -1)\n",
    "        \n",
    "        # phase transition matrix for different models m, f_t| f_{t+1}\n",
    "        p_ff = []\n",
    "        for i in range(self.nu_max):\n",
    "            vp = p[..., i:i + 1].repeat(i + 1, -1)\n",
    "            tmp = jnp.concatenate([vdiag(vp) + voffd(1 - vp[..., :-1]), jnp.zeros((p.shape[0], i + 1, self.nu_max - i - 1))], -1)\n",
    "            tmp = jnp.concatenate([tmp, 1 - tmp.sum(-1, keepdims=True)], -1)\n",
    "            tmp = jnp.concatenate([tmp, jnp.ones((p.shape[0], self.nu_max - i - 1, self.nu_max + 1))/(self.nu_max + 1)], -2)\n",
    "            tmp = jnp.concatenate([tmp, pi[:, i:i+1]], -2)\n",
    "            p_ff.append(tmp)\n",
    "        \n",
    "        self.p_mff = jnp.stack(p_ff, -3).reshape(-1, self.nu_max + 1, self.nu_max + 1)\n",
    "        \n",
    "        # state transition matrix f_t, j_t| j_{t+1}\n",
    "        p_fjj = jnp.zeros((self.nu_max + 1, 2, 2))\n",
    "        p_fjj = ops.index_add(p_fjj, ops.index[-1, :, 1], 1.)\n",
    "        p_fjj = ops.index_add(p_fjj, ops.index[:-1, :, 0], 1.)\n",
    "        \n",
    "        p_jcc = jnp.stack([jnp.eye(2), (jnp.ones((2, 2)) - jnp.eye(2))], 0)\n",
    "\n",
    "        self.p_fcc = einsum('jcz,fj->fcz', p_jcc, p_fjj[:, 0])\n",
    "        \n",
    "    def __set_prior(self, a=6., b=32.):\n",
    "        prior_mf = self.p_mff[:, -1]\n",
    "        M = prior_mf.shape[0]\n",
    "        prior_m = jnp.ones(M)/M\n",
    "        prior_m = prior_m.reshape(self.nu_max, -1)\n",
    "        prior_m = jnp.concatenate([jnp.zeros_like(prior_m[:self.nu_min]), prior_m[self.nu_min:]], 0).reshape(-1)\n",
    "        prior_m /= prior_m.sum()\n",
    "        prior_fm = (prior_mf * prior_m[:, None]).T\n",
    "        prior_c = jnp.ones(2)/2\n",
    "\n",
    "        pars = jnp.array([\n",
    "            [[a, b, 1, 1], [b, a, 1, 1]],\n",
    "            [[b, a, 1, 1], [a, b, 1, 1]],\n",
    "            [[1, 1, 1000., 1], [1, 1, 1, 1000.]]\n",
    "        ])[None].repeat(self.N, 0)\n",
    "        \n",
    "        probs = einsum('c,fm->cfm', prior_c, prior_fm)[None].repeat(self.N, 0)\n",
    "        self.prior = (probs, pars)\n",
    "        \n",
    "    \n",
    "    def __par_efe(self, p_c, params, U):\n",
    "        p_aco = params/params.sum(-1, keepdims=True)\n",
    "        q_ao = einsum('...aco,...c->...ao', p_aco, p_c)\n",
    "    \n",
    "        KL_a =  - jnp.sum(q_ao * U, -1) + jnp.sum(q_ao * log(q_ao), -1)\n",
    "    \n",
    "        H_ac = - (p_aco * digamma(params)).sum(-1) + digamma(params.sum(-1) + 1)\n",
    "        H_a = einsum('...c,...ac->...a', p_c, H_ac)\n",
    "    \n",
    "        return KL_a + H_a\n",
    "    \n",
    "    def logits(self, beliefs, gamma, U):\n",
    "        p_cfm, params = beliefs\n",
    "\n",
    "        # expected surprisal based action selection\n",
    "        p_c = einsum('...cfm->...c', p_cfm, optimize=False)\n",
    "\n",
    "        S_a = self.__par_efe(p_c, params, U)\n",
    "\n",
    "        return - gamma * ( S_a - S_a.min(-1, keepdims=True))\n",
    "\n",
    "    def action_selection(self, rng_key, beliefs, gamma=1e3, U=jnp.array([-1., 1., 0., 0.])):\n",
    "        # sample choices\n",
    "        return random.categorical(rng_key, logits(self, beliefs, gamma, U))\n",
    "\n",
    "    # parametric learning and inference\n",
    "    def learning(self, observations, responses, mask, prior):\n",
    "        p_cfm, params = prior\n",
    "        obs = jnp.eye(4)[observations]\n",
    "\n",
    "        p_aco = params/params.sum(-1, keepdims=True)\n",
    "\n",
    "        p_c = einsum('nco,no->nc', p_aco[jnp.arange(self.N), responses], obs)\n",
    "        \n",
    "        m = jnp.expand_dims(mask, -1)\n",
    "        p_c = m * p_c + (1 - m)/2.\n",
    "\n",
    "        post = einsum('nc,ncfm->ncfm', p_c, p_cfm)\n",
    "        \n",
    "        norm = post.reshape(post.shape[:-3] + (-1,)).sum(-1)[..., None, None, None]\n",
    "        post = post/norm\n",
    "\n",
    "        resp = jnp.eye(3)[responses]\n",
    "        post_c = post.reshape(post.shape[:-2] + (-1,)).sum(-1)\n",
    "\n",
    "        params_new = params + einsum('na,nc,no,n->naco', resp, post_c, obs, mask)\n",
    "        pred = einsum('fcz,mfg,ncfm->nzgm', self.p_fcc, self.p_mff, post)\n",
    "\n",
    "        return (pred, params_new)\n",
    "    \n",
    "# implement simulator for POMDP\n",
    "def estimate_beliefs(outcomes, choices, masks, nu_max=1):\n",
    "    T, N = choices.shape\n",
    "    assert outcomes.shape == (T, N)\n",
    "    assert masks.shape == (T, N)\n",
    "    agent = Agent(N, nu_max=nu_max)\n",
    "    def sim_fn(carry, t):\n",
    "        prior = carry\n",
    "        posterior = agent.learning(outcomes[t], choices[t], masks[t], prior)\n",
    "                \n",
    "        return posterior, {'beliefs': prior}\n",
    "    \n",
    "    _, sequence = lax.scan(sim_fn, agent.prior, jnp.arange(T))\n",
    "    \n",
    "    sequence['beliefs'][0].block_until_ready()\n",
    "    \n",
    "    return sequence, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro as npyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.handlers import mask as npyro_mask\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def model(beliefs, agent, y=None, mask=True):\n",
    "    T, N = beliefs[0].shape[:2]\n",
    "    with npyro.plate('N', N):\n",
    "        gamma = npyro.sample('gamma', dist.Gamma(20., 2.))\n",
    "        lams = npyro.sample('lams', dist.Normal(jnp.array([-1., 1.]), 2*jnp.ones(2)).to_event(1))\n",
    "        with npyro.plate('T', T):\n",
    "            U = jnp.stack([lams[..., 0], lams[..., 1], jnp.zeros_like(lams[..., 1]), jnp.zeros_like(lams[..., 1])], -1)\n",
    "            U -= logsumexp(U)\n",
    "            logits = agent.logits(beliefs, jnp.expand_dims(gamma, -1), jnp.expand_dims(U, -2))\n",
    "            obs = npyro.sample('obs', dist.CategoricalLogits(logits).mask(mask), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro.infer import log_likelihood\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def log_pred_density(model, samples, *args, **kwargs):\n",
    "    log_lk = log_likelihood(model, samples, *args, **kwargs)['obs'].sum(-2)\n",
    "    n = log_lk.shape[0]\n",
    "    _lpd = logsumexp(log_lk, 0) - jnp.log(n)\n",
    "    p_waic = n * log_lk.var(0)/(n - 1)\n",
    "    return {'lpd': _lpd, 'waic': _lpd - p_waic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats import load_data\n",
    "\n",
    "outcomes_data, responses_data, mask_data, ns = load_data()\n",
    "\n",
    "mask_data = jnp.array(mask_data)\n",
    "responses_data = jnp.array(responses_data).astype(jnp.int32)\n",
    "outcomes_data = jnp.array(outcomes_data).astype(jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "agents = {}\n",
    "for nu_max in range(1, 11):\n",
    "    sequences[nu_max], agents[nu_max] = estimate_beliefs(outcomes_data, responses_data, mask_data, nu_max=nu_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [1:01:40<00:00,  1.48s/it, 511 steps of size 8.09e-03. acc. prob=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -24277.12\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.72      2.21     10.62      7.15     14.22   1599.66      1.00\n",
      "  gamma[1]      9.03      1.99      8.91      5.46     11.84   1452.14      1.00\n",
      "  gamma[2]      9.69      2.27      9.45      6.38     13.72   1578.19      1.00\n",
      "  gamma[3]     12.68      2.21     12.66      9.21     16.40   1046.15      1.00\n",
      "  gamma[4]      8.48      1.73      8.39      5.69     11.16    883.57      1.00\n",
      "  gamma[5]      9.21      1.93      9.16      6.14     12.23   1459.91      1.00\n",
      "  gamma[6]      9.93      2.11      9.74      6.65     13.38   1945.40      1.00\n",
      "  gamma[7]      9.94      2.01      9.78      6.48     12.95   1981.24      1.00\n",
      "  gamma[8]     10.20      2.22     10.06      6.76     13.75   1237.09      1.00\n",
      "  gamma[9]     10.17      2.35      9.99      5.99     13.61   1449.67      1.00\n",
      " gamma[10]      8.70      1.89      8.66      5.89     11.84   1424.05      1.00\n",
      " gamma[11]      9.55      2.11      9.39      6.02     12.87   1167.47      1.00\n",
      " gamma[12]      9.53      2.13      9.38      6.04     12.85   1359.16      1.00\n",
      " gamma[13]      9.02      2.12      8.84      5.53     12.21   2077.89      1.00\n",
      " gamma[14]      9.01      2.09      8.85      5.44     12.23    650.56      1.01\n",
      " gamma[15]      6.77      1.45      6.65      4.55      9.19    699.40      1.00\n",
      " gamma[16]      8.82      1.65      8.76      6.13     11.46   1726.90      1.00\n",
      " gamma[17]     11.31      1.08     11.27      9.56     13.09    779.23      1.00\n",
      " gamma[18]     10.33      1.55     10.25      7.66     12.72    577.96      1.00\n",
      " gamma[19]      5.63      0.97      5.57      3.87      7.08    491.28      1.00\n",
      " gamma[20]      9.75      2.20      9.50      6.65     13.51   1372.32      1.00\n",
      " gamma[21]      9.35      2.05      9.21      6.01     12.46   1671.92      1.00\n",
      " gamma[22]      5.05      0.89      5.03      3.66      6.50    637.17      1.00\n",
      " gamma[23]      6.32      1.43      6.16      4.14      8.61    647.69      1.00\n",
      " gamma[24]      7.11      1.53      7.12      4.46      9.41    683.83      1.00\n",
      " gamma[25]      3.88      0.85      3.82      2.48      5.24    516.43      1.00\n",
      " gamma[26]      8.93      2.02      8.76      5.73     12.28    561.89      1.00\n",
      " gamma[27]     11.47      1.63     11.41      8.84     14.09    577.00      1.00\n",
      " gamma[28]      4.36      0.76      4.33      3.20      5.68    431.43      1.00\n",
      " gamma[29]     11.72      1.51     11.63      9.43     14.21    616.60      1.00\n",
      " gamma[30]      8.58      1.47      8.50      6.20     10.89    423.41      1.01\n",
      " gamma[31]      7.02      1.22      6.99      4.96      8.87    818.05      1.00\n",
      " gamma[32]     23.29      2.08     23.23     19.99     26.77    668.58      1.00\n",
      " gamma[33]     10.21      2.18     10.06      6.35     13.43   1497.56      1.00\n",
      " gamma[34]      4.42      0.78      4.40      3.12      5.68    612.38      1.00\n",
      " gamma[35]      5.68      0.99      5.66      4.11      7.40    685.95      1.00\n",
      " gamma[36]     12.06      2.11     12.04      8.82     15.60    545.74      1.00\n",
      " gamma[37]      8.59      1.69      8.45      6.10     11.58    345.81      1.00\n",
      " gamma[38]      5.20      0.81      5.21      3.92      6.59    853.90      1.00\n",
      " gamma[39]      2.98      0.58      2.92      2.10      4.02    550.07      1.00\n",
      " gamma[40]      2.66      0.51      2.64      1.87      3.48    618.03      1.00\n",
      " gamma[41]      8.44      1.60      8.35      6.15     11.33   1437.95      1.00\n",
      " gamma[42]      8.09      1.53      8.01      5.29     10.22    943.03      1.01\n",
      " gamma[43]      8.50      1.72      8.33      5.59     11.07   1587.29      1.00\n",
      " gamma[44]      7.07      1.41      6.95      4.82      9.44   1481.35      1.00\n",
      " gamma[45]      9.83      2.12      9.66      6.68     13.35   1618.35      1.00\n",
      " gamma[46]      5.15      0.99      5.08      3.40      6.67   1078.35      1.00\n",
      " gamma[47]      4.67      0.76      4.63      3.32      5.82   1586.37      1.00\n",
      " gamma[48]      8.25      1.65      8.17      5.45     10.83   1481.73      1.00\n",
      " gamma[49]      7.60      1.63      7.56      5.13     10.31   1388.20      1.00\n",
      " gamma[50]      6.12      1.04      6.07      4.52      7.85   1381.52      1.00\n",
      " gamma[51]      9.05      1.71      8.98      6.34     11.84   1388.34      1.00\n",
      " gamma[52]      7.77      1.30      7.72      5.61      9.83    631.96      1.00\n",
      " gamma[53]      9.83      1.13      9.79      7.76     11.54    513.89      1.00\n",
      " gamma[54]      9.96      1.86      9.84      7.17     13.16    460.36      1.00\n",
      " gamma[55]      5.40      1.05      5.30      3.66      7.01   1319.00      1.00\n",
      " gamma[56]      8.90      1.85      8.75      6.15     12.01   1418.93      1.00\n",
      " gamma[57]      4.99      0.82      4.96      3.70      6.27    561.38      1.00\n",
      " gamma[58]      7.08      1.42      6.98      4.79      9.44    714.34      1.00\n",
      " gamma[59]      7.63      1.35      7.61      5.33      9.56    673.79      1.00\n",
      " gamma[60]      3.67      0.74      3.62      2.50      4.83    397.55      1.00\n",
      " gamma[61]      9.30      1.58      9.32      6.78     11.92    336.97      1.00\n",
      " gamma[62]      6.06      0.89      6.04      4.47      7.39    366.77      1.00\n",
      " gamma[63]      5.31      1.03      5.22      3.59      6.84    555.63      1.00\n",
      " gamma[64]      8.54      1.80      8.41      5.70     11.33   1305.51      1.00\n",
      " gamma[65]      4.15      0.88      4.09      2.58      5.45    681.40      1.00\n",
      " gamma[66]      9.50      1.39      9.51      7.31     11.82    563.25      1.00\n",
      " gamma[67]      4.99      0.72      4.98      3.78      6.13    518.04      1.00\n",
      " gamma[68]      5.42      0.79      5.41      4.05      6.62    662.80      1.00\n",
      " gamma[69]      9.10      1.59      9.03      6.56     11.64    476.03      1.00\n",
      " gamma[70]      6.24      1.24      6.20      4.19      8.16    707.75      1.00\n",
      " gamma[71]      3.32      0.63      3.29      2.17      4.18    636.75      1.00\n",
      " gamma[72]      7.91      1.17      7.90      5.81      9.67    475.87      1.00\n",
      " gamma[73]      4.99      0.86      4.93      3.65      6.38    554.54      1.00\n",
      " lams[0,0]      1.59      0.69      1.45      0.60      2.63    505.24      1.00\n",
      " lams[0,1]      2.09      0.70      1.98      1.08      3.18    502.97      1.00\n",
      " lams[1,0]      1.57      0.69      1.43      0.62      2.58    551.19      1.00\n",
      " lams[1,1]      2.25      0.69      2.14      1.19      3.24    543.04      1.00\n",
      " lams[2,0]      1.51      0.77      1.36      0.36      2.53    462.65      1.00\n",
      " lams[2,1]      2.42      0.78      2.30      1.24      3.49    432.99      1.00\n",
      " lams[3,0]      8.24      1.16      8.28      6.52     10.28    842.07      1.00\n",
      " lams[3,1]      8.42      1.12      8.45      6.76     10.43    868.54      1.00\n",
      " lams[4,0]      1.66      0.62      1.51      0.74      2.56    538.76      1.00\n",
      " lams[4,1]      1.98      0.63      1.86      1.02      2.95    544.29      1.00\n",
      " lams[5,0]      1.70      0.74      1.51      0.73      2.83    314.85      1.00\n",
      " lams[5,1]      2.19      0.74      2.04      1.11      3.23    312.05      1.00\n",
      " lams[6,0]      1.70      0.74      1.53      0.74      2.77    467.23      1.00\n",
      " lams[6,1]      2.09      0.74      1.93      1.11      3.18    471.40      1.00\n",
      " lams[7,0]      1.58      0.70      1.45      0.56      2.58    442.85      1.00\n",
      " lams[7,1]      2.18      0.70      2.05      1.13      3.24    433.76      1.00\n",
      " lams[8,0]      1.51      0.74      1.35      0.52      2.57    503.20      1.00\n",
      " lams[8,1]      2.28      0.75      2.18      1.17      3.36    488.47      1.00\n",
      " lams[9,0]      1.55      0.79      1.37      0.49      2.73    484.22      1.00\n",
      " lams[9,1]      2.32      0.79      2.15      1.20      3.49    472.07      1.00\n",
      "lams[10,0]      1.44      0.63      1.29      0.62      2.36    411.80      1.00\n",
      "lams[10,1]      2.15      0.65      2.03      1.19      3.08    427.38      1.00\n",
      "lams[11,0]      1.53      0.65      1.37      0.67      2.49    311.26      1.00\n",
      "lams[11,1]      2.05      0.67      1.91      1.07      3.05    303.57      1.00\n",
      "lams[12,0]      1.49      0.61      1.35      0.66      2.40    469.52      1.00\n",
      "lams[12,1]      2.02      0.63      1.89      1.17      3.03    477.32      1.00\n",
      "lams[13,0]      1.22      0.72      1.04      0.28      2.23    390.93      1.00\n",
      "lams[13,1]      2.45      0.74      2.32      1.43      3.63    416.19      1.00\n",
      "lams[14,0]      0.73      0.18      0.69      0.46      0.97    390.44      1.00\n",
      "lams[14,1]      1.47      0.30      1.41      1.04      1.95    330.10      1.01\n",
      "lams[15,0]      0.90      0.21      0.86      0.57      1.19    503.24      1.00\n",
      "lams[15,1]      1.71      0.33      1.66      1.21      2.26    478.53      1.00\n",
      "lams[16,0]      1.62      0.62      1.48      0.81      2.61    386.59      1.00\n",
      "lams[16,1]      1.97      0.64      1.83      1.06      2.93    384.64      1.00\n",
      "lams[17,0]      0.01      0.04      0.02     -0.06      0.08    701.65      1.00\n",
      "lams[17,1]      0.58      0.03      0.57      0.53      0.64    694.02      1.00\n",
      "lams[18,0]      0.29      0.03      0.29      0.23      0.34    852.52      1.00\n",
      "lams[18,1]      0.77      0.06      0.76      0.67      0.87    501.69      1.00\n",
      "lams[19,0]     -0.19      0.13     -0.17     -0.39      0.02    402.93      1.00\n",
      "lams[19,1]      1.17      0.16      1.15      0.92      1.41    414.54      1.00\n",
      "lams[20,0]      1.46      0.68      1.31      0.55      2.46    475.99      1.00\n",
      "lams[20,1]      2.20      0.70      2.06      1.11      3.17    464.30      1.00\n",
      "lams[21,0]      1.46      0.60      1.33      0.62      2.26    346.43      1.00\n",
      "lams[21,1]      1.91      0.62      1.80      1.00      2.77    363.15      1.00\n",
      "lams[22,0]      0.25      0.05      0.26      0.18      0.33    947.98      1.00\n",
      "lams[22,1]      1.01      0.14      0.99      0.80      1.20    436.78      1.00\n",
      "lams[23,0]      0.79      0.13      0.78      0.59      0.99    452.46      1.00\n",
      "lams[23,1]      1.42      0.25      1.39      1.00      1.81    451.00      1.00\n",
      "lams[24,0]      0.87      0.22      0.83      0.57      1.18    282.74      1.00\n",
      "lams[24,1]      1.66      0.34      1.62      1.14      2.19    236.25      1.00\n",
      "lams[25,0]      0.25      0.07      0.25      0.15      0.37    757.22      1.00\n",
      "lams[25,1]      1.36      0.26      1.32      0.99      1.73    293.43      1.00\n",
      "lams[26,0]      0.88      0.18      0.84      0.61      1.13    244.22      1.00\n",
      "lams[26,1]      1.30      0.26      1.25      0.90      1.66    239.80      1.00\n",
      "lams[27,0]     -0.23      0.15     -0.21     -0.48     -0.02    436.53      1.00\n",
      "lams[27,1]      0.83      0.10      0.82      0.67      0.98    435.91      1.00\n",
      "lams[28,0]     -0.61      0.23     -0.59     -0.93     -0.25    408.72      1.00\n",
      "lams[28,1]      1.36      0.22      1.33      1.04      1.69    389.64      1.00\n",
      "lams[29,0]      0.09      0.06      0.09      0.00      0.19    562.69      1.00\n",
      "lams[29,1]      0.75      0.06      0.74      0.65      0.83    520.49      1.00\n",
      "lams[30,0]      0.02      0.09      0.03     -0.12      0.15    339.55      1.01\n",
      "lams[30,1]      0.98      0.11      0.97      0.82      1.16    352.84      1.01\n",
      "lams[31,0]      0.53      0.05      0.52      0.45      0.61    878.11      1.00\n",
      "lams[31,1]      1.07      0.13      1.05      0.87      1.27    652.09      1.00\n",
      "lams[32,0]      0.07      0.05      0.07     -0.02      0.14    550.93      1.01\n",
      "lams[32,1]      0.47      0.03      0.47      0.43      0.52    557.81      1.00\n",
      "lams[33,0]      1.77      0.73      1.58      0.77      2.87    432.88      1.00\n",
      "lams[33,1]      2.01      0.73      1.85      0.98      3.16    434.98      1.00\n",
      "lams[34,0]      0.17      0.06      0.18      0.07      0.27    736.47      1.00\n",
      "lams[34,1]      1.15      0.16      1.14      0.91      1.38    510.83      1.00\n",
      "lams[35,0]      0.41      0.04      0.41      0.35      0.47   1338.95      1.00\n",
      "lams[35,1]      0.94      0.11      0.92      0.77      1.12    542.14      1.00\n",
      "lams[36,0]     -0.22      0.17     -0.20     -0.51      0.02    465.14      1.00\n",
      "lams[36,1]      0.94      0.13      0.92      0.75      1.14    452.54      1.00\n",
      "lams[37,0]     -0.35      0.21     -0.31     -0.66     -0.01    298.20      1.00\n",
      "lams[37,1]      1.28      0.20      1.25      0.96      1.58    286.19      1.00\n",
      "lams[38,0]      0.10      0.06      0.10     -0.01      0.19    831.20      1.00\n",
      "lams[38,1]      0.97      0.11      0.96      0.79      1.14    764.24      1.00\n",
      "lams[39,0]     -0.39      0.19     -0.37     -0.70     -0.12    436.54      1.00\n",
      "lams[39,1]      1.72      0.29      1.68      1.22      2.14    435.25      1.00\n",
      "lams[40,0]      0.18      0.08      0.19      0.05      0.31    951.97      1.00\n",
      "lams[40,1]      1.31      0.21      1.29      0.99      1.64    489.81      1.00\n",
      "lams[41,0]      1.74      0.75      1.56      0.71      2.87    431.25      1.00\n",
      "lams[41,1]      2.29      0.76      2.13      1.19      3.44    436.28      1.00\n",
      "lams[42,0]      1.50      0.60      1.36      0.66      2.35    506.33      1.00\n",
      "lams[42,1]      2.18      0.63      2.06      1.31      3.17    490.90      1.00\n",
      "lams[43,0]      1.75      0.83      1.57      0.60      2.96    441.32      1.01\n",
      "lams[43,1]      2.60      0.81      2.42      1.34      3.74    448.09      1.01\n",
      "lams[44,0]      1.72      0.81      1.54      0.56      2.92    501.82      1.00\n",
      "lams[44,1]      2.73      0.82      2.55      1.52      4.04    496.54      1.00\n",
      "lams[45,0]      1.78      0.72      1.64      0.74      2.77    587.88      1.00\n",
      "lams[45,1]      2.11      0.72      1.99      1.04      3.11    589.65      1.00\n",
      "lams[46,0]      1.97      0.63      1.85      1.05      2.90    494.24      1.00\n",
      "lams[46,1]      2.60      0.67      2.50      1.52      3.58    482.67      1.00\n",
      "lams[47,0]      2.14      0.66      1.98      1.16      3.06    429.60      1.00\n",
      "lams[47,1]      2.66      0.68      2.52      1.64      3.64    430.98      1.00\n",
      "lams[48,0]      1.55      0.57      1.43      0.72      2.32    438.66      1.00\n",
      "lams[48,1]      2.11      0.59      2.01      1.20      2.99    444.83      1.00\n",
      "lams[49,0]      1.57      0.71      1.43      0.57      2.61    494.04      1.00\n",
      "lams[49,1]      2.46      0.74      2.33      1.32      3.49    493.03      1.00\n",
      "lams[50,0]      1.75      0.62      1.60      0.87      2.62    493.53      1.00\n",
      "lams[50,1]      2.43      0.65      2.30      1.43      3.31    503.07      1.00\n",
      "lams[51,0]      1.43      0.67      1.29      0.48      2.41    464.93      1.00\n",
      "lams[51,1]      2.34      0.69      2.22      1.27      3.34    454.35      1.00\n",
      "lams[52,0]     -0.25      0.15     -0.23     -0.48     -0.01    481.99      1.00\n",
      "lams[52,1]      1.20      0.16      1.18      0.96      1.44    503.30      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.27    676.96      1.00\n",
      "lams[53,1]      0.68      0.04      0.68      0.61      0.75    476.89      1.00\n",
      "lams[54,0]      0.09      0.09      0.10     -0.04      0.22    337.40      1.00\n",
      "lams[54,1]      1.04      0.14      1.02      0.83      1.21    316.81      1.00\n",
      "lams[55,0]      1.79      0.66      1.67      0.80      2.74    509.90      1.00\n",
      "lams[55,1]      2.80      0.70      2.72      1.76      3.88    505.67      1.00\n",
      "lams[56,0]      1.49      0.69      1.32      0.56      2.53    465.03      1.00\n",
      "lams[56,1]      2.32      0.71      2.16      1.31      3.42    415.76      1.00\n",
      "lams[57,0]      0.19      0.05      0.20      0.12      0.28    776.78      1.00\n",
      "lams[57,1]      1.02      0.13      1.00      0.85      1.24    508.95      1.00\n",
      "lams[58,0]      0.52      0.05      0.52      0.44      0.59    916.68      1.00\n",
      "lams[58,1]      1.14      0.16      1.11      0.87      1.36    532.06      1.00\n",
      "lams[59,0]      0.27      0.05      0.28      0.19      0.36    747.11      1.00\n",
      "lams[59,1]      1.14      0.14      1.11      0.91      1.34    538.72      1.00\n",
      "lams[60,0]     -0.89      0.34     -0.83     -1.39     -0.39    283.25      1.00\n",
      "lams[60,1]      1.79      0.34      1.74      1.26      2.28    263.28      1.00\n",
      "lams[61,0]     -0.40      0.21     -0.37     -0.71     -0.08    259.27      1.00\n",
      "lams[61,1]      1.11      0.17      1.08      0.87      1.37    255.60      1.00\n",
      "lams[62,0]     -0.27      0.13     -0.25     -0.45     -0.05    261.26      1.00\n",
      "lams[62,1]      0.92      0.11      0.91      0.75      1.08    270.97      1.00\n",
      "lams[63,0]     -0.63      0.25     -0.60     -1.02     -0.23    433.15      1.01\n",
      "lams[63,1]      1.74      0.28      1.70      1.32      2.23    428.40      1.01\n",
      "lams[64,0]      1.63      0.63      1.48      0.71      2.50    457.96      1.00\n",
      "lams[64,1]      2.15      0.65      2.01      1.23      3.15    471.64      1.00\n",
      "lams[65,0]     -0.02      0.13     -0.00     -0.23      0.16    502.71      1.00\n",
      "lams[65,1]      1.72      0.31      1.69      1.23      2.13    505.10      1.00\n",
      "lams[66,0]      0.06      0.07      0.07     -0.05      0.18    467.23      1.00\n",
      "lams[66,1]      0.87      0.08      0.86      0.75      1.00    478.01      1.00\n",
      "lams[67,0]     -0.29      0.12     -0.28     -0.48     -0.11    486.17      1.00\n",
      "lams[67,1]      0.94      0.11      0.93      0.77      1.10    489.13      1.00\n",
      "lams[68,0]      0.11      0.05      0.12      0.03      0.19    695.18      1.00\n",
      "lams[68,1]      0.83      0.09      0.82      0.69      0.96    584.09      1.00\n",
      "lams[69,0]     -0.20      0.16     -0.18     -0.45      0.03    389.32      1.00\n",
      "lams[69,1]      1.12      0.15      1.10      0.88      1.36    361.90      1.00\n",
      "lams[70,0]      0.33      0.08      0.33      0.20      0.46   1205.50      1.00\n",
      "lams[70,1]      1.57      0.25      1.54      1.20      1.97    518.57      1.00\n",
      "lams[71,0]     -0.82      0.28     -0.78     -1.25     -0.37    574.02      1.00\n",
      "lams[71,1]      1.79      0.30      1.75      1.29      2.26    522.68      1.00\n",
      "lams[72,0]      0.15      0.05      0.16      0.07      0.24    495.16      1.01\n",
      "lams[72,1]      0.98      0.10      0.97      0.82      1.13    436.35      1.00\n",
      "lams[73,0]     -1.03      0.34     -0.97     -1.49     -0.48    466.17      1.00\n",
      "lams[73,1]      1.20      0.20      1.18      0.90      1.50    458.93      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [1:01:09<00:00,  1.47s/it, 511 steps of size 1.03e-02. acc. prob=0.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -24310.44\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.53      2.32     10.38      6.90     14.47   2065.36      1.00\n",
      "  gamma[1]      8.91      1.88      8.80      5.77     11.83   2082.76      1.00\n",
      "  gamma[2]      9.75      2.06      9.60      6.20     12.75   1654.13      1.00\n",
      "  gamma[3]     14.13      2.15     14.06     10.58     17.50   1911.30      1.00\n",
      "  gamma[4]      8.79      1.94      8.62      5.80     11.97   1813.72      1.00\n",
      "  gamma[5]      9.49      2.09      9.30      6.19     12.95   1714.38      1.00\n",
      "  gamma[6]     10.23      2.13     10.07      6.91     13.58   1388.07      1.00\n",
      "  gamma[7]     10.32      2.08     10.20      6.90     13.60   1400.61      1.00\n",
      "  gamma[8]     10.32      2.27     10.07      6.68     13.90   1520.68      1.00\n",
      "  gamma[9]     10.28      2.22     10.12      6.53     13.37   1732.30      1.00\n",
      " gamma[10]      8.68      1.93      8.53      5.40     11.65   1931.62      1.00\n",
      " gamma[11]      9.69      2.22      9.51      5.85     12.93   1845.54      1.00\n",
      " gamma[12]      9.61      2.26      9.45      5.83     13.03   1470.56      1.00\n",
      " gamma[13]      9.09      1.93      8.96      5.92     11.90   1450.62      1.00\n",
      " gamma[14]      9.53      2.03      9.38      6.36     12.92    863.89      1.00\n",
      " gamma[15]      6.55      1.42      6.49      4.26      8.95   1075.40      1.00\n",
      " gamma[16]      8.46      1.54      8.46      5.68     10.77   1386.03      1.00\n",
      " gamma[17]     11.16      1.05     11.17      9.52     12.90   1113.21      1.00\n",
      " gamma[18]      9.73      1.48      9.75      7.59     12.42    850.16      1.00\n",
      " gamma[19]      5.44      0.91      5.38      3.87      6.78    738.05      1.00\n",
      " gamma[20]      9.68      2.18      9.53      6.06     12.97   1619.41      1.00\n",
      " gamma[21]      9.43      2.04      9.23      6.17     12.77   1781.68      1.00\n",
      " gamma[22]      4.67      0.78      4.67      3.21      5.83   1180.22      1.00\n",
      " gamma[23]      6.40      1.49      6.26      3.86      8.64    928.39      1.00\n",
      " gamma[24]      7.50      1.53      7.43      5.02     10.14   1078.36      1.00\n",
      " gamma[25]      4.04      0.78      3.99      2.83      5.30   1097.65      1.00\n",
      " gamma[26]      9.54      2.13      9.34      5.75     12.71    738.84      1.00\n",
      " gamma[27]     10.51      1.54     10.49      7.88     12.90    607.41      1.00\n",
      " gamma[28]      3.65      0.67      3.64      2.56      4.69    964.31      1.00\n",
      " gamma[29]     10.83      1.47     10.81      8.46     13.23   1048.04      1.00\n",
      " gamma[30]      7.61      1.29      7.58      5.36      9.62    807.40      1.00\n",
      " gamma[31]      7.23      1.31      7.11      5.12      9.33    840.77      1.00\n",
      " gamma[32]     22.15      1.98     22.11     19.34     25.71    968.30      1.00\n",
      " gamma[33]     10.15      2.17      9.99      6.58     13.40   1825.04      1.00\n",
      " gamma[34]      4.12      0.71      4.11      3.00      5.33   1383.42      1.00\n",
      " gamma[35]      5.80      0.91      5.79      4.40      7.36   1464.45      1.00\n",
      " gamma[36]     10.43      1.63     10.37      7.82     13.10   1000.28      1.00\n",
      " gamma[37]      8.31      1.76      8.24      5.34     11.06    893.99      1.00\n",
      " gamma[38]      4.70      0.83      4.70      3.37      6.06    934.73      1.00\n",
      " gamma[39]      2.63      0.52      2.61      1.82      3.47    982.96      1.00\n",
      " gamma[40]      2.84      0.54      2.81      2.00      3.73   1156.51      1.00\n",
      " gamma[41]      8.35      1.63      8.24      5.89     11.13   2160.65      1.00\n",
      " gamma[42]      8.30      1.65      8.16      5.77     11.05   2273.71      1.00\n",
      " gamma[43]      8.27      1.71      8.12      5.38     10.99   1878.60      1.00\n",
      " gamma[44]      7.18      1.46      7.03      5.11      9.77   1821.23      1.00\n",
      " gamma[45]      9.88      2.25      9.68      6.09     13.28   2127.16      1.00\n",
      " gamma[46]      5.06      0.92      5.05      3.53      6.55   2024.96      1.00\n",
      " gamma[47]      4.61      0.79      4.58      3.33      5.91   2033.02      1.00\n",
      " gamma[48]      8.18      1.58      8.07      5.41     10.51   1911.93      1.00\n",
      " gamma[49]      7.67      1.58      7.57      4.97     10.01   1431.88      1.00\n",
      " gamma[50]      6.21      1.17      6.06      4.44      8.09   1579.75      1.00\n",
      " gamma[51]      8.97      1.80      8.86      6.33     12.34   1722.88      1.00\n",
      " gamma[52]      7.82      1.41      7.77      5.78     10.35   1058.10      1.00\n",
      " gamma[53]      9.87      1.16      9.82      7.74     11.57    831.39      1.00\n",
      " gamma[54]      9.61      1.74      9.52      6.51     12.25    974.71      1.00\n",
      " gamma[55]      5.50      1.11      5.44      3.73      7.20   1717.55      1.00\n",
      " gamma[56]      8.90      1.91      8.76      6.10     12.11   1623.64      1.00\n",
      " gamma[57]      4.86      0.80      4.83      3.34      5.98    836.46      1.00\n",
      " gamma[58]      7.09      1.40      7.04      4.89      9.55   1543.40      1.00\n",
      " gamma[59]      7.59      1.36      7.52      5.53      9.97    675.57      1.00\n",
      " gamma[60]      3.60      0.67      3.60      2.46      4.67    814.53      1.00\n",
      " gamma[61]      9.15      1.64      9.03      6.37     11.73    843.71      1.00\n",
      " gamma[62]      6.06      0.89      6.06      4.65      7.51    905.21      1.00\n",
      " gamma[63]      5.30      1.10      5.22      3.34      6.92    642.06      1.00\n",
      " gamma[64]      8.59      1.84      8.46      5.84     11.65   1852.73      1.00\n",
      " gamma[65]      4.10      0.87      4.01      2.78      5.54    932.63      1.00\n",
      " gamma[66]      9.48      1.41      9.47      7.10     11.64    661.11      1.00\n",
      " gamma[67]      4.89      0.74      4.85      3.67      6.11    884.71      1.00\n",
      " gamma[68]      5.23      0.79      5.20      3.92      6.54    985.71      1.00\n",
      " gamma[69]      9.22      1.55      9.12      6.48     11.67    818.63      1.00\n",
      " gamma[70]      6.05      1.26      5.99      3.86      7.95   1346.16      1.00\n",
      " gamma[71]      3.20      0.64      3.15      2.22      4.31    822.24      1.00\n",
      " gamma[72]      7.72      1.26      7.70      5.65      9.73   1118.13      1.00\n",
      " gamma[73]      4.90      0.84      4.85      3.60      6.30   1011.88      1.00\n",
      " lams[0,0]      1.57      0.67      1.43      0.58      2.49    698.69      1.00\n",
      " lams[0,1]      2.07      0.68      1.95      1.05      3.04    729.66      1.00\n",
      " lams[1,0]      1.63      0.74      1.44      0.60      2.76    601.37      1.00\n",
      " lams[1,1]      2.30      0.75      2.13      1.23      3.46    611.61      1.00\n",
      " lams[2,0]      1.53      0.78      1.37      0.44      2.63    581.08      1.00\n",
      " lams[2,1]      2.41      0.79      2.25      1.16      3.59    600.46      1.00\n",
      " lams[3,0]      8.18      1.00      8.20      6.61      9.83   1435.96      1.00\n",
      " lams[3,1]      8.30      0.98      8.32      6.78      9.94   1439.17      1.00\n",
      " lams[4,0]      1.71      0.67      1.56      0.82      2.70    540.20      1.00\n",
      " lams[4,1]      2.00      0.68      1.87      1.01      2.98    561.57      1.00\n",
      " lams[5,0]      1.70      0.76      1.53      0.63      2.75    644.68      1.00\n",
      " lams[5,1]      2.14      0.76      1.98      1.10      3.28    648.03      1.00\n",
      " lams[6,0]      1.71      0.71      1.55      0.67      2.72    608.20      1.00\n",
      " lams[6,1]      2.06      0.70      1.91      0.99      3.09    620.27      1.00\n",
      " lams[7,0]      1.52      0.68      1.38      0.58      2.47    552.97      1.00\n",
      " lams[7,1]      2.09      0.69      1.96      1.05      2.98    546.94      1.00\n",
      " lams[8,0]      1.51      0.74      1.36      0.47      2.45    578.94      1.00\n",
      " lams[8,1]      2.22      0.75      2.09      1.16      3.21    602.37      1.00\n",
      " lams[9,0]      1.57      0.81      1.35      0.52      2.80    383.34      1.01\n",
      " lams[9,1]      2.29      0.81      2.09      1.19      3.55    412.25      1.01\n",
      "lams[10,0]      1.44      0.61      1.31      0.58      2.33    603.07      1.00\n",
      "lams[10,1]      2.10      0.63      1.97      1.21      3.08    630.06      1.00\n",
      "lams[11,0]      1.57      0.68      1.41      0.64      2.52    520.17      1.00\n",
      "lams[11,1]      2.05      0.69      1.91      1.00      3.01    536.04      1.00\n",
      "lams[12,0]      1.60      0.69      1.46      0.66      2.68    581.63      1.00\n",
      "lams[12,1]      2.08      0.71      1.95      1.03      3.16    597.47      1.00\n",
      "lams[13,0]      1.27      0.72      1.09      0.35      2.42    559.15      1.00\n",
      "lams[13,1]      2.45      0.73      2.30      1.36      3.50    526.14      1.00\n",
      "lams[14,0]      0.69      0.15      0.66      0.48      0.91    719.28      1.00\n",
      "lams[14,1]      1.36      0.24      1.33      0.99      1.72    597.03      1.00\n",
      "lams[15,0]      0.94      0.22      0.90      0.60      1.24    696.69      1.00\n",
      "lams[15,1]      1.71      0.35      1.66      1.20      2.24    705.04      1.00\n",
      "lams[16,0]      1.67      0.64      1.52      0.78      2.54    641.95      1.00\n",
      "lams[16,1]      2.00      0.65      1.86      1.04      2.92    638.85      1.00\n",
      "lams[17,0]      0.01      0.05      0.01     -0.07      0.08   1041.08      1.00\n",
      "lams[17,1]      0.56      0.03      0.56      0.50      0.62    937.30      1.00\n",
      "lams[18,0]      0.28      0.04      0.29      0.23      0.33    674.71      1.00\n",
      "lams[18,1]      0.77      0.07      0.76      0.66      0.87    655.97      1.00\n",
      "lams[19,0]     -0.21      0.14     -0.20     -0.44     -0.02    755.05      1.00\n",
      "lams[19,1]      1.16      0.16      1.15      0.92      1.40    654.36      1.00\n",
      "lams[20,0]      1.53      0.75      1.35      0.52      2.58    556.70      1.00\n",
      "lams[20,1]      2.24      0.75      2.08      1.13      3.33    538.48      1.00\n",
      "lams[21,0]      1.50      0.58      1.35      0.71      2.38    479.44      1.00\n",
      "lams[21,1]      1.93      0.59      1.79      1.08      2.84    487.54      1.00\n",
      "lams[22,0]      0.25      0.05      0.25      0.18      0.33   1874.22      1.00\n",
      "lams[22,1]      1.03      0.14      1.01      0.80      1.22    923.83      1.00\n",
      "lams[23,0]      0.79      0.13      0.78      0.58      0.98    651.41      1.00\n",
      "lams[23,1]      1.37      0.25      1.34      0.98      1.73    666.83      1.00\n",
      "lams[24,0]      0.85      0.20      0.81      0.55      1.12    537.97      1.00\n",
      "lams[24,1]      1.57      0.31      1.53      1.14      2.06    516.25      1.00\n",
      "lams[25,0]      0.24      0.06      0.24      0.15      0.35   1498.63      1.00\n",
      "lams[25,1]      1.29      0.20      1.26      0.97      1.58   1007.89      1.00\n",
      "lams[26,0]      0.86      0.18      0.82      0.60      1.09    337.75      1.00\n",
      "lams[26,1]      1.22      0.24      1.18      0.87      1.57    348.64      1.00\n",
      "lams[27,0]     -0.27      0.17     -0.23     -0.49     -0.01    516.59      1.00\n",
      "lams[27,1]      0.83      0.11      0.81      0.67      0.98    507.42      1.00\n",
      "lams[28,0]     -0.75      0.26     -0.71     -1.20     -0.36    699.86      1.00\n",
      "lams[28,1]      1.51      0.26      1.47      1.14      1.95    684.80      1.00\n",
      "lams[29,0]      0.08      0.06      0.09     -0.01      0.18    775.18      1.00\n",
      "lams[29,1]      0.75      0.06      0.74      0.65      0.85    645.57      1.00\n",
      "lams[30,0]     -0.03      0.10     -0.02     -0.17      0.13    667.88      1.00\n",
      "lams[30,1]      1.03      0.13      1.01      0.84      1.22    635.36      1.00\n",
      "lams[31,0]      0.54      0.05      0.53      0.45      0.62    755.75      1.00\n",
      "lams[31,1]      1.02      0.13      1.01      0.81      1.21    663.54      1.00\n",
      "lams[32,0]      0.05      0.05      0.05     -0.02      0.12    955.76      1.00\n",
      "lams[32,1]      0.44      0.02      0.44      0.41      0.48   1042.19      1.00\n",
      "lams[33,0]      1.77      0.72      1.60      0.75      2.84    576.43      1.01\n",
      "lams[33,1]      2.00      0.72      1.84      1.02      3.15    580.15      1.01\n",
      "lams[34,0]      0.18      0.06      0.19      0.09      0.29   1050.69      1.00\n",
      "lams[34,1]      1.16      0.16      1.15      0.94      1.43    818.98      1.00\n",
      "lams[35,0]      0.41      0.04      0.42      0.36      0.47   1435.91      1.00\n",
      "lams[35,1]      0.90      0.10      0.88      0.75      1.05   1196.76      1.00\n",
      "lams[36,0]     -0.26      0.16     -0.23     -0.53     -0.02    780.98      1.00\n",
      "lams[36,1]      0.96      0.12      0.94      0.77      1.14    747.47      1.00\n",
      "lams[37,0]     -0.52      0.29     -0.48     -0.97     -0.10    635.69      1.00\n",
      "lams[37,1]      1.38      0.26      1.34      0.98      1.75    632.98      1.00\n",
      "lams[38,0]      0.07      0.07      0.08     -0.04      0.20    710.69      1.00\n",
      "lams[38,1]      1.02      0.14      1.00      0.81      1.23    704.62      1.00\n",
      "lams[39,0]     -0.41      0.19     -0.39     -0.72     -0.12    895.31      1.00\n",
      "lams[39,1]      1.83      0.32      1.78      1.31      2.28    849.26      1.00\n",
      "lams[40,0]      0.16      0.08      0.17      0.04      0.28   1143.34      1.00\n",
      "lams[40,1]      1.24      0.20      1.22      0.94      1.55    944.32      1.00\n",
      "lams[41,0]      1.72      0.75      1.54      0.68      2.81    451.95      1.00\n",
      "lams[41,1]      2.28      0.77      2.10      1.28      3.50    464.61      1.01\n",
      "lams[42,0]      1.46      0.58      1.32      0.69      2.32    412.22      1.00\n",
      "lams[42,1]      2.12      0.61      2.00      1.24      3.02    423.28      1.00\n",
      "lams[43,0]      1.83      0.89      1.62      0.61      3.09    559.06      1.00\n",
      "lams[43,1]      2.69      0.89      2.51      1.40      4.02    592.91      1.00\n",
      "lams[44,0]      1.59      0.69      1.44      0.58      2.60    862.61      1.00\n",
      "lams[44,1]      2.60      0.71      2.47      1.57      3.73    876.66      1.00\n",
      "lams[45,0]      1.81      0.76      1.64      0.75      2.91    557.46      1.00\n",
      "lams[45,1]      2.14      0.76      1.98      1.04      3.29    569.25      1.00\n",
      "lams[46,0]      1.98      0.60      1.84      1.16      2.94    530.73      1.00\n",
      "lams[46,1]      2.61      0.63      2.49      1.68      3.58    541.98      1.00\n",
      "lams[47,0]      2.16      0.65      2.03      1.18      3.21    531.66      1.00\n",
      "lams[47,1]      2.69      0.68      2.58      1.72      3.86    549.70      1.00\n",
      "lams[48,0]      1.61      0.66      1.45      0.70      2.47    498.51      1.00\n",
      "lams[48,1]      2.17      0.68      2.04      1.27      3.16    520.72      1.00\n",
      "lams[49,0]      1.45      0.61      1.31      0.61      2.33    569.24      1.01\n",
      "lams[49,1]      2.33      0.65      2.21      1.38      3.33    500.56      1.01\n",
      "lams[50,0]      1.71      0.59      1.60      0.84      2.57    656.63      1.00\n",
      "lams[50,1]      2.39      0.63      2.28      1.41      3.34    713.19      1.00\n",
      "lams[51,0]      1.50      0.76      1.35      0.45      2.54    542.55      1.00\n",
      "lams[51,1]      2.43      0.78      2.30      1.28      3.46    536.68      1.00\n",
      "lams[52,0]     -0.26      0.17     -0.23     -0.53     -0.01    965.59      1.00\n",
      "lams[52,1]      1.20      0.18      1.18      0.93      1.46    933.89      1.00\n",
      "lams[53,0]      0.21      0.03      0.22      0.16      0.26    967.86      1.00\n",
      "lams[53,1]      0.68      0.04      0.68      0.61      0.76    615.88      1.00\n",
      "lams[54,0]      0.08      0.09      0.09     -0.05      0.22    653.74      1.00\n",
      "lams[54,1]      1.05      0.13      1.04      0.86      1.26    669.10      1.00\n",
      "lams[55,0]      1.78      0.70      1.62      0.83      2.78    511.83      1.00\n",
      "lams[55,1]      2.77      0.74      2.64      1.69      3.91    561.69      1.00\n",
      "lams[56,0]      1.48      0.70      1.29      0.56      2.52    567.81      1.00\n",
      "lams[56,1]      2.30      0.72      2.15      1.25      3.30    553.73      1.00\n",
      "lams[57,0]      0.19      0.05      0.20      0.09      0.27    919.63      1.00\n",
      "lams[57,1]      1.03      0.13      1.02      0.84      1.24    631.54      1.00\n",
      "lams[58,0]      0.52      0.05      0.51      0.44      0.60    779.91      1.00\n",
      "lams[58,1]      1.14      0.17      1.11      0.89      1.39    765.37      1.00\n",
      "lams[59,0]      0.27      0.05      0.27      0.18      0.35    519.56      1.00\n",
      "lams[59,1]      1.14      0.14      1.12      0.93      1.37    434.92      1.00\n",
      "lams[60,0]     -0.88      0.31     -0.85     -1.31     -0.38    638.67      1.00\n",
      "lams[60,1]      1.79      0.32      1.74      1.34      2.28    631.06      1.00\n",
      "lams[61,0]     -0.42      0.22     -0.38     -0.77     -0.10    665.47      1.00\n",
      "lams[61,1]      1.12      0.18      1.10      0.85      1.37    655.24      1.00\n",
      "lams[62,0]     -0.27      0.12     -0.25     -0.44     -0.06    598.00      1.00\n",
      "lams[62,1]      0.92      0.11      0.91      0.75      1.08    610.29      1.00\n",
      "lams[63,0]     -0.62      0.28     -0.59     -1.01     -0.16    468.62      1.00\n",
      "lams[63,1]      1.74      0.31      1.69      1.24      2.19    459.53      1.00\n",
      "lams[64,0]      1.63      0.62      1.49      0.75      2.55    438.16      1.00\n",
      "lams[64,1]      2.15      0.64      2.02      1.29      3.23    431.66      1.00\n",
      "lams[65,0]     -0.03      0.14     -0.01     -0.23      0.19    909.15      1.00\n",
      "lams[65,1]      1.75      0.32      1.70      1.27      2.25    659.18      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.05      0.17    567.19      1.00\n",
      "lams[66,1]      0.87      0.08      0.86      0.75      1.00    512.38      1.00\n",
      "lams[67,0]     -0.31      0.13     -0.29     -0.50     -0.11    684.87      1.00\n",
      "lams[67,1]      0.96      0.12      0.94      0.76      1.12    732.43      1.00\n",
      "lams[68,0]      0.10      0.05      0.11      0.03      0.19    896.23      1.00\n",
      "lams[68,1]      0.85      0.09      0.83      0.71      0.99    830.79      1.00\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.39      0.05    438.44      1.00\n",
      "lams[69,1]      1.11      0.14      1.09      0.90      1.33    484.44      1.00\n",
      "lams[70,0]      0.36      0.08      0.36      0.23      0.50   1748.56      1.00\n",
      "lams[70,1]      1.59      0.27      1.55      1.18      2.03    964.99      1.01\n",
      "lams[71,0]     -0.82      0.29     -0.79     -1.27     -0.38    623.89      1.00\n",
      "lams[71,1]      1.82      0.32      1.79      1.30      2.28    565.25      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.05      0.25    962.18      1.00\n",
      "lams[72,1]      1.00      0.11      0.98      0.83      1.17    816.87      1.00\n",
      "lams[73,0]     -1.07      0.33     -1.03     -1.60     -0.58    835.34      1.00\n",
      "lams[73,1]      1.23      0.20      1.21      0.91      1.53    829.71      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [59:16<00:00,  1.42s/it, 511 steps of size 8.57e-03. acc. prob=0.94]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -24567.67\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.36      2.46     10.27      6.80     14.44   2127.28      1.00\n",
      "  gamma[1]      8.68      1.83      8.61      5.49     11.21   2130.95      1.00\n",
      "  gamma[2]      9.92      2.02      9.81      6.49     13.05   1602.60      1.00\n",
      "  gamma[3]     14.69      2.22     14.53     11.53     18.54   2301.31      1.00\n",
      "  gamma[4]      8.92      1.93      8.72      6.12     12.11   1804.24      1.00\n",
      "  gamma[5]      9.52      2.12      9.36      6.27     13.24   2120.85      1.00\n",
      "  gamma[6]     10.47      2.20     10.38      7.04     13.91   1802.27      1.00\n",
      "  gamma[7]     10.55      2.18     10.49      7.02     13.84   1809.12      1.00\n",
      "  gamma[8]     10.25      2.32     10.09      6.57     13.98   1483.64      1.00\n",
      "  gamma[9]     10.29      2.29     10.09      6.85     14.15   1778.80      1.00\n",
      " gamma[10]      8.56      1.83      8.47      5.57     11.16   1578.62      1.00\n",
      " gamma[11]      9.65      2.14      9.48      5.84     12.67   2002.17      1.00\n",
      " gamma[12]      9.68      2.10      9.53      6.29     12.98   1687.09      1.00\n",
      " gamma[13]      9.20      2.02      9.08      5.81     12.17   1948.45      1.00\n",
      " gamma[14]      9.78      2.34      9.67      6.04     13.46    842.47      1.00\n",
      " gamma[15]      6.35      1.37      6.23      4.11      8.54    733.65      1.00\n",
      " gamma[16]      8.15      1.65      8.04      5.38     10.52   1710.75      1.00\n",
      " gamma[17]     10.29      0.99     10.26      8.65     11.83    714.69      1.00\n",
      " gamma[18]      8.92      1.36      8.87      7.02     11.47    541.92      1.00\n",
      " gamma[19]      5.14      0.88      5.10      3.65      6.51    471.39      1.00\n",
      " gamma[20]      9.74      2.15      9.55      6.62     13.46   1286.19      1.00\n",
      " gamma[21]      9.44      2.01      9.32      6.18     12.56   1499.64      1.00\n",
      " gamma[22]      4.49      0.74      4.44      3.32      5.65    992.52      1.00\n",
      " gamma[23]      6.56      1.45      6.46      4.15      8.84    435.44      1.00\n",
      " gamma[24]      7.80      1.74      7.70      5.15     10.74    972.11      1.00\n",
      " gamma[25]      4.21      0.78      4.19      2.86      5.46   1126.06      1.00\n",
      " gamma[26]      9.66      2.15      9.48      6.15     13.05    748.23      1.00\n",
      " gamma[27]      9.37      1.37      9.32      7.30     11.80    610.12      1.00\n",
      " gamma[28]      3.31      0.64      3.28      2.34      4.40    559.59      1.00\n",
      " gamma[29]      9.70      1.26      9.65      7.89     12.02    550.72      1.00\n",
      " gamma[30]      6.79      1.17      6.73      4.91      8.69    635.75      1.00\n",
      " gamma[31]      7.29      1.27      7.22      5.20      9.38    729.37      1.00\n",
      " gamma[32]     18.34      1.79     18.34     15.63     21.51    536.33      1.00\n",
      " gamma[33]     10.07      2.20      9.94      6.44     13.33   1809.07      1.00\n",
      " gamma[34]      3.99      0.77      3.95      2.59      5.06    790.92      1.00\n",
      " gamma[35]      5.87      0.93      5.83      4.23      7.29   1307.35      1.00\n",
      " gamma[36]      8.78      1.40      8.67      6.42     11.01    820.36      1.00\n",
      " gamma[37]      7.36      1.47      7.28      5.03      9.78    562.55      1.00\n",
      " gamma[38]      4.49      0.76      4.46      3.17      5.63    583.68      1.00\n",
      " gamma[39]      2.45      0.50      2.43      1.59      3.13    591.40      1.00\n",
      " gamma[40]      3.12      0.58      3.10      2.15      4.05    635.40      1.00\n",
      " gamma[41]      8.24      1.59      8.12      5.67     10.79   1947.95      1.00\n",
      " gamma[42]      8.26      1.61      8.10      5.72     10.95   1168.66      1.00\n",
      " gamma[43]      8.13      1.71      7.97      5.04     10.57   1772.56      1.00\n",
      " gamma[44]      7.18      1.56      7.10      4.53      9.57   2167.26      1.00\n",
      " gamma[45]      9.92      2.18      9.78      6.34     13.41   1393.02      1.00\n",
      " gamma[46]      5.00      0.89      4.93      3.53      6.38    927.31      1.00\n",
      " gamma[47]      4.55      0.76      4.52      3.24      5.60   1790.87      1.00\n",
      " gamma[48]      8.24      1.68      8.09      5.30     10.65   1601.21      1.00\n",
      " gamma[49]      7.74      1.61      7.65      4.94     10.06   1259.00      1.00\n",
      " gamma[50]      6.28      1.15      6.21      4.26      7.98   1235.45      1.00\n",
      " gamma[51]      9.13      1.81      9.03      6.38     12.28   1410.00      1.00\n",
      " gamma[52]      7.83      1.32      7.71      5.72      9.95    693.11      1.00\n",
      " gamma[53]      9.75      1.15      9.70      7.81     11.58    769.17      1.00\n",
      " gamma[54]      9.48      1.67      9.40      6.64     12.16    533.98      1.00\n",
      " gamma[55]      5.57      1.10      5.49      4.06      7.58   1350.80      1.00\n",
      " gamma[56]      8.88      1.95      8.75      5.91     12.15   1619.06      1.00\n",
      " gamma[57]      4.80      0.82      4.79      3.41      6.00    564.78      1.01\n",
      " gamma[58]      7.02      1.40      6.95      4.75      9.24    547.29      1.00\n",
      " gamma[59]      7.60      1.38      7.51      5.24      9.69    714.83      1.00\n",
      " gamma[60]      3.60      0.68      3.58      2.55      4.76    645.51      1.00\n",
      " gamma[61]      9.13      1.61      9.04      6.38     11.71    698.17      1.00\n",
      " gamma[62]      5.94      0.84      5.92      4.44      7.15    561.26      1.00\n",
      " gamma[63]      5.27      1.07      5.23      3.58      7.07    429.32      1.00\n",
      " gamma[64]      8.61      1.77      8.50      5.76     11.44   1369.44      1.00\n",
      " gamma[65]      4.16      0.86      4.11      2.82      5.52    742.75      1.00\n",
      " gamma[66]      9.48      1.37      9.42      7.19     11.69    790.66      1.00\n",
      " gamma[67]      4.84      0.73      4.84      3.63      5.98    549.15      1.00\n",
      " gamma[68]      5.17      0.78      5.16      3.84      6.32    716.02      1.00\n",
      " gamma[69]      9.23      1.71      9.16      6.32     11.86    593.70      1.00\n",
      " gamma[70]      6.06      1.24      5.99      4.05      8.14    854.29      1.00\n",
      " gamma[71]      3.15      0.62      3.08      2.18      4.16    544.06      1.00\n",
      " gamma[72]      7.79      1.20      7.78      6.05     10.02    514.11      1.00\n",
      " gamma[73]      4.90      0.84      4.87      3.38      6.14    619.12      1.00\n",
      " lams[0,0]      1.59      0.73      1.40      0.62      2.58    413.61      1.00\n",
      " lams[0,1]      2.09      0.74      1.95      1.01      3.10    438.73      1.00\n",
      " lams[1,0]      1.65      0.72      1.51      0.66      2.80    503.25      1.00\n",
      " lams[1,1]      2.33      0.72      2.22      1.21      3.41    517.35      1.00\n",
      " lams[2,0]      1.45      0.72      1.29      0.48      2.51    434.35      1.00\n",
      " lams[2,1]      2.29      0.72      2.18      1.26      3.43    482.81      1.00\n",
      " lams[3,0]      8.00      0.92      8.02      6.51      9.51    980.22      1.00\n",
      " lams[3,1]      8.09      0.91      8.11      6.65      9.63    998.55      1.00\n",
      " lams[4,0]      1.67      0.62      1.54      0.80      2.61    555.40      1.00\n",
      " lams[4,1]      1.94      0.63      1.81      1.02      2.87    563.99      1.00\n",
      " lams[5,0]      1.64      0.69      1.49      0.70      2.69    366.71      1.00\n",
      " lams[5,1]      2.06      0.70      1.94      0.93      2.99    390.90      1.00\n",
      " lams[6,0]      1.72      0.73      1.55      0.67      2.73    483.26      1.00\n",
      " lams[6,1]      2.04      0.73      1.89      0.96      3.10    494.08      1.00\n",
      " lams[7,0]      1.59      0.78      1.42      0.56      2.73    517.07      1.00\n",
      " lams[7,1]      2.12      0.78      1.96      1.03      3.25    529.30      1.00\n",
      " lams[8,0]      1.59      0.83      1.38      0.51      2.79    451.68      1.00\n",
      " lams[8,1]      2.27      0.82      2.12      1.12      3.52    476.56      1.00\n",
      " lams[9,0]      1.61      0.77      1.47      0.53      2.73    321.85      1.00\n",
      " lams[9,1]      2.28      0.77      2.17      1.16      3.43    320.68      1.00\n",
      "lams[10,0]      1.51      0.67      1.37      0.62      2.40    402.55      1.00\n",
      "lams[10,1]      2.14      0.68      2.03      1.19      3.08    412.98      1.00\n",
      "lams[11,0]      1.62      0.70      1.45      0.65      2.62    295.30      1.00\n",
      "lams[11,1]      2.08      0.71      1.94      1.01      3.08    302.26      1.00\n",
      "lams[12,0]      1.63      0.71      1.47      0.68      2.72    435.69      1.00\n",
      "lams[12,1]      2.09      0.72      1.96      1.12      3.25    443.98      1.00\n",
      "lams[13,0]      1.26      0.68      1.13      0.36      2.28    493.66      1.00\n",
      "lams[13,1]      2.37      0.69      2.25      1.29      3.41    465.02      1.00\n",
      "lams[14,0]      0.69      0.15      0.67      0.48      0.90    532.50      1.00\n",
      "lams[14,1]      1.34      0.26      1.30      0.90      1.69    399.71      1.00\n",
      "lams[15,0]      0.98      0.24      0.95      0.65      1.35    489.51      1.00\n",
      "lams[15,1]      1.73      0.35      1.67      1.17      2.29    454.09      1.00\n",
      "lams[16,0]      1.74      0.68      1.59      0.86      2.82    460.86      1.00\n",
      "lams[16,1]      2.08      0.69      1.93      1.09      3.11    455.03      1.00\n",
      "lams[17,0]     -0.02      0.05     -0.01     -0.09      0.06    600.99      1.00\n",
      "lams[17,1]      0.58      0.04      0.58      0.52      0.64    609.31      1.00\n",
      "lams[18,0]      0.28      0.04      0.28      0.22      0.33    612.36      1.00\n",
      "lams[18,1]      0.80      0.07      0.79      0.69      0.91    493.55      1.00\n",
      "lams[19,0]     -0.24      0.14     -0.22     -0.46     -0.03    411.66      1.00\n",
      "lams[19,1]      1.20      0.17      1.17      0.91      1.44    397.13      1.00\n",
      "lams[20,0]      1.59      0.76      1.45      0.52      2.64    529.99      1.00\n",
      "lams[20,1]      2.28      0.77      2.14      1.12      3.34    558.61      1.00\n",
      "lams[21,0]      1.45      0.56      1.34      0.69      2.32    417.00      1.02\n",
      "lams[21,1]      1.87      0.59      1.76      1.02      2.73    425.55      1.02\n",
      "lams[22,0]      0.25      0.05      0.25      0.18      0.33   1953.93      1.00\n",
      "lams[22,1]      1.04      0.14      1.02      0.84      1.24    846.01      1.00\n",
      "lams[23,0]      0.79      0.14      0.77      0.60      0.99    284.51      1.01\n",
      "lams[23,1]      1.34      0.25      1.30      0.94      1.67    268.35      1.01\n",
      "lams[24,0]      0.84      0.20      0.79      0.56      1.15    525.49      1.00\n",
      "lams[24,1]      1.52      0.32      1.45      1.02      2.00    509.79      1.00\n",
      "lams[25,0]      0.25      0.06      0.25      0.15      0.34   1281.54      1.00\n",
      "lams[25,1]      1.23      0.19      1.21      0.95      1.52    838.96      1.00\n",
      "lams[26,0]      0.85      0.17      0.83      0.62      1.10    374.15      1.00\n",
      "lams[26,1]      1.20      0.23      1.17      0.87      1.56    393.22      1.00\n",
      "lams[27,0]     -0.30      0.16     -0.28     -0.54     -0.04    545.35      1.00\n",
      "lams[27,1]      0.85      0.11      0.83      0.68      1.01    500.46      1.00\n",
      "lams[28,0]     -0.81      0.30     -0.76     -1.28     -0.38    435.28      1.00\n",
      "lams[28,1]      1.60      0.29      1.56      1.18      2.03    437.79      1.00\n",
      "lams[29,0]      0.07      0.06      0.07     -0.02      0.17    403.76      1.00\n",
      "lams[29,1]      0.77      0.06      0.76      0.66      0.86    410.44      1.00\n",
      "lams[30,0]     -0.07      0.11     -0.05     -0.24      0.11    525.80      1.00\n",
      "lams[30,1]      1.10      0.15      1.08      0.88      1.31    493.01      1.00\n",
      "lams[31,0]      0.54      0.05      0.53      0.46      0.62    623.41      1.00\n",
      "lams[31,1]      1.00      0.12      0.98      0.81      1.20    580.43      1.00\n",
      "lams[32,0]     -0.01      0.06     -0.00     -0.10      0.08    426.01      1.00\n",
      "lams[32,1]      0.47      0.03      0.47      0.42      0.52    411.59      1.00\n",
      "lams[33,0]      1.84      0.71      1.71      0.78      2.83    597.72      1.00\n",
      "lams[33,1]      2.06      0.70      1.95      1.00      3.08    602.95      1.00\n",
      "lams[34,0]      0.20      0.06      0.20      0.10      0.30    959.98      1.00\n",
      "lams[34,1]      1.18      0.18      1.15      0.90      1.46    583.11      1.00\n",
      "lams[35,0]      0.42      0.03      0.41      0.35      0.47   1808.57      1.00\n",
      "lams[35,1]      0.88      0.09      0.87      0.73      1.02   1059.55      1.00\n",
      "lams[36,0]     -0.31      0.17     -0.29     -0.55     -0.03    702.82      1.00\n",
      "lams[36,1]      1.02      0.13      1.00      0.81      1.23    693.78      1.00\n",
      "lams[37,0]     -0.61      0.28     -0.57     -1.07     -0.17    432.33      1.00\n",
      "lams[37,1]      1.49      0.26      1.46      1.10      1.93    437.61      1.00\n",
      "lams[38,0]      0.07      0.07      0.07     -0.04      0.19    601.88      1.00\n",
      "lams[38,1]      1.04      0.14      1.02      0.83      1.26    511.18      1.00\n",
      "lams[39,0]     -0.41      0.20     -0.39     -0.70     -0.11    547.64      1.01\n",
      "lams[39,1]      1.90      0.34      1.85      1.32      2.40    475.59      1.00\n",
      "lams[40,0]      0.15      0.07      0.16      0.04      0.26    658.59      1.00\n",
      "lams[40,1]      1.16      0.18      1.13      0.89      1.43    516.14      1.00\n",
      "lams[41,0]      1.73      0.73      1.56      0.71      2.76    368.59      1.00\n",
      "lams[41,1]      2.29      0.74      2.15      1.21      3.36    370.30      1.00\n",
      "lams[42,0]      1.47      0.60      1.32      0.67      2.36    438.02      1.00\n",
      "lams[42,1]      2.14      0.64      1.98      1.24      3.07    426.15      1.00\n",
      "lams[43,0]      1.83      0.84      1.66      0.63      3.04    348.62      1.00\n",
      "lams[43,1]      2.71      0.83      2.56      1.42      3.90    368.13      1.00\n",
      "lams[44,0]      1.61      0.72      1.47      0.66      2.69    496.84      1.00\n",
      "lams[44,1]      2.63      0.73      2.54      1.56      3.69    514.32      1.00\n",
      "lams[45,0]      1.76      0.71      1.58      0.78      2.87    546.60      1.00\n",
      "lams[45,1]      2.08      0.72      1.91      1.11      3.23    553.10      1.00\n",
      "lams[46,0]      1.96      0.63      1.83      0.99      2.85    396.75      1.00\n",
      "lams[46,1]      2.59      0.66      2.49      1.61      3.60    396.58      1.00\n",
      "lams[47,0]      2.13      0.60      2.01      1.22      3.09    615.42      1.00\n",
      "lams[47,1]      2.66      0.61      2.56      1.72      3.66    600.69      1.00\n",
      "lams[48,0]      1.60      0.63      1.47      0.68      2.51    537.75      1.00\n",
      "lams[48,1]      2.16      0.64      2.02      1.17      3.12    541.40      1.00\n",
      "lams[49,0]      1.53      0.66      1.39      0.57      2.41    478.36      1.00\n",
      "lams[49,1]      2.41      0.69      2.30      1.41      3.41    476.30      1.00\n",
      "lams[50,0]      1.72      0.59      1.58      0.84      2.55    493.90      1.00\n",
      "lams[50,1]      2.39      0.63      2.29      1.46      3.33    496.55      1.00\n",
      "lams[51,0]      1.49      0.76      1.32      0.51      2.60    520.27      1.00\n",
      "lams[51,1]      2.40      0.79      2.25      1.21      3.51    519.56      1.00\n",
      "lams[52,0]     -0.28      0.17     -0.26     -0.54     -0.03    528.17      1.00\n",
      "lams[52,1]      1.22      0.17      1.20      0.95      1.46    515.53      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.26    843.36      1.00\n",
      "lams[53,1]      0.69      0.04      0.68      0.62      0.75    732.31      1.00\n",
      "lams[54,0]      0.07      0.09      0.09     -0.06      0.20    438.53      1.00\n",
      "lams[54,1]      1.06      0.13      1.04      0.87      1.27    434.83      1.00\n",
      "lams[55,0]      1.74      0.62      1.64      0.87      2.74    858.61      1.00\n",
      "lams[55,1]      2.72      0.68      2.66      1.61      3.66    854.95      1.00\n",
      "lams[56,0]      1.50      0.71      1.32      0.57      2.58    403.90      1.00\n",
      "lams[56,1]      2.33      0.72      2.17      1.31      3.45    418.34      1.00\n",
      "lams[57,0]      0.19      0.06      0.19      0.11      0.29    536.05      1.01\n",
      "lams[57,1]      1.05      0.13      1.03      0.83      1.25    466.29      1.01\n",
      "lams[58,0]      0.52      0.05      0.52      0.44      0.59    717.02      1.00\n",
      "lams[58,1]      1.15      0.16      1.13      0.88      1.37    400.73      1.00\n",
      "lams[59,0]      0.27      0.05      0.28      0.19      0.36    816.62      1.00\n",
      "lams[59,1]      1.14      0.15      1.12      0.92      1.36    585.45      1.01\n",
      "lams[60,0]     -0.88      0.30     -0.83     -1.36     -0.39    418.53      1.00\n",
      "lams[60,1]      1.80      0.31      1.75      1.30      2.27    379.64      1.01\n",
      "lams[61,0]     -0.42      0.21     -0.39     -0.73     -0.07    486.02      1.00\n",
      "lams[61,1]      1.12      0.17      1.10      0.87      1.38    478.61      1.00\n",
      "lams[62,0]     -0.28      0.12     -0.26     -0.46     -0.08    482.50      1.00\n",
      "lams[62,1]      0.93      0.10      0.92      0.78      1.11    492.86      1.00\n",
      "lams[63,0]     -0.63      0.28     -0.57     -1.04     -0.18    309.62      1.01\n",
      "lams[63,1]      1.74      0.32      1.68      1.27      2.23    323.36      1.00\n",
      "lams[64,0]      1.62      0.65      1.47      0.75      2.53    318.82      1.00\n",
      "lams[64,1]      2.15      0.67      2.01      1.17      3.06    334.88      1.00\n",
      "lams[65,0]     -0.02      0.13     -0.00     -0.21      0.17    681.11      1.00\n",
      "lams[65,1]      1.72      0.30      1.67      1.27      2.21    617.29      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.04      0.17    517.27      1.00\n",
      "lams[66,1]      0.87      0.08      0.86      0.74      0.99    537.15      1.00\n",
      "lams[67,0]     -0.31      0.12     -0.30     -0.49     -0.09    507.87      1.00\n",
      "lams[67,1]      0.96      0.11      0.95      0.78      1.15    450.03      1.00\n",
      "lams[68,0]      0.10      0.05      0.11      0.02      0.19    747.01      1.01\n",
      "lams[68,1]      0.85      0.09      0.84      0.72      1.00    639.24      1.00\n",
      "lams[69,0]     -0.21      0.16     -0.19     -0.43      0.05    423.86      1.00\n",
      "lams[69,1]      1.12      0.16      1.10      0.86      1.35    420.70      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.24      0.50   1990.10      1.00\n",
      "lams[70,1]      1.58      0.25      1.54      1.15      1.94    653.86      1.00\n",
      "lams[71,0]     -0.85      0.30     -0.81     -1.33     -0.40    428.36      1.00\n",
      "lams[71,1]      1.84      0.33      1.81      1.34      2.34    402.56      1.00\n",
      "lams[72,0]      0.15      0.06      0.15      0.05      0.24    483.04      1.00\n",
      "lams[72,1]      0.99      0.11      0.98      0.81      1.14    458.64      1.00\n",
      "lams[73,0]     -1.07      0.34     -1.02     -1.63     -0.56    583.49      1.00\n",
      "lams[73,1]      1.23      0.20      1.20      0.94      1.59    568.16      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [57:29<00:00,  1.38s/it, 511 steps of size 8.42e-03. acc. prob=0.94]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -24853.27\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.11      2.14     10.00      6.77     13.66   1224.28      1.00\n",
      "  gamma[1]      8.56      1.79      8.39      5.76     11.58   1276.65      1.00\n",
      "  gamma[2]      9.90      2.10      9.77      6.40     13.01   1022.90      1.00\n",
      "  gamma[3]     14.96      1.88     14.93     11.86     17.85   1656.66      1.00\n",
      "  gamma[4]      8.97      1.89      8.73      6.04     12.00   1459.90      1.00\n",
      "  gamma[5]      9.56      2.17      9.43      6.09     13.09   1401.70      1.00\n",
      "  gamma[6]     10.54      2.12     10.47      7.03     13.82   1589.54      1.00\n",
      "  gamma[7]     10.74      2.27     10.64      7.15     14.39   1360.63      1.00\n",
      "  gamma[8]     10.33      2.28     10.14      6.60     13.83   1141.40      1.00\n",
      "  gamma[9]     10.41      2.21     10.27      6.81     13.88   1292.15      1.00\n",
      " gamma[10]      8.64      2.00      8.49      5.31     11.76   1153.80      1.00\n",
      " gamma[11]      9.59      2.05      9.38      6.55     13.07   1515.97      1.00\n",
      " gamma[12]      9.55      2.31      9.38      6.01     13.35   2271.71      1.00\n",
      " gamma[13]      9.43      2.07      9.34      5.96     12.66   1428.94      1.00\n",
      " gamma[14]      9.91      2.17      9.73      6.40     13.21    633.39      1.00\n",
      " gamma[15]      6.16      1.27      6.05      4.15      8.14    476.59      1.00\n",
      " gamma[16]      7.98      1.55      7.86      5.43     10.32   1562.00      1.00\n",
      " gamma[17]      9.33      0.99      9.33      7.73     10.90    509.42      1.00\n",
      " gamma[18]      8.20      1.22      8.14      6.17     10.00    671.29      1.00\n",
      " gamma[19]      4.89      0.87      4.86      3.48      6.25    558.38      1.00\n",
      " gamma[20]      9.96      2.15      9.80      6.51     13.42   1628.33      1.00\n",
      " gamma[21]      9.31      2.12      9.12      5.67     12.32    914.52      1.00\n",
      " gamma[22]      4.25      0.77      4.19      3.01      5.46    877.90      1.00\n",
      " gamma[23]      6.68      1.40      6.54      4.50      9.06    662.83      1.00\n",
      " gamma[24]      8.13      1.79      7.96      5.40     11.13    861.29      1.00\n",
      " gamma[25]      4.33      0.85      4.27      2.94      5.73    568.60      1.00\n",
      " gamma[26]      9.81      2.27      9.62      6.06     13.18    754.45      1.00\n",
      " gamma[27]      8.52      1.28      8.48      6.23     10.48    611.67      1.00\n",
      " gamma[28]      3.11      0.55      3.06      2.15      3.95    543.50      1.00\n",
      " gamma[29]      8.87      1.17      8.86      6.94     10.74    635.88      1.00\n",
      " gamma[30]      6.31      1.16      6.24      4.68      8.44    458.16      1.00\n",
      " gamma[31]      7.07      1.35      6.99      4.88      9.14    526.35      1.00\n",
      " gamma[32]     14.59      1.56     14.59     12.40     17.53    631.68      1.01\n",
      " gamma[33]      9.87      2.05      9.77      6.54     13.17   1685.06      1.00\n",
      " gamma[34]      3.81      0.73      3.75      2.70      5.03    571.43      1.00\n",
      " gamma[35]      5.79      0.97      5.78      4.33      7.44    581.07      1.00\n",
      " gamma[36]      7.74      1.30      7.66      5.54      9.64    651.52      1.00\n",
      " gamma[37]      6.88      1.49      6.79      4.57      9.37    457.55      1.00\n",
      " gamma[38]      4.24      0.72      4.20      3.01      5.37    446.92      1.00\n",
      " gamma[39]      2.32      0.48      2.30      1.51      3.00    504.79      1.00\n",
      " gamma[40]      3.40      0.56      3.41      2.52      4.29    861.46      1.00\n",
      " gamma[41]      8.30      1.56      8.23      5.53     10.51   1723.18      1.00\n",
      " gamma[42]      8.33      1.70      8.19      5.73     11.15   1734.56      1.00\n",
      " gamma[43]      8.00      1.75      7.91      4.93     10.50   1497.93      1.00\n",
      " gamma[44]      7.25      1.46      7.14      4.96      9.62   1404.42      1.00\n",
      " gamma[45]     10.02      2.28      9.83      6.56     13.87   1920.20      1.00\n",
      " gamma[46]      4.96      0.90      4.91      3.61      6.52   1752.72      1.00\n",
      " gamma[47]      4.50      0.79      4.47      3.25      5.77   1490.23      1.00\n",
      " gamma[48]      8.36      1.68      8.23      5.87     11.19   1116.04      1.00\n",
      " gamma[49]      7.76      1.55      7.71      5.34     10.16   1761.79      1.00\n",
      " gamma[50]      6.30      1.19      6.26      4.34      8.25    959.52      1.00\n",
      " gamma[51]      9.03      1.79      8.92      6.14     11.90   1504.16      1.00\n",
      " gamma[52]      7.84      1.40      7.77      5.63     10.17    445.03      1.00\n",
      " gamma[53]      9.83      1.19      9.79      8.00     11.88    700.87      1.00\n",
      " gamma[54]      9.58      1.66      9.54      6.38     11.86    671.84      1.00\n",
      " gamma[55]      5.60      1.11      5.54      3.97      7.53   1510.11      1.00\n",
      " gamma[56]      8.77      1.90      8.60      5.87     11.89   1140.63      1.00\n",
      " gamma[57]      4.86      0.76      4.80      3.50      5.98    982.95      1.00\n",
      " gamma[58]      6.95      1.47      6.83      4.65      9.36    820.79      1.00\n",
      " gamma[59]      7.62      1.33      7.57      5.53      9.86    666.15      1.00\n",
      " gamma[60]      3.59      0.68      3.58      2.52      4.74    547.98      1.00\n",
      " gamma[61]      9.32      1.63      9.29      6.40     11.82    422.97      1.00\n",
      " gamma[62]      6.03      0.86      6.00      4.60      7.43    506.46      1.00\n",
      " gamma[63]      5.34      1.04      5.29      3.72      7.04    484.15      1.00\n",
      " gamma[64]      8.61      1.93      8.46      5.03     11.30   1213.56      1.00\n",
      " gamma[65]      4.10      0.87      4.04      2.81      5.55    548.87      1.00\n",
      " gamma[66]      9.47      1.43      9.45      7.11     11.82    431.25      1.00\n",
      " gamma[67]      4.79      0.69      4.77      3.68      5.84    637.22      1.00\n",
      " gamma[68]      5.13      0.72      5.10      4.02      6.34    519.59      1.01\n",
      " gamma[69]      9.28      1.63      9.17      6.68     11.97    504.76      1.00\n",
      " gamma[70]      5.98      1.23      5.89      3.79      7.83    599.01      1.00\n",
      " gamma[71]      3.17      0.60      3.16      2.11      4.06    616.31      1.00\n",
      " gamma[72]      7.80      1.15      7.76      5.97      9.67    640.38      1.00\n",
      " gamma[73]      4.85      0.83      4.83      3.45      6.11    450.12      1.00\n",
      " lams[0,0]      1.60      0.71      1.44      0.66      2.76    457.84      1.00\n",
      " lams[0,1]      2.10      0.72      1.95      1.07      3.20    464.98      1.00\n",
      " lams[1,0]      1.66      0.78      1.47      0.60      2.76    507.20      1.00\n",
      " lams[1,1]      2.33      0.79      2.16      1.26      3.57    469.04      1.00\n",
      " lams[2,0]      1.50      0.80      1.34      0.49      2.68    398.11      1.00\n",
      " lams[2,1]      2.32      0.79      2.17      1.23      3.47    392.46      1.00\n",
      " lams[3,0]      7.93      0.90      7.96      6.55      9.41    786.79      1.00\n",
      " lams[3,1]      8.01      0.89      8.04      6.59      9.43    772.76      1.00\n",
      " lams[4,0]      1.67      0.65      1.53      0.76      2.54    578.61      1.00\n",
      " lams[4,1]      1.93      0.65      1.81      0.95      2.83    586.45      1.00\n",
      " lams[5,0]      1.72      0.77      1.52      0.72      2.91    444.29      1.00\n",
      " lams[5,1]      2.12      0.77      1.95      1.02      3.28    448.47      1.00\n",
      " lams[6,0]      1.71      0.71      1.58      0.66      2.71    502.18      1.00\n",
      " lams[6,1]      2.01      0.70      1.90      0.95      3.03    503.75      1.00\n",
      " lams[7,0]      1.63      0.76      1.43      0.65      2.79    432.52      1.00\n",
      " lams[7,1]      2.14      0.77      1.96      1.11      3.33    448.14      1.00\n",
      " lams[8,0]      1.58      0.76      1.39      0.54      2.64    402.50      1.00\n",
      " lams[8,1]      2.22      0.77      2.04      1.09      3.39    398.46      1.00\n",
      " lams[9,0]      1.54      0.70      1.39      0.61      2.62    535.80      1.00\n",
      " lams[9,1]      2.17      0.71      2.03      1.13      3.27    544.86      1.00\n",
      "lams[10,0]      1.53      0.69      1.35      0.56      2.49    427.83      1.00\n",
      "lams[10,1]      2.14      0.70      1.96      1.15      3.23    422.55      1.00\n",
      "lams[11,0]      1.62      0.68      1.46      0.72      2.66    393.65      1.00\n",
      "lams[11,1]      2.06      0.69      1.91      1.08      3.07    405.22      1.01\n",
      "lams[12,0]      1.67      0.70      1.52      0.69      2.71    398.90      1.00\n",
      "lams[12,1]      2.12      0.71      1.99      1.11      3.24    385.84      1.00\n",
      "lams[13,0]      1.33      0.70      1.15      0.41      2.38    549.43      1.00\n",
      "lams[13,1]      2.33      0.72      2.18      1.26      3.39    578.58      1.00\n",
      "lams[14,0]      0.70      0.15      0.67      0.47      0.91    371.57      1.00\n",
      "lams[14,1]      1.32      0.25      1.28      0.91      1.66    311.77      1.00\n",
      "lams[15,0]      1.01      0.25      0.96      0.67      1.37    296.91      1.00\n",
      "lams[15,1]      1.73      0.35      1.68      1.16      2.22    254.89      1.00\n",
      "lams[16,0]      1.75      0.66      1.61      0.84      2.69    382.50      1.00\n",
      "lams[16,1]      2.08      0.67      1.94      1.11      3.03    379.42      1.00\n",
      "lams[17,0]     -0.05      0.06     -0.05     -0.14      0.04    405.79      1.00\n",
      "lams[17,1]      0.62      0.05      0.61      0.54      0.69    381.56      1.00\n",
      "lams[18,0]      0.27      0.04      0.27      0.21      0.33    831.18      1.00\n",
      "lams[18,1]      0.83      0.08      0.82      0.72      0.95    671.74      1.00\n",
      "lams[19,0]     -0.25      0.14     -0.23     -0.46     -0.01    530.06      1.00\n",
      "lams[19,1]      1.24      0.18      1.22      0.95      1.51    470.39      1.00\n",
      "lams[20,0]      1.52      0.72      1.36      0.53      2.58    537.78      1.00\n",
      "lams[20,1]      2.18      0.73      2.05      1.10      3.26    556.09      1.00\n",
      "lams[21,0]      1.46      0.61      1.33      0.65      2.29    402.68      1.00\n",
      "lams[21,1]      1.89      0.63      1.75      1.04      2.82    396.01      1.00\n",
      "lams[22,0]      0.25      0.05      0.26      0.18      0.35    960.92      1.00\n",
      "lams[22,1]      1.07      0.16      1.06      0.84      1.30    694.08      1.00\n",
      "lams[23,0]      0.78      0.12      0.76      0.57      0.95    573.69      1.00\n",
      "lams[23,1]      1.30      0.22      1.27      0.95      1.62    503.55      1.00\n",
      "lams[24,0]      0.82      0.18      0.79      0.57      1.10    767.49      1.00\n",
      "lams[24,1]      1.46      0.28      1.41      1.03      1.90    570.13      1.00\n",
      "lams[25,0]      0.25      0.06      0.26      0.17      0.34   1092.01      1.00\n",
      "lams[25,1]      1.20      0.19      1.18      0.91      1.50    417.53      1.00\n",
      "lams[26,0]      0.85      0.16      0.82      0.60      1.11    530.06      1.00\n",
      "lams[26,1]      1.18      0.22      1.14      0.85      1.52    487.58      1.00\n",
      "lams[27,0]     -0.32      0.16     -0.29     -0.56     -0.08    371.78      1.00\n",
      "lams[27,1]      0.87      0.11      0.85      0.70      1.03    400.36      1.00\n",
      "lams[28,0]     -0.81      0.26     -0.78     -1.21     -0.40    464.47      1.00\n",
      "lams[28,1]      1.63      0.27      1.60      1.21      2.06    447.19      1.00\n",
      "lams[29,0]      0.06      0.07      0.06     -0.04      0.16    585.46      1.00\n",
      "lams[29,1]      0.79      0.07      0.78      0.68      0.89    563.86      1.00\n",
      "lams[30,0]     -0.09      0.13     -0.07     -0.27      0.11    356.15      1.00\n",
      "lams[30,1]      1.14      0.17      1.12      0.88      1.37    361.59      1.00\n",
      "lams[31,0]      0.55      0.06      0.54      0.46      0.64    553.35      1.00\n",
      "lams[31,1]      1.02      0.14      1.00      0.81      1.24    431.72      1.00\n",
      "lams[32,0]     -0.10      0.08     -0.09     -0.23      0.01    558.54      1.01\n",
      "lams[32,1]      0.53      0.04      0.53      0.46      0.59    513.66      1.02\n",
      "lams[33,0]      1.87      0.73      1.73      0.75      2.96    567.39      1.00\n",
      "lams[33,1]      2.08      0.73      1.96      0.92      3.16    578.18      1.00\n",
      "lams[34,0]      0.20      0.06      0.21      0.09      0.30    705.05      1.00\n",
      "lams[34,1]      1.20      0.18      1.18      0.92      1.49    494.52      1.00\n",
      "lams[35,0]      0.42      0.03      0.42      0.36      0.47    933.23      1.00\n",
      "lams[35,1]      0.88      0.11      0.87      0.72      1.03    427.70      1.00\n",
      "lams[36,0]     -0.35      0.18     -0.32     -0.62     -0.05    555.70      1.00\n",
      "lams[36,1]      1.07      0.15      1.06      0.85      1.32    598.28      1.00\n",
      "lams[37,0]     -0.58      0.31     -0.52     -1.09     -0.16    250.85      1.00\n",
      "lams[37,1]      1.50      0.30      1.44      1.09      1.98    245.24      1.00\n",
      "lams[38,0]      0.06      0.08      0.06     -0.06      0.18    484.13      1.00\n",
      "lams[38,1]      1.08      0.15      1.06      0.84      1.29    443.27      1.00\n",
      "lams[39,0]     -0.40      0.20     -0.38     -0.71     -0.07    493.86      1.00\n",
      "lams[39,1]      1.95      0.36      1.91      1.32      2.48    438.08      1.00\n",
      "lams[40,0]      0.16      0.06      0.16      0.06      0.25   1099.08      1.00\n",
      "lams[40,1]      1.08      0.15      1.05      0.86      1.32    749.91      1.00\n",
      "lams[41,0]      1.69      0.70      1.51      0.70      2.69    330.31      1.00\n",
      "lams[41,1]      2.25      0.71      2.09      1.14      3.21    343.11      1.00\n",
      "lams[42,0]      1.56      0.70      1.40      0.65      2.52    279.44      1.00\n",
      "lams[42,1]      2.22      0.72      2.08      1.19      3.18    278.50      1.00\n",
      "lams[43,0]      1.82      0.85      1.63      0.70      3.17    545.58      1.00\n",
      "lams[43,1]      2.72      0.86      2.58      1.35      3.91    575.62      1.00\n",
      "lams[44,0]      1.64      0.79      1.42      0.61      2.83    311.40      1.00\n",
      "lams[44,1]      2.64      0.81      2.47      1.54      4.01    293.21      1.00\n",
      "lams[45,0]      1.76      0.71      1.60      0.78      2.76    467.19      1.00\n",
      "lams[45,1]      2.09      0.72      1.94      1.09      3.11    472.25      1.00\n",
      "lams[46,0]      1.99      0.62      1.88      1.02      2.83    589.94      1.00\n",
      "lams[46,1]      2.63      0.65      2.54      1.60      3.52    550.75      1.00\n",
      "lams[47,0]      2.19      0.66      2.06      1.21      3.25    484.76      1.00\n",
      "lams[47,1]      2.74      0.67      2.61      1.71      3.84    478.24      1.00\n",
      "lams[48,0]      1.60      0.67      1.47      0.68      2.51    437.99      1.00\n",
      "lams[48,1]      2.15      0.69      2.02      1.15      3.08    444.44      1.00\n",
      "lams[49,0]      1.49      0.63      1.34      0.66      2.41    550.76      1.00\n",
      "lams[49,1]      2.36      0.65      2.24      1.41      3.31    547.63      1.00\n",
      "lams[50,0]      1.72      0.60      1.62      0.87      2.60    406.92      1.00\n",
      "lams[50,1]      2.39      0.63      2.30      1.44      3.33    393.00      1.00\n",
      "lams[51,0]      1.45      0.67      1.34      0.52      2.42    494.91      1.01\n",
      "lams[51,1]      2.37      0.70      2.26      1.30      3.39    506.95      1.01\n",
      "lams[52,0]     -0.29      0.18     -0.27     -0.55     -0.02    293.44      1.00\n",
      "lams[52,1]      1.23      0.18      1.21      0.98      1.53    320.19      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.27    711.81      1.00\n",
      "lams[53,1]      0.69      0.05      0.68      0.62      0.76    633.51      1.00\n",
      "lams[54,0]      0.08      0.08      0.09     -0.04      0.22    569.18      1.00\n",
      "lams[54,1]      1.05      0.12      1.04      0.85      1.23    563.10      1.00\n",
      "lams[55,0]      1.83      0.73      1.68      0.84      2.91    417.00      1.00\n",
      "lams[55,1]      2.81      0.76      2.69      1.68      3.87    409.34      1.00\n",
      "lams[56,0]      1.51      0.70      1.37      0.55      2.51    364.03      1.00\n",
      "lams[56,1]      2.35      0.70      2.23      1.28      3.39    369.92      1.00\n",
      "lams[57,0]      0.20      0.05      0.20      0.10      0.27   1234.69      1.00\n",
      "lams[57,1]      1.03      0.12      1.02      0.85      1.21    891.97      1.00\n",
      "lams[58,0]      0.52      0.05      0.52      0.44      0.61    831.53      1.00\n",
      "lams[58,1]      1.16      0.18      1.14      0.89      1.42    609.35      1.00\n",
      "lams[59,0]      0.27      0.05      0.28      0.18      0.36    777.77      1.00\n",
      "lams[59,1]      1.14      0.14      1.12      0.92      1.34    566.28      1.00\n",
      "lams[60,0]     -0.89      0.31     -0.86     -1.36     -0.37    509.59      1.00\n",
      "lams[60,1]      1.80      0.32      1.76      1.34      2.32    482.36      1.00\n",
      "lams[61,0]     -0.39      0.21     -0.36     -0.72     -0.09    362.75      1.00\n",
      "lams[61,1]      1.11      0.17      1.08      0.86      1.37    350.18      1.00\n",
      "lams[62,0]     -0.27      0.12     -0.26     -0.46     -0.08    437.10      1.00\n",
      "lams[62,1]      0.93      0.11      0.91      0.77      1.09    421.39      1.00\n",
      "lams[63,0]     -0.61      0.26     -0.57     -1.00     -0.22    384.12      1.00\n",
      "lams[63,1]      1.72      0.30      1.67      1.30      2.20    376.53      1.00\n",
      "lams[64,0]      1.65      0.65      1.54      0.72      2.62    554.92      1.00\n",
      "lams[64,1]      2.18      0.67      2.06      1.19      3.22    575.02      1.00\n",
      "lams[65,0]     -0.03      0.13     -0.01     -0.22      0.18    660.88      1.00\n",
      "lams[65,1]      1.74      0.31      1.71      1.23      2.18    435.49      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.06      0.18    358.73      1.00\n",
      "lams[66,1]      0.87      0.08      0.86      0.74      1.00    356.93      1.00\n",
      "lams[67,0]     -0.32      0.13     -0.30     -0.52     -0.12    463.00      1.00\n",
      "lams[67,1]      0.97      0.11      0.95      0.79      1.13    514.73      1.00\n",
      "lams[68,0]      0.10      0.05      0.11      0.03      0.18    532.28      1.00\n",
      "lams[68,1]      0.86      0.09      0.85      0.73      1.00    470.02      1.01\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.42      0.04    392.08      1.00\n",
      "lams[69,1]      1.11      0.15      1.09      0.91      1.35    376.81      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.25      0.51   1243.21      1.00\n",
      "lams[70,1]      1.60      0.26      1.56      1.15      1.99    509.62      1.00\n",
      "lams[71,0]     -0.83      0.29     -0.79     -1.28     -0.40    440.23      1.00\n",
      "lams[71,1]      1.84      0.32      1.79      1.37      2.34    454.82      1.00\n",
      "lams[72,0]      0.15      0.06      0.15      0.05      0.23    631.87      1.00\n",
      "lams[72,1]      0.99      0.10      0.98      0.84      1.14    526.71      1.00\n",
      "lams[73,0]     -1.09      0.34     -1.03     -1.58     -0.53    411.39      1.00\n",
      "lams[73,1]      1.24      0.21      1.21      0.91      1.54    424.58      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [59:13<00:00,  1.42s/it, 511 steps of size 8.69e-03. acc. prob=0.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -25117.53\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.11      2.30      9.93      6.60     14.08   2568.99      1.00\n",
      "  gamma[1]      8.40      1.86      8.29      5.48     11.41   1647.06      1.00\n",
      "  gamma[2]      9.96      2.25      9.76      6.69     13.80   1554.44      1.00\n",
      "  gamma[3]     15.22      2.16     15.16     11.94     19.09   1697.25      1.00\n",
      "  gamma[4]      9.01      1.84      8.87      5.98     11.73   1288.02      1.00\n",
      "  gamma[5]      9.55      2.06      9.51      6.03     12.59   1840.04      1.00\n",
      "  gamma[6]     10.83      2.24     10.74      7.52     14.75   1557.70      1.00\n",
      "  gamma[7]     10.95      2.44     10.75      6.95     14.75   1939.37      1.00\n",
      "  gamma[8]     10.36      2.26     10.22      6.64     13.88   1660.74      1.00\n",
      "  gamma[9]     10.25      2.30     10.04      6.79     14.13   1695.31      1.00\n",
      " gamma[10]      8.39      1.95      8.26      5.12     11.28   1258.02      1.00\n",
      " gamma[11]      9.52      2.15      9.40      6.01     12.79   1784.90      1.00\n",
      " gamma[12]      9.64      2.12      9.46      6.48     12.95   1319.94      1.00\n",
      " gamma[13]      9.71      2.13      9.49      6.55     13.26   1764.05      1.00\n",
      " gamma[14]     10.10      2.24      9.96      6.40     13.55    811.91      1.00\n",
      " gamma[15]      6.06      1.28      5.94      3.92      8.15    929.79      1.00\n",
      " gamma[16]      7.71      1.56      7.63      5.57     10.59   1634.49      1.00\n",
      " gamma[17]      8.45      0.98      8.41      6.91     10.04    662.12      1.00\n",
      " gamma[18]      7.72      1.21      7.62      5.85      9.70    816.95      1.00\n",
      " gamma[19]      4.76      0.85      4.73      3.40      6.13    598.53      1.00\n",
      " gamma[20]      9.90      2.18      9.70      6.27     13.20   1735.83      1.00\n",
      " gamma[21]      9.47      2.00      9.38      6.28     12.64   1391.64      1.00\n",
      " gamma[22]      4.04      0.76      3.99      2.88      5.45    675.68      1.00\n",
      " gamma[23]      6.71      1.45      6.60      4.28      8.97    708.05      1.00\n",
      " gamma[24]      8.31      1.78      8.13      5.50     11.31    711.65      1.00\n",
      " gamma[25]      4.31      0.87      4.28      2.76      5.66    745.93      1.00\n",
      " gamma[26]      9.76      2.17      9.59      6.12     12.98    811.12      1.00\n",
      " gamma[27]      7.83      1.13      7.77      5.94      9.54    591.45      1.01\n",
      " gamma[28]      2.98      0.58      2.94      2.04      3.92    702.96      1.00\n",
      " gamma[29]      8.24      1.15      8.20      6.46     10.16    750.55      1.00\n",
      " gamma[30]      6.04      1.08      6.04      4.23      7.63    620.38      1.00\n",
      " gamma[31]      7.00      1.28      6.95      4.86      9.03    819.69      1.00\n",
      " gamma[32]     11.89      1.49     11.85      9.51     14.39    789.00      1.00\n",
      " gamma[33]      9.79      2.02      9.59      6.70     13.12   1699.60      1.00\n",
      " gamma[34]      3.77      0.68      3.75      2.55      4.78    770.27      1.00\n",
      " gamma[35]      5.88      0.97      5.88      4.38      7.55    747.99      1.01\n",
      " gamma[36]      6.96      1.17      6.93      5.17      8.97    503.01      1.00\n",
      " gamma[37]      6.70      1.46      6.59      4.23      8.91    520.57      1.01\n",
      " gamma[38]      4.09      0.71      4.07      2.94      5.18   1089.26      1.00\n",
      " gamma[39]      2.26      0.45      2.21      1.53      3.01    775.87      1.01\n",
      " gamma[40]      3.53      0.61      3.49      2.66      4.64    591.97      1.00\n",
      " gamma[41]      8.29      1.53      8.26      6.00     11.05   1612.72      1.00\n",
      " gamma[42]      8.39      1.67      8.34      5.59     10.83   1589.76      1.00\n",
      " gamma[43]      7.87      1.68      7.69      5.15     10.52   1828.36      1.00\n",
      " gamma[44]      7.20      1.58      7.12      4.55      9.50   1533.61      1.00\n",
      " gamma[45]      9.98      2.23      9.86      6.61     13.55   1728.88      1.00\n",
      " gamma[46]      4.95      0.95      4.88      3.44      6.53   1820.67      1.00\n",
      " gamma[47]      4.57      0.81      4.54      3.21      5.83   1653.39      1.00\n",
      " gamma[48]      8.31      1.75      8.19      5.14     10.71   1083.25      1.00\n",
      " gamma[49]      7.80      1.58      7.74      5.26     10.22   1446.19      1.00\n",
      " gamma[50]      6.31      1.18      6.20      4.31      8.14   1558.16      1.00\n",
      " gamma[51]      9.08      1.77      9.01      5.96     11.67   1376.81      1.00\n",
      " gamma[52]      7.84      1.37      7.78      5.66     10.18    717.96      1.00\n",
      " gamma[53]      9.83      1.14      9.79      8.08     11.82    692.93      1.00\n",
      " gamma[54]      9.56      1.77      9.52      6.48     12.14    748.20      1.00\n",
      " gamma[55]      5.68      1.11      5.60      3.98      7.35   1611.17      1.00\n",
      " gamma[56]      8.83      1.90      8.76      5.58     11.67   2204.58      1.00\n",
      " gamma[57]      4.82      0.81      4.75      3.39      6.01    724.05      1.01\n",
      " gamma[58]      7.02      1.39      6.91      4.83      9.31    782.74      1.00\n",
      " gamma[59]      7.58      1.34      7.50      5.43      9.80    739.88      1.00\n",
      " gamma[60]      3.57      0.68      3.54      2.47      4.73    619.64      1.00\n",
      " gamma[61]      9.15      1.59      9.14      6.48     11.64    632.17      1.00\n",
      " gamma[62]      6.02      0.91      6.02      4.53      7.50    568.85      1.00\n",
      " gamma[63]      5.32      1.01      5.27      3.71      6.99    592.09      1.00\n",
      " gamma[64]      8.72      1.83      8.57      5.63     11.74   1790.96      1.00\n",
      " gamma[65]      4.12      0.88      4.04      2.72      5.51    717.89      1.00\n",
      " gamma[66]      9.54      1.42      9.52      7.10     11.91    544.08      1.00\n",
      " gamma[67]      4.87      0.66      4.86      3.86      5.98    811.78      1.00\n",
      " gamma[68]      5.07      0.77      5.07      3.71      6.27    716.72      1.00\n",
      " gamma[69]      9.38      1.64      9.32      6.47     11.82    730.18      1.00\n",
      " gamma[70]      6.10      1.26      6.00      3.98      7.93   1075.95      1.00\n",
      " gamma[71]      3.15      0.60      3.11      2.17      4.12    625.43      1.01\n",
      " gamma[72]      7.79      1.23      7.77      5.49      9.53    513.48      1.00\n",
      " gamma[73]      4.94      0.79      4.91      3.46      6.04    752.10      1.00\n",
      " lams[0,0]      1.63      0.75      1.44      0.64      2.75    467.05      1.00\n",
      " lams[0,1]      2.11      0.75      1.95      0.98      3.15    479.79      1.00\n",
      " lams[1,0]      1.69      0.77      1.52      0.61      2.80    586.60      1.00\n",
      " lams[1,1]      2.36      0.78      2.20      1.26      3.52    572.30      1.00\n",
      " lams[2,0]      1.58      0.83      1.39      0.39      2.78    566.31      1.00\n",
      " lams[2,1]      2.36      0.82      2.17      1.18      3.60    548.89      1.00\n",
      " lams[3,0]      7.90      0.90      7.88      6.53      9.41    943.75      1.00\n",
      " lams[3,1]      7.98      0.89      7.97      6.66      9.50    938.63      1.00\n",
      " lams[4,0]      1.68      0.65      1.54      0.78      2.61    490.83      1.00\n",
      " lams[4,1]      1.93      0.65      1.80      1.02      2.88    489.83      1.00\n",
      " lams[5,0]      1.66      0.68      1.52      0.68      2.61    537.70      1.00\n",
      " lams[5,1]      2.05      0.69      1.92      1.05      3.03    535.35      1.00\n",
      " lams[6,0]      1.71      0.72      1.57      0.71      2.81    433.85      1.01\n",
      " lams[6,1]      2.00      0.72      1.86      0.96      3.08    435.63      1.01\n",
      " lams[7,0]      1.56      0.71      1.40      0.62      2.61    474.38      1.00\n",
      " lams[7,1]      2.05      0.71      1.91      1.08      3.15    488.15      1.00\n",
      " lams[8,0]      1.56      0.73      1.39      0.55      2.66    649.49      1.00\n",
      " lams[8,1]      2.17      0.72      2.00      1.12      3.36    682.25      1.00\n",
      " lams[9,0]      1.58      0.73      1.42      0.61      2.75    576.87      1.00\n",
      " lams[9,1]      2.20      0.73      2.05      1.13      3.38    576.08      1.00\n",
      "lams[10,0]      1.54      0.68      1.37      0.62      2.48    477.44      1.01\n",
      "lams[10,1]      2.14      0.71      1.99      1.15      3.18    479.37      1.01\n",
      "lams[11,0]      1.65      0.71      1.48      0.76      2.78    421.56      1.00\n",
      "lams[11,1]      2.09      0.72      1.92      1.14      3.25    422.13      1.00\n",
      "lams[12,0]      1.67      0.71      1.52      0.66      2.70    566.44      1.00\n",
      "lams[12,1]      2.10      0.71      1.98      1.05      3.15    581.40      1.00\n",
      "lams[13,0]      1.47      0.80      1.27      0.37      2.56    348.80      1.00\n",
      "lams[13,1]      2.36      0.79      2.21      1.19      3.45    325.06      1.00\n",
      "lams[14,0]      0.70      0.15      0.67      0.49      0.92    562.59      1.00\n",
      "lams[14,1]      1.29      0.24      1.26      0.92      1.64    519.43      1.00\n",
      "lams[15,0]      1.03      0.24      0.99      0.67      1.36    653.16      1.00\n",
      "lams[15,1]      1.74      0.34      1.70      1.22      2.26    564.47      1.00\n",
      "lams[16,0]      1.72      0.63      1.60      0.88      2.72    469.77      1.00\n",
      "lams[16,1]      2.06      0.64      1.93      1.10      3.02    465.91      1.00\n",
      "lams[17,0]     -0.09      0.07     -0.08     -0.19      0.02    560.99      1.00\n",
      "lams[17,1]      0.65      0.06      0.65      0.57      0.74    554.09      1.00\n",
      "lams[18,0]      0.27      0.04      0.27      0.21      0.33    909.80      1.00\n",
      "lams[18,1]      0.85      0.08      0.84      0.73      1.00    723.27      1.00\n",
      "lams[19,0]     -0.25      0.14     -0.24     -0.46     -0.01    422.70      1.00\n",
      "lams[19,1]      1.25      0.18      1.23      0.97      1.54    447.74      1.00\n",
      "lams[20,0]      1.53      0.72      1.34      0.57      2.64    440.61      1.00\n",
      "lams[20,1]      2.18      0.73      2.01      1.05      3.24    443.17      1.00\n",
      "lams[21,0]      1.49      0.59      1.36      0.69      2.35    543.24      1.00\n",
      "lams[21,1]      1.90      0.60      1.78      1.04      2.79    542.86      1.00\n",
      "lams[22,0]      0.26      0.05      0.26      0.18      0.35   1271.44      1.00\n",
      "lams[22,1]      1.10      0.17      1.08      0.85      1.36    463.80      1.00\n",
      "lams[23,0]      0.79      0.12      0.77      0.59      0.96    560.56      1.00\n",
      "lams[23,1]      1.30      0.22      1.27      0.93      1.60    548.06      1.00\n",
      "lams[24,0]      0.82      0.19      0.78      0.54      1.09    517.28      1.00\n",
      "lams[24,1]      1.43      0.28      1.39      0.99      1.83    443.65      1.00\n",
      "lams[25,0]      0.26      0.06      0.27      0.18      0.36   1151.31      1.00\n",
      "lams[25,1]      1.20      0.21      1.17      0.87      1.48    551.78      1.00\n",
      "lams[26,0]      0.85      0.17      0.81      0.60      1.09    374.96      1.00\n",
      "lams[26,1]      1.17      0.23      1.13      0.83      1.50    403.59      1.00\n",
      "lams[27,0]     -0.34      0.16     -0.32     -0.57     -0.10    546.49      1.01\n",
      "lams[27,1]      0.89      0.11      0.88      0.72      1.07    533.37      1.01\n",
      "lams[28,0]     -0.83      0.30     -0.79     -1.26     -0.38    520.22      1.00\n",
      "lams[28,1]      1.67      0.31      1.64      1.19      2.10    505.09      1.00\n",
      "lams[29,0]      0.05      0.07      0.05     -0.07      0.15    703.34      1.00\n",
      "lams[29,1]      0.81      0.08      0.81      0.70      0.94    718.28      1.00\n",
      "lams[30,0]     -0.08      0.12     -0.07     -0.26      0.09    526.57      1.00\n",
      "lams[30,1]      1.15      0.17      1.13      0.92      1.43    525.44      1.00\n",
      "lams[31,0]      0.56      0.06      0.55      0.46      0.64    841.00      1.00\n",
      "lams[31,1]      1.02      0.13      1.00      0.80      1.22    712.42      1.00\n",
      "lams[32,0]     -0.22      0.12     -0.20     -0.40     -0.04    611.15      1.00\n",
      "lams[32,1]      0.61      0.06      0.60      0.50      0.70    656.19      1.00\n",
      "lams[33,0]      1.77      0.69      1.62      0.77      2.76    694.23      1.00\n",
      "lams[33,1]      1.99      0.69      1.84      0.95      2.98    695.31      1.00\n",
      "lams[34,0]      0.22      0.06      0.22      0.13      0.32    996.15      1.00\n",
      "lams[34,1]      1.19      0.18      1.17      0.93      1.47    666.19      1.00\n",
      "lams[35,0]      0.42      0.04      0.42      0.37      0.48   1469.37      1.00\n",
      "lams[35,1]      0.87      0.10      0.85      0.71      1.02    709.82      1.01\n",
      "lams[36,0]     -0.38      0.20     -0.36     -0.65     -0.07    414.76      1.00\n",
      "lams[36,1]      1.12      0.17      1.10      0.87      1.37    403.84      1.00\n",
      "lams[37,0]     -0.50      0.28     -0.45     -0.90     -0.06    377.79      1.02\n",
      "lams[37,1]      1.45      0.28      1.40      1.03      1.88    391.55      1.02\n",
      "lams[38,0]      0.05      0.08      0.06     -0.07      0.18    797.35      1.00\n",
      "lams[38,1]      1.10      0.16      1.09      0.86      1.33    682.06      1.00\n",
      "lams[39,0]     -0.37      0.19     -0.36     -0.67     -0.09    660.56      1.00\n",
      "lams[39,1]      1.95      0.35      1.91      1.41      2.46    549.71      1.01\n",
      "lams[40,0]      0.15      0.07      0.16      0.06      0.26    876.88      1.00\n",
      "lams[40,1]      1.05      0.15      1.04      0.81      1.26    569.37      1.00\n",
      "lams[41,0]      1.72      0.71      1.60      0.73      2.82    339.81      1.00\n",
      "lams[41,1]      2.28      0.72      2.17      1.26      3.45    362.25      1.00\n",
      "lams[42,0]      1.50      0.62      1.37      0.67      2.42    373.35      1.00\n",
      "lams[42,1]      2.16      0.65      2.04      1.19      3.06    342.24      1.00\n",
      "lams[43,0]      1.76      0.82      1.57      0.66      2.96    466.83      1.00\n",
      "lams[43,1]      2.68      0.82      2.52      1.44      3.81    470.31      1.00\n",
      "lams[44,0]      1.59      0.70      1.46      0.60      2.55    495.47      1.00\n",
      "lams[44,1]      2.61      0.71      2.49      1.51      3.64    520.36      1.00\n",
      "lams[45,0]      1.80      0.73      1.67      0.78      2.91    510.98      1.00\n",
      "lams[45,1]      2.12      0.74      2.02      1.13      3.30    533.29      1.00\n",
      "lams[46,0]      1.99      0.62      1.87      1.08      2.87    575.05      1.00\n",
      "lams[46,1]      2.63      0.66      2.53      1.62      3.58    575.01      1.01\n",
      "lams[47,0]      2.13      0.62      2.03      1.17      3.03    539.55      1.00\n",
      "lams[47,1]      2.66      0.64      2.57      1.67      3.60    547.16      1.00\n",
      "lams[48,0]      1.52      0.57      1.39      0.74      2.36    738.77      1.00\n",
      "lams[48,1]      2.08      0.59      1.97      1.23      2.99    708.53      1.00\n",
      "lams[49,0]      1.47      0.61      1.34      0.57      2.37    529.62      1.00\n",
      "lams[49,1]      2.34      0.64      2.22      1.34      3.32    539.38      1.00\n",
      "lams[50,0]      1.72      0.57      1.62      0.92      2.64    673.44      1.00\n",
      "lams[50,1]      2.39      0.60      2.30      1.51      3.33    636.62      1.00\n",
      "lams[51,0]      1.46      0.73      1.30      0.50      2.45    461.80      1.00\n",
      "lams[51,1]      2.38      0.75      2.22      1.37      3.43    463.65      1.00\n",
      "lams[52,0]     -0.31      0.17     -0.28     -0.56     -0.02    581.04      1.00\n",
      "lams[52,1]      1.24      0.18      1.22      0.95      1.48    533.96      1.00\n",
      "lams[53,0]      0.21      0.03      0.22      0.16      0.27    868.04      1.00\n",
      "lams[53,1]      0.69      0.04      0.68      0.61      0.75    659.95      1.00\n",
      "lams[54,0]      0.08      0.09      0.09     -0.06      0.21    333.83      1.00\n",
      "lams[54,1]      1.06      0.14      1.03      0.86      1.27    348.28      1.00\n",
      "lams[55,0]      1.73      0.68      1.58      0.76      2.70    294.70      1.00\n",
      "lams[55,1]      2.70      0.71      2.58      1.48      3.64    319.00      1.00\n",
      "lams[56,0]      1.48      0.67      1.32      0.53      2.43    547.02      1.01\n",
      "lams[56,1]      2.31      0.68      2.19      1.30      3.27    542.65      1.01\n",
      "lams[57,0]      0.19      0.05      0.20      0.11      0.28   1034.29      1.01\n",
      "lams[57,1]      1.04      0.13      1.03      0.83      1.23    735.42      1.01\n",
      "lams[58,0]      0.52      0.05      0.51      0.44      0.61    894.48      1.00\n",
      "lams[58,1]      1.15      0.16      1.12      0.91      1.39    571.50      1.00\n",
      "lams[59,0]      0.27      0.05      0.28      0.19      0.35    972.96      1.00\n",
      "lams[59,1]      1.14      0.14      1.12      0.92      1.35    603.07      1.00\n",
      "lams[60,0]     -0.91      0.32     -0.86     -1.40     -0.43    528.60      1.00\n",
      "lams[60,1]      1.82      0.33      1.78      1.30      2.33    502.15      1.00\n",
      "lams[61,0]     -0.42      0.22     -0.39     -0.74     -0.09    436.84      1.00\n",
      "lams[61,1]      1.13      0.17      1.10      0.87      1.39    431.72      1.00\n",
      "lams[62,0]     -0.28      0.13     -0.26     -0.48     -0.07    491.82      1.00\n",
      "lams[62,1]      0.93      0.11      0.92      0.77      1.11    445.01      1.00\n",
      "lams[63,0]     -0.62      0.27     -0.59     -1.02     -0.21    366.83      1.00\n",
      "lams[63,1]      1.73      0.29      1.70      1.24      2.13    359.12      1.00\n",
      "lams[64,0]      1.64      0.65      1.48      0.71      2.63    612.99      1.00\n",
      "lams[64,1]      2.16      0.66      2.02      1.16      3.17    609.35      1.00\n",
      "lams[65,0]     -0.03      0.13     -0.01     -0.26      0.15    616.34      1.00\n",
      "lams[65,1]      1.74      0.30      1.71      1.24      2.18    614.44      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.04      0.18    566.37      1.00\n",
      "lams[66,1]      0.87      0.08      0.85      0.73      0.98    522.58      1.00\n",
      "lams[67,0]     -0.30      0.11     -0.29     -0.47     -0.12    716.79      1.00\n",
      "lams[67,1]      0.95      0.10      0.94      0.79      1.10    786.04      1.00\n",
      "lams[68,0]      0.10      0.06      0.10      0.00      0.18    472.81      1.00\n",
      "lams[68,1]      0.86      0.10      0.85      0.73      1.03    600.56      1.00\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.40      0.05    524.19      1.00\n",
      "lams[69,1]      1.11      0.15      1.09      0.89      1.33    547.29      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.25      0.49   1281.37      1.00\n",
      "lams[70,1]      1.57      0.25      1.53      1.18      1.98    866.92      1.00\n",
      "lams[71,0]     -0.84      0.28     -0.80     -1.29     -0.39    519.33      1.01\n",
      "lams[71,1]      1.85      0.31      1.80      1.39      2.38    461.31      1.01\n",
      "lams[72,0]      0.14      0.06      0.15      0.05      0.24    480.71      1.00\n",
      "lams[72,1]      0.99      0.11      0.98      0.84      1.17    411.15      1.00\n",
      "lams[73,0]     -1.05      0.33     -1.00     -1.56     -0.53    612.28      1.00\n",
      "lams[73,1]      1.22      0.20      1.18      0.90      1.51    622.55      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [56:43<00:00,  1.36s/it, 511 steps of size 9.21e-03. acc. prob=0.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -25352.72\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.08      2.27      9.92      6.63     13.86   1621.71      1.00\n",
      "  gamma[1]      8.44      1.82      8.29      5.27     11.13   2367.83      1.00\n",
      "  gamma[2]      9.96      2.16      9.78      6.43     13.27   1320.31      1.00\n",
      "  gamma[3]     15.43      2.08     15.33     12.20     18.93   1774.36      1.00\n",
      "  gamma[4]      8.99      1.86      8.89      5.94     11.71   1650.46      1.00\n",
      "  gamma[5]      9.64      2.16      9.40      5.98     12.93   2011.73      1.00\n",
      "  gamma[6]     10.95      2.25     10.83      7.52     14.70   1582.84      1.00\n",
      "  gamma[7]     10.94      2.24     10.79      7.47     14.69   1849.19      1.00\n",
      "  gamma[8]     10.39      2.29     10.23      6.78     14.11   2168.08      1.00\n",
      "  gamma[9]     10.30      2.30     10.17      6.67     14.04   1945.32      1.00\n",
      " gamma[10]      8.41      1.89      8.26      5.63     11.60   1781.90      1.00\n",
      " gamma[11]      9.57      2.09      9.38      5.77     12.40   1878.46      1.00\n",
      " gamma[12]      9.59      2.17      9.39      5.93     13.01   1845.75      1.00\n",
      " gamma[13]      9.81      2.24      9.61      5.92     13.19   2064.35      1.00\n",
      " gamma[14]     10.37      2.33     10.17      6.82     14.33   1193.95      1.00\n",
      " gamma[15]      5.92      1.31      5.80      3.54      7.79    790.17      1.00\n",
      " gamma[16]      7.63      1.47      7.61      5.02      9.62   1665.20      1.00\n",
      " gamma[17]      7.92      0.87      7.91      6.52      9.31    837.02      1.00\n",
      " gamma[18]      7.27      1.09      7.23      5.32      8.86   1203.45      1.00\n",
      " gamma[19]      4.46      0.82      4.43      3.01      5.66    462.28      1.01\n",
      " gamma[20]     10.01      2.27      9.81      6.59     13.91   1795.60      1.00\n",
      " gamma[21]      9.44      2.20      9.27      5.97     12.90   1590.89      1.00\n",
      " gamma[22]      3.90      0.73      3.89      2.84      5.23    658.18      1.00\n",
      " gamma[23]      6.68      1.47      6.57      4.16      9.00    729.95      1.00\n",
      " gamma[24]      8.37      1.91      8.21      5.27     11.26    735.33      1.00\n",
      " gamma[25]      4.34      0.80      4.30      2.99      5.52   1156.53      1.00\n",
      " gamma[26]      9.73      2.07      9.50      6.37     12.95   1156.34      1.00\n",
      " gamma[27]      7.08      1.04      7.06      5.47      8.93    593.39      1.00\n",
      " gamma[28]      2.84      0.54      2.82      1.93      3.62    749.90      1.00\n",
      " gamma[29]      7.73      1.08      7.70      5.84      9.37    956.68      1.00\n",
      " gamma[30]      5.77      0.99      5.77      4.03      7.32    766.90      1.00\n",
      " gamma[31]      6.94      1.27      6.91      4.90      8.94    942.08      1.00\n",
      " gamma[32]     10.02      1.39     10.04      7.72     12.22    277.43      1.00\n",
      " gamma[33]      9.65      2.09      9.51      6.58     13.27   1666.78      1.00\n",
      " gamma[34]      3.68      0.68      3.63      2.56      4.70    905.14      1.00\n",
      " gamma[35]      5.82      0.97      5.75      4.29      7.47   1227.85      1.00\n",
      " gamma[36]      6.65      1.04      6.63      4.83      8.33    724.70      1.00\n",
      " gamma[37]      6.56      1.30      6.49      4.35      8.53    631.43      1.00\n",
      " gamma[38]      3.97      0.70      3.99      2.81      5.04    811.55      1.00\n",
      " gamma[39]      2.23      0.45      2.22      1.49      2.95    573.14      1.00\n",
      " gamma[40]      3.67      0.66      3.64      2.66      4.86    925.27      1.00\n",
      " gamma[41]      8.36      1.60      8.26      5.57     10.78   1803.86      1.00\n",
      " gamma[42]      8.41      1.73      8.24      5.34     10.93   1870.26      1.00\n",
      " gamma[43]      7.78      1.62      7.63      4.94     10.12   2642.74      1.00\n",
      " gamma[44]      7.19      1.47      7.03      4.79      9.57   1771.68      1.00\n",
      " gamma[45]      9.94      2.28      9.75      6.41     13.65   1858.02      1.00\n",
      " gamma[46]      4.91      0.92      4.86      3.35      6.36   1313.73      1.00\n",
      " gamma[47]      4.55      0.79      4.48      3.34      5.88   1422.27      1.00\n",
      " gamma[48]      8.41      1.67      8.35      5.68     11.02   1468.26      1.00\n",
      " gamma[49]      7.82      1.51      7.72      5.23     10.06   1436.93      1.00\n",
      " gamma[50]      6.43      1.23      6.33      4.31      8.21   1763.99      1.00\n",
      " gamma[51]      9.08      1.89      9.02      6.27     12.20   1452.32      1.00\n",
      " gamma[52]      7.90      1.44      7.79      5.31     10.00    720.47      1.00\n",
      " gamma[53]      9.77      1.19      9.74      7.81     11.63    950.91      1.00\n",
      " gamma[54]      9.57      1.63      9.58      7.34     12.67    568.70      1.00\n",
      " gamma[55]      5.71      1.19      5.61      3.96      7.85   1578.96      1.00\n",
      " gamma[56]      8.81      1.96      8.74      5.37     11.67   1969.29      1.00\n",
      " gamma[57]      4.75      0.79      4.74      3.43      6.05    612.66      1.00\n",
      " gamma[58]      6.97      1.38      6.85      4.74      9.18   1194.20      1.00\n",
      " gamma[59]      7.62      1.36      7.56      5.33      9.72    575.81      1.00\n",
      " gamma[60]      3.59      0.66      3.56      2.44      4.61    530.84      1.00\n",
      " gamma[61]      9.20      1.57      9.14      6.88     11.98    785.55      1.00\n",
      " gamma[62]      5.97      0.87      5.96      4.69      7.49    759.17      1.00\n",
      " gamma[63]      5.28      1.11      5.17      3.41      6.87    620.25      1.00\n",
      " gamma[64]      8.65      1.92      8.59      5.23     11.45   1508.54      1.00\n",
      " gamma[65]      4.04      0.90      3.96      2.63      5.50    569.91      1.00\n",
      " gamma[66]      9.62      1.47      9.57      7.26     11.93    641.33      1.00\n",
      " gamma[67]      4.83      0.74      4.84      3.51      5.93    854.19      1.00\n",
      " gamma[68]      5.10      0.72      5.08      3.92      6.29   1030.90      1.00\n",
      " gamma[69]      9.27      1.57      9.22      6.63     11.70    706.77      1.00\n",
      " gamma[70]      6.08      1.26      6.04      4.18      8.07    958.99      1.00\n",
      " gamma[71]      3.18      0.62      3.12      2.24      4.24    661.19      1.00\n",
      " gamma[72]      7.85      1.23      7.82      5.97      9.93    634.93      1.00\n",
      " gamma[73]      4.93      0.82      4.92      3.61      6.29    610.54      1.00\n",
      " lams[0,0]      1.63      0.72      1.46      0.64      2.62    491.50      1.01\n",
      " lams[0,1]      2.11      0.72      1.97      1.08      3.13    488.70      1.01\n",
      " lams[1,0]      1.65      0.71      1.51      0.68      2.71    505.53      1.00\n",
      " lams[1,1]      2.30      0.72      2.18      1.20      3.30    521.96      1.00\n",
      " lams[2,0]      1.58      0.76      1.41      0.52      2.64    610.62      1.00\n",
      " lams[2,1]      2.32      0.75      2.17      1.22      3.40    585.72      1.00\n",
      " lams[3,0]      7.84      0.87      7.84      6.40      9.20   1211.22      1.00\n",
      " lams[3,1]      7.90      0.86      7.90      6.46      9.26   1206.96      1.00\n",
      " lams[4,0]      1.70      0.65      1.54      0.79      2.66    491.49      1.00\n",
      " lams[4,1]      1.94      0.66      1.79      1.01      2.95    500.92      1.00\n",
      " lams[5,0]      1.67      0.71      1.50      0.69      2.78    407.95      1.00\n",
      " lams[5,1]      2.05      0.71      1.90      1.02      3.13    409.57      1.00\n",
      " lams[6,0]      1.69      0.70      1.53      0.69      2.70    465.86      1.01\n",
      " lams[6,1]      1.97      0.70      1.81      0.97      3.03    490.72      1.00\n",
      " lams[7,0]      1.68      0.79      1.52      0.64      2.92    389.93      1.00\n",
      " lams[7,1]      2.16      0.78      2.00      1.09      3.43    399.84      1.00\n",
      " lams[8,0]      1.64      0.82      1.45      0.56      2.89    512.85      1.00\n",
      " lams[8,1]      2.22      0.81      2.05      1.07      3.45    527.83      1.00\n",
      " lams[9,0]      1.63      0.76      1.47      0.59      2.70    455.49      1.00\n",
      " lams[9,1]      2.22      0.76      2.08      1.11      3.33    441.00      1.00\n",
      "lams[10,0]      1.54      0.62      1.39      0.71      2.48    576.74      1.00\n",
      "lams[10,1]      2.12      0.64      2.00      1.20      3.13    579.48      1.00\n",
      "lams[11,0]      1.69      0.70      1.56      0.74      2.72    470.97      1.01\n",
      "lams[11,1]      2.11      0.71      2.00      1.10      3.16    465.21      1.01\n",
      "lams[12,0]      1.68      0.72      1.55      0.73      2.75    620.14      1.00\n",
      "lams[12,1]      2.10      0.72      1.99      0.98      3.08    616.63      1.00\n",
      "lams[13,0]      1.41      0.74      1.24      0.47      2.49    472.00      1.00\n",
      "lams[13,1]      2.24      0.74      2.09      1.17      3.34    472.46      1.00\n",
      "lams[14,0]      0.70      0.14      0.67      0.50      0.93    723.25      1.00\n",
      "lams[14,1]      1.26      0.24      1.22      0.88      1.61    658.27      1.00\n",
      "lams[15,0]      1.07      0.26      1.03      0.62      1.42    621.22      1.00\n",
      "lams[15,1]      1.78      0.38      1.73      1.23      2.35    546.68      1.00\n",
      "lams[16,0]      1.72      0.64      1.56      0.93      2.74    553.86      1.00\n",
      "lams[16,1]      2.05      0.65      1.89      1.23      3.08    561.23      1.00\n",
      "lams[17,0]     -0.11      0.07     -0.10     -0.21      0.00    738.46      1.00\n",
      "lams[17,1]      0.68      0.05      0.67      0.59      0.76    786.89      1.00\n",
      "lams[18,0]      0.27      0.04      0.27      0.21      0.33   1052.11      1.00\n",
      "lams[18,1]      0.88      0.09      0.86      0.73      1.00   1053.44      1.00\n",
      "lams[19,0]     -0.27      0.16     -0.25     -0.50     -0.02    484.78      1.01\n",
      "lams[19,1]      1.31      0.20      1.28      1.01      1.62    407.86      1.01\n",
      "lams[20,0]      1.64      0.81      1.46      0.54      2.83    438.72      1.00\n",
      "lams[20,1]      2.26      0.83      2.07      1.10      3.49    448.33      1.00\n",
      "lams[21,0]      1.48      0.62      1.31      0.65      2.37    387.05      1.00\n",
      "lams[21,1]      1.89      0.64      1.74      1.00      2.86    393.25      1.00\n",
      "lams[22,0]      0.26      0.05      0.27      0.18      0.35   1402.58      1.00\n",
      "lams[22,1]      1.13      0.17      1.10      0.88      1.43    529.16      1.00\n",
      "lams[23,0]      0.80      0.13      0.78      0.59      0.98    583.26      1.00\n",
      "lams[23,1]      1.30      0.23      1.26      0.94      1.64    553.09      1.00\n",
      "lams[24,0]      0.83      0.20      0.80      0.57      1.12    448.20      1.00\n",
      "lams[24,1]      1.43      0.31      1.39      1.00      1.86    402.49      1.00\n",
      "lams[25,0]      0.27      0.06      0.27      0.17      0.35   1364.79      1.00\n",
      "lams[25,1]      1.18      0.18      1.15      0.91      1.44    873.67      1.00\n",
      "lams[26,0]      0.84      0.15      0.82      0.61      1.07    854.26      1.00\n",
      "lams[26,1]      1.17      0.20      1.15      0.86      1.49    826.11      1.00\n",
      "lams[27,0]     -0.40      0.17     -0.38     -0.63     -0.14    462.92      1.00\n",
      "lams[27,1]      0.94      0.12      0.92      0.75      1.12    483.88      1.00\n",
      "lams[28,0]     -0.83      0.28     -0.80     -1.30     -0.42    684.06      1.00\n",
      "lams[28,1]      1.71      0.30      1.66      1.21      2.17    646.32      1.00\n",
      "lams[29,0]      0.04      0.07      0.04     -0.07      0.14    908.88      1.00\n",
      "lams[29,1]      0.84      0.08      0.83      0.71      0.96    832.18      1.00\n",
      "lams[30,0]     -0.08      0.11     -0.07     -0.24      0.10    633.74      1.00\n",
      "lams[30,1]      1.17      0.16      1.15      0.93      1.44    559.15      1.00\n",
      "lams[31,0]      0.56      0.06      0.55      0.47      0.64    834.20      1.00\n",
      "lams[31,1]      1.02      0.14      0.99      0.80      1.21    781.88      1.00\n",
      "lams[32,0]     -0.33      0.16     -0.30     -0.56     -0.08    219.11      1.00\n",
      "lams[32,1]      0.68      0.09      0.67      0.55      0.82    216.84      1.00\n",
      "lams[33,0]      1.88      0.73      1.73      0.83      2.97    518.72      1.00\n",
      "lams[33,1]      2.09      0.73      1.96      1.02      3.21    518.48      1.00\n",
      "lams[34,0]      0.23      0.06      0.23      0.12      0.31   1541.93      1.00\n",
      "lams[34,1]      1.20      0.18      1.18      0.89      1.45    853.67      1.00\n",
      "lams[35,0]      0.42      0.04      0.42      0.36      0.48   1440.15      1.00\n",
      "lams[35,1]      0.87      0.10      0.86      0.72      1.03    959.90      1.00\n",
      "lams[36,0]     -0.37      0.17     -0.34     -0.62     -0.10    559.15      1.00\n",
      "lams[36,1]      1.13      0.15      1.10      0.90      1.37    583.90      1.00\n",
      "lams[37,0]     -0.41      0.24     -0.38     -0.79     -0.07    452.58      1.00\n",
      "lams[37,1]      1.40      0.25      1.35      1.06      1.80    457.05      1.00\n",
      "lams[38,0]      0.05      0.08      0.06     -0.07      0.17    785.17      1.00\n",
      "lams[38,1]      1.12      0.16      1.10      0.88      1.38    677.71      1.00\n",
      "lams[39,0]     -0.35      0.19     -0.32     -0.62     -0.05    479.31      1.00\n",
      "lams[39,1]      1.96      0.36      1.90      1.40      2.52    417.92      1.00\n",
      "lams[40,0]      0.15      0.06      0.16      0.06      0.26    983.12      1.00\n",
      "lams[40,1]      1.03      0.14      1.01      0.80      1.24    784.48      1.00\n",
      "lams[41,0]      1.68      0.72      1.50      0.77      2.79    587.18      1.00\n",
      "lams[41,1]      2.23      0.72      2.08      1.25      3.33    611.80      1.00\n",
      "lams[42,0]      1.45      0.59      1.30      0.65      2.26    513.93      1.00\n",
      "lams[42,1]      2.11      0.63      1.98      1.20      3.01    501.91      1.00\n",
      "lams[43,0]      1.73      0.79      1.58      0.63      2.84    538.46      1.00\n",
      "lams[43,1]      2.66      0.79      2.53      1.52      3.91    609.42      1.00\n",
      "lams[44,0]      1.67      0.81      1.52      0.56      2.85    434.65      1.01\n",
      "lams[44,1]      2.68      0.83      2.54      1.46      3.92    460.84      1.01\n",
      "lams[45,0]      1.73      0.71      1.54      0.80      2.84    478.32      1.00\n",
      "lams[45,1]      2.06      0.71      1.89      1.01      3.12    481.41      1.00\n",
      "lams[46,0]      1.99      0.62      1.87      1.11      2.94    503.59      1.00\n",
      "lams[46,1]      2.64      0.64      2.56      1.68      3.65    495.43      1.00\n",
      "lams[47,0]      2.19      0.65      2.08      1.21      3.12    660.86      1.00\n",
      "lams[47,1]      2.73      0.67      2.63      1.77      3.82    694.72      1.00\n",
      "lams[48,0]      1.62      0.70      1.47      0.69      2.66    354.28      1.00\n",
      "lams[48,1]      2.17      0.72      2.02      1.20      3.32    342.91      1.00\n",
      "lams[49,0]      1.49      0.61      1.37      0.64      2.37    490.89      1.00\n",
      "lams[49,1]      2.35      0.64      2.24      1.40      3.32    496.91      1.00\n",
      "lams[50,0]      1.69      0.58      1.58      0.83      2.53    589.36      1.00\n",
      "lams[50,1]      2.35      0.61      2.26      1.48      3.34    590.96      1.00\n",
      "lams[51,0]      1.40      0.64      1.27      0.47      2.33    556.68      1.00\n",
      "lams[51,1]      2.32      0.66      2.21      1.31      3.33    589.85      1.00\n",
      "lams[52,0]     -0.32      0.19     -0.30     -0.58     -0.03    541.87      1.00\n",
      "lams[52,1]      1.24      0.19      1.22      0.95      1.49    546.85      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.26    855.07      1.00\n",
      "lams[53,1]      0.69      0.05      0.68      0.62      0.76    862.24      1.00\n",
      "lams[54,0]      0.08      0.08      0.09     -0.05      0.21    588.48      1.00\n",
      "lams[54,1]      1.05      0.12      1.04      0.88      1.23    449.18      1.00\n",
      "lams[55,0]      1.82      0.70      1.66      0.85      2.97    621.78      1.00\n",
      "lams[55,1]      2.78      0.74      2.68      1.67      3.99    634.28      1.00\n",
      "lams[56,0]      1.44      0.65      1.28      0.55      2.40    489.67      1.00\n",
      "lams[56,1]      2.27      0.68      2.13      1.23      3.26    526.78      1.00\n",
      "lams[57,0]      0.19      0.06      0.19      0.10      0.28    740.05      1.00\n",
      "lams[57,1]      1.05      0.14      1.03      0.84      1.23    528.58      1.00\n",
      "lams[58,0]      0.52      0.05      0.52      0.43      0.60   1358.67      1.00\n",
      "lams[58,1]      1.15      0.16      1.13      0.89      1.38    729.37      1.00\n",
      "lams[59,0]      0.27      0.05      0.27      0.18      0.35    713.27      1.00\n",
      "lams[59,1]      1.14      0.14      1.12      0.91      1.35    415.69      1.00\n",
      "lams[60,0]     -0.90      0.30     -0.86     -1.36     -0.44    470.80      1.01\n",
      "lams[60,1]      1.81      0.31      1.77      1.31      2.26    464.35      1.00\n",
      "lams[61,0]     -0.41      0.21     -0.38     -0.72     -0.07    664.06      1.00\n",
      "lams[61,1]      1.12      0.16      1.10      0.85      1.36    644.86      1.00\n",
      "lams[62,0]     -0.28      0.13     -0.26     -0.44     -0.07    664.49      1.00\n",
      "lams[62,1]      0.94      0.11      0.92      0.76      1.08    604.56      1.00\n",
      "lams[63,0]     -0.64      0.28     -0.60     -1.08     -0.20    475.69      1.00\n",
      "lams[63,1]      1.74      0.32      1.70      1.27      2.25    503.71      1.00\n",
      "lams[64,0]      1.63      0.65      1.48      0.74      2.58    533.55      1.00\n",
      "lams[64,1]      2.17      0.67      2.03      1.13      3.15    537.82      1.00\n",
      "lams[65,0]     -0.04      0.14     -0.02     -0.25      0.17    447.21      1.00\n",
      "lams[65,1]      1.77      0.34      1.73      1.26      2.27    398.21      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.04      0.18    517.01      1.00\n",
      "lams[66,1]      0.86      0.08      0.86      0.74      0.99    549.57      1.00\n",
      "lams[67,0]     -0.31      0.13     -0.30     -0.52     -0.12    768.80      1.00\n",
      "lams[67,1]      0.96      0.12      0.95      0.77      1.14    761.42      1.00\n",
      "lams[68,0]      0.10      0.05      0.11      0.02      0.18    867.98      1.00\n",
      "lams[68,1]      0.86      0.09      0.85      0.73      1.00    870.83      1.00\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.41      0.04    534.92      1.01\n",
      "lams[69,1]      1.11      0.15      1.10      0.90      1.36    566.55      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.24      0.49   1294.33      1.00\n",
      "lams[70,1]      1.57      0.25      1.54      1.18      1.96    721.24      1.00\n",
      "lams[71,0]     -0.83      0.29     -0.82     -1.24     -0.36    545.17      1.00\n",
      "lams[71,1]      1.83      0.31      1.80      1.33      2.28    542.66      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.06      0.25    570.43      1.00\n",
      "lams[72,1]      0.99      0.11      0.97      0.83      1.16    562.47      1.00\n",
      "lams[73,0]     -1.06      0.34     -1.01     -1.57     -0.55    424.00      1.00\n",
      "lams[73,1]      1.22      0.20      1.19      0.88      1.49    449.49      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [58:20<00:00,  1.40s/it, 511 steps of size 9.23e-03. acc. prob=0.92]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -25560.84\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]      9.93      2.36      9.68      5.98     13.42   1611.29      1.00\n",
      "  gamma[1]      8.22      1.86      8.12      5.30     11.28   1529.22      1.00\n",
      "  gamma[2]      9.93      2.01      9.83      6.76     13.19   2037.23      1.00\n",
      "  gamma[3]     15.37      2.04     15.28     12.03     18.66   2016.93      1.00\n",
      "  gamma[4]      8.97      1.98      8.83      5.48     11.81   1676.83      1.00\n",
      "  gamma[5]      9.57      1.97      9.37      6.23     12.55   1321.95      1.00\n",
      "  gamma[6]     11.06      2.17     10.88      7.60     14.49   2549.49      1.00\n",
      "  gamma[7]     10.97      2.37     10.82      7.11     14.97   1516.93      1.00\n",
      "  gamma[8]     10.32      2.19     10.21      6.46     13.46   1740.10      1.00\n",
      "  gamma[9]     10.34      2.19     10.13      7.19     14.34   1284.61      1.00\n",
      " gamma[10]      8.38      1.85      8.26      5.73     11.58   1457.87      1.00\n",
      " gamma[11]      9.55      2.09      9.42      6.17     13.01   1789.61      1.00\n",
      " gamma[12]      9.63      2.00      9.50      6.23     12.71   1937.82      1.00\n",
      " gamma[13]      9.94      2.21      9.72      6.39     13.62   1616.61      1.00\n",
      " gamma[14]     10.40      2.38     10.13      6.80     14.66   1108.72      1.00\n",
      " gamma[15]      5.87      1.30      5.78      3.83      8.03    811.48      1.00\n",
      " gamma[16]      7.46      1.53      7.31      4.78      9.67   1925.07      1.00\n",
      " gamma[17]      7.30      0.82      7.28      5.88      8.54    615.30      1.01\n",
      " gamma[18]      6.80      1.07      6.78      4.99      8.45    795.36      1.00\n",
      " gamma[19]      4.35      0.74      4.29      3.21      5.60    615.64      1.00\n",
      " gamma[20]     10.07      2.30      9.92      6.10     13.53   1810.12      1.00\n",
      " gamma[21]      9.36      2.14      9.19      5.95     12.83   2291.88      1.00\n",
      " gamma[22]      3.84      0.71      3.80      2.56      4.87    911.39      1.00\n",
      " gamma[23]      6.66      1.47      6.46      4.35      8.98    783.32      1.00\n",
      " gamma[24]      8.64      1.88      8.57      5.63     11.74    682.54      1.00\n",
      " gamma[25]      4.31      0.81      4.28      2.89      5.58    778.02      1.00\n",
      " gamma[26]      9.64      2.12      9.43      6.13     12.95   1118.82      1.00\n",
      " gamma[27]      6.52      1.01      6.45      4.88      8.16    537.73      1.00\n",
      " gamma[28]      2.76      0.52      2.73      1.95      3.60    730.20      1.00\n",
      " gamma[29]      7.35      1.07      7.26      5.49      9.03    708.58      1.00\n",
      " gamma[30]      5.54      0.96      5.53      3.92      7.01    388.54      1.00\n",
      " gamma[31]      6.68      1.34      6.62      4.45      8.72    918.57      1.00\n",
      " gamma[32]      8.69      1.22      8.73      6.72     10.63    596.30      1.00\n",
      " gamma[33]      9.45      2.09      9.26      5.81     12.55   1564.52      1.00\n",
      " gamma[34]      3.59      0.64      3.58      2.49      4.59    828.79      1.00\n",
      " gamma[35]      5.83      0.93      5.77      4.25      7.27    960.11      1.00\n",
      " gamma[36]      6.20      0.97      6.20      4.54      7.68    514.76      1.00\n",
      " gamma[37]      6.42      1.31      6.38      4.30      8.44    528.24      1.00\n",
      " gamma[38]      3.83      0.67      3.80      2.84      5.06    874.75      1.00\n",
      " gamma[39]      2.21      0.43      2.19      1.55      2.93    815.40      1.00\n",
      " gamma[40]      3.83      0.64      3.81      2.78      4.86    934.72      1.00\n",
      " gamma[41]      8.37      1.69      8.27      5.53     11.00   2106.82      1.00\n",
      " gamma[42]      8.39      1.82      8.23      5.55     11.27   1693.31      1.00\n",
      " gamma[43]      7.64      1.61      7.52      5.07     10.29   1427.63      1.00\n",
      " gamma[44]      7.15      1.50      7.09      4.90      9.74   1613.04      1.00\n",
      " gamma[45]     10.13      2.25     10.03      6.81     13.88   1406.34      1.00\n",
      " gamma[46]      4.90      0.93      4.86      3.44      6.48   1438.96      1.00\n",
      " gamma[47]      4.57      0.78      4.55      3.10      5.70   1568.10      1.00\n",
      " gamma[48]      8.38      1.79      8.27      5.47     11.16   1426.65      1.00\n",
      " gamma[49]      7.84      1.62      7.75      5.05     10.24   2411.84      1.00\n",
      " gamma[50]      6.41      1.12      6.37      4.52      8.18   1359.94      1.00\n",
      " gamma[51]      9.10      1.83      9.01      6.21     12.06   2085.08      1.00\n",
      " gamma[52]      7.84      1.44      7.79      5.66     10.24    592.78      1.00\n",
      " gamma[53]      9.77      1.15      9.76      7.94     11.61    908.64      1.00\n",
      " gamma[54]      9.43      1.69      9.43      6.64     12.11    567.00      1.00\n",
      " gamma[55]      5.76      1.13      5.71      4.04      7.59   1712.12      1.00\n",
      " gamma[56]      8.77      1.91      8.61      5.78     11.78   2236.92      1.00\n",
      " gamma[57]      4.72      0.80      4.72      3.43      6.00    836.39      1.00\n",
      " gamma[58]      7.09      1.30      7.03      5.09      9.22   1183.62      1.00\n",
      " gamma[59]      7.57      1.41      7.56      5.16      9.71    800.63      1.00\n",
      " gamma[60]      3.58      0.70      3.55      2.52      4.75    515.54      1.00\n",
      " gamma[61]      9.18      1.62      9.12      6.34     11.66    546.30      1.00\n",
      " gamma[62]      5.98      0.88      6.00      4.48      7.38    709.55      1.00\n",
      " gamma[63]      5.36      1.07      5.32      3.68      7.14    702.09      1.00\n",
      " gamma[64]      8.65      1.96      8.55      5.66     11.81   1471.62      1.00\n",
      " gamma[65]      4.05      0.90      3.97      2.59      5.47    825.39      1.00\n",
      " gamma[66]      9.64      1.42      9.60      7.18     11.74    763.03      1.00\n",
      " gamma[67]      4.83      0.74      4.81      3.70      6.06    630.90      1.00\n",
      " gamma[68]      5.02      0.77      4.99      3.80      6.32   1034.41      1.00\n",
      " gamma[69]      9.13      1.58      9.01      6.66     11.74    594.60      1.00\n",
      " gamma[70]      6.05      1.15      6.04      4.32      8.04    994.29      1.00\n",
      " gamma[71]      3.17      0.64      3.13      2.17      4.25    626.05      1.00\n",
      " gamma[72]      7.82      1.27      7.83      5.91     10.02    623.77      1.00\n",
      " gamma[73]      4.90      0.81      4.88      3.43      6.01    748.75      1.00\n",
      " lams[0,0]      1.58      0.65      1.46      0.61      2.49    688.77      1.00\n",
      " lams[0,1]      2.06      0.66      1.96      1.09      3.02    737.94      1.00\n",
      " lams[1,0]      1.72      0.76      1.59      0.70      2.83    454.66      1.00\n",
      " lams[1,1]      2.38      0.78      2.26      1.22      3.52    462.32      1.01\n",
      " lams[2,0]      1.56      0.81      1.39      0.51      2.64    538.38      1.00\n",
      " lams[2,1]      2.28      0.80      2.12      1.14      3.32    545.89      1.00\n",
      " lams[3,0]      7.78      0.91      7.76      6.42      9.36   1071.00      1.00\n",
      " lams[3,1]      7.85      0.90      7.83      6.51      9.41   1062.48      1.00\n",
      " lams[4,0]      1.75      0.67      1.60      0.74      2.62    331.80      1.00\n",
      " lams[4,1]      2.00      0.68      1.85      1.03      2.97    340.50      1.00\n",
      " lams[5,0]      1.60      0.66      1.50      0.73      2.51    497.41      1.01\n",
      " lams[5,1]      1.97      0.66      1.87      1.05      2.90    493.42      1.01\n",
      " lams[6,0]      1.61      0.66      1.49      0.65      2.54    553.73      1.00\n",
      " lams[6,1]      1.88      0.66      1.75      0.92      2.81    565.07      1.00\n",
      " lams[7,0]      1.58      0.72      1.43      0.59      2.60    555.55      1.00\n",
      " lams[7,1]      2.05      0.72      1.92      1.01      3.15    544.98      1.00\n",
      " lams[8,0]      1.70      0.81      1.52      0.63      2.99    391.28      1.00\n",
      " lams[8,1]      2.26      0.81      2.09      1.13      3.53    393.44      1.00\n",
      " lams[9,0]      1.64      0.76      1.47      0.61      2.81    631.29      1.00\n",
      " lams[9,1]      2.20      0.76      2.05      1.14      3.41    646.88      1.00\n",
      "lams[10,0]      1.55      0.61      1.43      0.67      2.48    495.66      1.00\n",
      "lams[10,1]      2.12      0.63      2.00      1.24      3.13    494.37      1.00\n",
      "lams[11,0]      1.63      0.66      1.50      0.69      2.63    571.58      1.00\n",
      "lams[11,1]      2.04      0.67      1.92      1.04      3.04    577.60      1.00\n",
      "lams[12,0]      1.68      0.72      1.51      0.65      2.68    598.84      1.00\n",
      "lams[12,1]      2.09      0.72      1.94      1.05      3.12    589.59      1.00\n",
      "lams[13,0]      1.44      0.75      1.24      0.54      2.62    556.78      1.00\n",
      "lams[13,1]      2.22      0.77      2.05      1.18      3.36    542.07      1.00\n",
      "lams[14,0]      0.71      0.15      0.69      0.49      0.92    594.04      1.00\n",
      "lams[14,1]      1.27      0.24      1.22      0.91      1.62    546.54      1.00\n",
      "lams[15,0]      1.10      0.27      1.04      0.73      1.52    585.71      1.00\n",
      "lams[15,1]      1.79      0.38      1.73      1.17      2.37    528.35      1.00\n",
      "lams[16,0]      1.76      0.61      1.67      0.83      2.65    651.40      1.00\n",
      "lams[16,1]      2.09      0.63      2.02      1.16      3.05    659.62      1.00\n",
      "lams[17,0]     -0.15      0.07     -0.14     -0.27     -0.03    580.04      1.02\n",
      "lams[17,1]      0.71      0.06      0.71      0.62      0.81    632.08      1.01\n",
      "lams[18,0]      0.26      0.04      0.27      0.20      0.33    687.69      1.00\n",
      "lams[18,1]      0.91      0.10      0.89      0.76      1.06    665.27      1.00\n",
      "lams[19,0]     -0.27      0.14     -0.25     -0.50     -0.05    559.22      1.00\n",
      "lams[19,1]      1.32      0.19      1.29      1.02      1.60    522.35      1.00\n",
      "lams[20,0]      1.49      0.66      1.34      0.54      2.44    492.37      1.00\n",
      "lams[20,1]      2.09      0.66      1.94      1.07      3.03    497.92      1.00\n",
      "lams[21,0]      1.50      0.60      1.38      0.70      2.38    368.45      1.00\n",
      "lams[21,1]      1.91      0.62      1.79      1.03      2.86    360.74      1.00\n",
      "lams[22,0]      0.26      0.05      0.27      0.18      0.35   1204.04      1.00\n",
      "lams[22,1]      1.13      0.17      1.11      0.86      1.40    727.73      1.00\n",
      "lams[23,0]      0.80      0.14      0.77      0.59      0.98    484.76      1.00\n",
      "lams[23,1]      1.30      0.24      1.26      0.97      1.67    440.81      1.00\n",
      "lams[24,0]      0.82      0.18      0.80      0.56      1.10    453.07      1.00\n",
      "lams[24,1]      1.38      0.28      1.33      0.98      1.80    415.63      1.00\n",
      "lams[25,0]      0.27      0.05      0.27      0.19      0.36   1439.75      1.00\n",
      "lams[25,1]      1.18      0.18      1.16      0.90      1.44    636.47      1.00\n",
      "lams[26,0]      0.86      0.16      0.83      0.61      1.10    684.89      1.00\n",
      "lams[26,1]      1.18      0.22      1.16      0.82      1.47    703.05      1.00\n",
      "lams[27,0]     -0.44      0.19     -0.42     -0.74     -0.17    484.75      1.00\n",
      "lams[27,1]      0.99      0.14      0.97      0.79      1.21    472.98      1.00\n",
      "lams[28,0]     -0.83      0.28     -0.79     -1.28     -0.42    593.23      1.00\n",
      "lams[28,1]      1.73      0.30      1.68      1.24      2.18    542.34      1.00\n",
      "lams[29,0]      0.03      0.07      0.04     -0.08      0.15    625.44      1.00\n",
      "lams[29,1]      0.86      0.09      0.85      0.71      0.98    593.51      1.00\n",
      "lams[30,0]     -0.08      0.12     -0.07     -0.25      0.10    336.26      1.00\n",
      "lams[30,1]      1.20      0.17      1.17      0.95      1.45    295.81      1.00\n",
      "lams[31,0]      0.57      0.07      0.56      0.47      0.67    750.10      1.00\n",
      "lams[31,1]      1.04      0.15      1.02      0.81      1.27    658.73      1.00\n",
      "lams[32,0]     -0.44      0.19     -0.40     -0.73     -0.16    435.07      1.00\n",
      "lams[32,1]      0.76      0.11      0.74      0.60      0.91    420.56      1.00\n",
      "lams[33,0]      1.91      0.76      1.76      0.82      3.02    402.07      1.00\n",
      "lams[33,1]      2.13      0.75      1.98      1.03      3.24    410.13      1.00\n",
      "lams[34,0]      0.23      0.06      0.24      0.12      0.32   1312.08      1.00\n",
      "lams[34,1]      1.21      0.18      1.18      0.91      1.46    699.59      1.00\n",
      "lams[35,0]      0.42      0.03      0.42      0.37      0.47   1078.91      1.00\n",
      "lams[35,1]      0.86      0.09      0.85      0.72      1.01    744.25      1.00\n",
      "lams[36,0]     -0.39      0.18     -0.37     -0.63     -0.09    455.16      1.00\n",
      "lams[36,1]      1.17      0.16      1.14      0.91      1.40    435.29      1.00\n",
      "lams[37,0]     -0.38      0.23     -0.34     -0.71     -0.05    399.60      1.00\n",
      "lams[37,1]      1.38      0.25      1.34      1.00      1.75    369.61      1.00\n",
      "lams[38,0]      0.05      0.08      0.06     -0.08      0.17    674.52      1.00\n",
      "lams[38,1]      1.15      0.17      1.13      0.88      1.39    664.50      1.00\n",
      "lams[39,0]     -0.32      0.18     -0.30     -0.58     -0.01    723.06      1.00\n",
      "lams[39,1]      1.94      0.35      1.90      1.42      2.46    706.89      1.00\n",
      "lams[40,0]      0.16      0.06      0.16      0.07      0.26    935.25      1.00\n",
      "lams[40,1]      1.00      0.13      0.98      0.80      1.20    644.50      1.00\n",
      "lams[41,0]      1.75      0.73      1.61      0.78      2.86    487.47      1.00\n",
      "lams[41,1]      2.30      0.73      2.17      1.26      3.43    482.42      1.00\n",
      "lams[42,0]      1.51      0.60      1.39      0.67      2.36    538.40      1.00\n",
      "lams[42,1]      2.17      0.63      2.08      1.33      3.18    534.06      1.00\n",
      "lams[43,0]      1.74      0.79      1.59      0.58      2.87    659.92      1.00\n",
      "lams[43,1]      2.69      0.78      2.57      1.50      3.86    666.23      1.00\n",
      "lams[44,0]      1.56      0.68      1.42      0.57      2.51    549.17      1.00\n",
      "lams[44,1]      2.59      0.71      2.46      1.50      3.66    554.63      1.00\n",
      "lams[45,0]      1.82      0.76      1.66      0.72      2.96    464.94      1.00\n",
      "lams[45,1]      2.14      0.76      2.00      1.05      3.34    471.79      1.00\n",
      "lams[46,0]      2.03      0.70      1.91      0.98      2.98    476.09      1.00\n",
      "lams[46,1]      2.68      0.74      2.55      1.56      3.74    500.92      1.00\n",
      "lams[47,0]      2.14      0.62      2.02      1.22      3.10    717.00      1.00\n",
      "lams[47,1]      2.68      0.64      2.59      1.67      3.64    725.24      1.00\n",
      "lams[48,0]      1.56      0.63      1.42      0.68      2.51    482.24      1.01\n",
      "lams[48,1]      2.12      0.65      1.99      1.20      3.13    539.40      1.01\n",
      "lams[49,0]      1.52      0.64      1.38      0.69      2.52    442.24      1.00\n",
      "lams[49,1]      2.38      0.67      2.26      1.40      3.39    487.71      1.00\n",
      "lams[50,0]      1.71      0.60      1.59      0.88      2.59    595.89      1.00\n",
      "lams[50,1]      2.36      0.63      2.26      1.46      3.28    566.78      1.00\n",
      "lams[51,0]      1.43      0.65      1.30      0.54      2.41    543.72      1.00\n",
      "lams[51,1]      2.35      0.68      2.22      1.32      3.40    603.60      1.00\n",
      "lams[52,0]     -0.33      0.20     -0.30     -0.63     -0.05    384.25      1.00\n",
      "lams[52,1]      1.26      0.20      1.22      0.99      1.57    394.52      1.00\n",
      "lams[53,0]      0.21      0.03      0.22      0.16      0.26    906.73      1.00\n",
      "lams[53,1]      0.69      0.04      0.68      0.62      0.76    801.00      1.00\n",
      "lams[54,0]      0.07      0.09      0.08     -0.07      0.21    479.24      1.00\n",
      "lams[54,1]      1.07      0.13      1.04      0.87      1.27    439.73      1.00\n",
      "lams[55,0]      1.75      0.68      1.65      0.80      2.75    547.71      1.00\n",
      "lams[55,1]      2.71      0.72      2.61      1.59      3.77    579.06      1.00\n",
      "lams[56,0]      1.39      0.63      1.25      0.54      2.29    603.07      1.00\n",
      "lams[56,1]      2.24      0.65      2.12      1.26      3.16    650.66      1.00\n",
      "lams[57,0]      0.18      0.06      0.19      0.10      0.28    845.32      1.00\n",
      "lams[57,1]      1.06      0.14      1.04      0.84      1.26    651.79      1.00\n",
      "lams[58,0]      0.52      0.05      0.51      0.44      0.59   1104.30      1.00\n",
      "lams[58,1]      1.14      0.14      1.12      0.90      1.35    856.21      1.00\n",
      "lams[59,0]      0.27      0.06      0.27      0.17      0.35    892.68      1.00\n",
      "lams[59,1]      1.14      0.15      1.13      0.91      1.37    626.07      1.00\n",
      "lams[60,0]     -0.90      0.33     -0.86     -1.38     -0.42    389.98      1.00\n",
      "lams[60,1]      1.82      0.34      1.77      1.32      2.26    372.93      1.00\n",
      "lams[61,0]     -0.41      0.22     -0.38     -0.73     -0.08    392.32      1.00\n",
      "lams[61,1]      1.12      0.17      1.10      0.86      1.37    393.96      1.00\n",
      "lams[62,0]     -0.28      0.13     -0.26     -0.48     -0.08    512.18      1.00\n",
      "lams[62,1]      0.93      0.11      0.92      0.77      1.11    514.01      1.00\n",
      "lams[63,0]     -0.62      0.27     -0.58     -1.07     -0.24    566.37      1.00\n",
      "lams[63,1]      1.72      0.30      1.68      1.28      2.21    541.53      1.00\n",
      "lams[64,0]      1.65      0.70      1.49      0.74      2.72    345.16      1.00\n",
      "lams[64,1]      2.18      0.72      2.03      1.13      3.24    347.53      1.00\n",
      "lams[65,0]     -0.04      0.14     -0.03     -0.24      0.18    643.85      1.00\n",
      "lams[65,1]      1.77      0.33      1.72      1.28      2.28    612.91      1.00\n",
      "lams[66,0]      0.07      0.07      0.08     -0.03      0.18    782.47      1.00\n",
      "lams[66,1]      0.86      0.08      0.85      0.74      0.98    677.79      1.00\n",
      "lams[67,0]     -0.32      0.13     -0.30     -0.52     -0.11    502.78      1.00\n",
      "lams[67,1]      0.96      0.12      0.95      0.78      1.15    496.03      1.00\n",
      "lams[68,0]      0.09      0.06      0.10      0.01      0.18    923.26      1.00\n",
      "lams[68,1]      0.87      0.10      0.86      0.72      1.02    803.33      1.00\n",
      "lams[69,0]     -0.20      0.15     -0.19     -0.43      0.02    444.72      1.00\n",
      "lams[69,1]      1.13      0.15      1.11      0.89      1.35    430.58      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.22      0.49   1837.86      1.00\n",
      "lams[70,1]      1.58      0.24      1.55      1.23      1.97    728.60      1.00\n",
      "lams[71,0]     -0.84      0.31     -0.80     -1.29     -0.34    428.77      1.00\n",
      "lams[71,1]      1.84      0.34      1.79      1.32      2.33    423.88      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.05      0.24    535.18      1.00\n",
      "lams[72,1]      0.99      0.11      0.98      0.82      1.15    459.67      1.00\n",
      "lams[73,0]     -1.07      0.34     -1.03     -1.52     -0.52    558.32      1.00\n",
      "lams[73,1]      1.23      0.20      1.20      0.93      1.55    561.22      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [1:01:14<00:00,  1.47s/it, 511 steps of size 9.96e-03. acc. prob=0.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -25745.14\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]      9.97      2.27      9.74      6.42     13.43   2138.94      1.00\n",
      "  gamma[1]      8.08      1.64      7.98      5.29     10.58   1734.13      1.00\n",
      "  gamma[2]     10.01      2.26      9.77      6.63     13.98   1905.97      1.00\n",
      "  gamma[3]     15.61      2.16     15.46     12.15     19.32   3155.21      1.00\n",
      "  gamma[4]      9.03      1.89      8.91      5.78     12.03   1540.86      1.00\n",
      "  gamma[5]      9.52      2.14      9.37      6.22     12.89   1439.63      1.00\n",
      "  gamma[6]     11.08      2.24     10.92      7.63     14.58   1806.18      1.00\n",
      "  gamma[7]     11.17      2.37     11.01      7.71     15.23   1936.74      1.00\n",
      "  gamma[8]     10.28      2.25     10.12      6.58     13.68   1416.30      1.00\n",
      "  gamma[9]     10.30      2.39     10.13      6.20     13.90   2260.92      1.00\n",
      " gamma[10]      8.37      1.81      8.24      5.59     11.47   1887.95      1.00\n",
      " gamma[11]      9.57      2.09      9.46      6.34     12.98   1878.94      1.00\n",
      " gamma[12]      9.54      2.01      9.37      6.09     12.65   1273.79      1.00\n",
      " gamma[13]     10.19      2.33     10.11      6.42     13.72   1968.20      1.00\n",
      " gamma[14]     10.53      2.43     10.28      6.72     14.28    856.23      1.00\n",
      " gamma[15]      5.85      1.28      5.76      3.70      7.73    987.09      1.00\n",
      " gamma[16]      7.39      1.45      7.31      5.12      9.79   1423.64      1.00\n",
      " gamma[17]      6.84      0.82      6.81      5.59      8.27    799.95      1.00\n",
      " gamma[18]      6.50      1.08      6.49      4.53      8.09   1020.23      1.00\n",
      " gamma[19]      4.24      0.76      4.18      3.13      5.56   1010.39      1.00\n",
      " gamma[20]     10.15      2.17     10.04      6.50     13.52   1448.05      1.00\n",
      " gamma[21]      9.41      2.10      9.26      5.77     12.55   1833.42      1.00\n",
      " gamma[22]      3.75      0.71      3.70      2.56      4.87    998.81      1.00\n",
      " gamma[23]      6.78      1.43      6.77      4.12      8.81    602.51      1.00\n",
      " gamma[24]      8.69      1.88      8.51      5.69     11.75    600.65      1.00\n",
      " gamma[25]      4.36      0.82      4.32      3.06      5.74   1076.43      1.00\n",
      " gamma[26]      9.67      2.03      9.56      6.50     13.21   1002.16      1.00\n",
      " gamma[27]      6.11      0.97      6.08      4.57      7.64    610.03      1.00\n",
      " gamma[28]      2.76      0.47      2.74      1.98      3.54    629.01      1.00\n",
      " gamma[29]      6.96      1.06      6.92      5.32      8.85    976.99      1.00\n",
      " gamma[30]      5.36      1.01      5.32      3.87      7.13    688.07      1.00\n",
      " gamma[31]      6.62      1.30      6.58      4.51      8.61    699.71      1.00\n",
      " gamma[32]      7.78      1.17      7.73      5.91      9.76    824.98      1.01\n",
      " gamma[33]      9.39      1.92      9.27      6.02     12.30   2064.27      1.00\n",
      " gamma[34]      3.50      0.64      3.49      2.39      4.45    960.93      1.00\n",
      " gamma[35]      5.71      0.96      5.69      4.07      7.21   1023.22      1.00\n",
      " gamma[36]      5.87      1.07      5.85      4.19      7.70    544.96      1.00\n",
      " gamma[37]      6.25      1.20      6.21      4.40      8.24    794.45      1.00\n",
      " gamma[38]      3.74      0.63      3.74      2.69      4.73    693.38      1.01\n",
      " gamma[39]      2.16      0.45      2.12      1.38      2.82    899.11      1.00\n",
      " gamma[40]      3.93      0.67      3.91      2.81      4.99   1328.08      1.00\n",
      " gamma[41]      8.38      1.79      8.18      5.37     11.12   1632.01      1.00\n",
      " gamma[42]      8.46      1.75      8.32      5.61     11.16   1396.29      1.00\n",
      " gamma[43]      7.73      1.59      7.61      5.09     10.25   1800.57      1.00\n",
      " gamma[44]      7.16      1.47      7.04      4.79      9.42   1967.32      1.00\n",
      " gamma[45]     10.19      2.30     10.06      6.35     13.84   1414.29      1.00\n",
      " gamma[46]      4.91      0.89      4.85      3.40      6.31   2151.35      1.00\n",
      " gamma[47]      4.55      0.79      4.50      3.16      5.73   1841.99      1.00\n",
      " gamma[48]      8.38      1.65      8.32      5.74     10.97   1226.57      1.00\n",
      " gamma[49]      7.88      1.62      7.72      5.09     10.20   1401.06      1.00\n",
      " gamma[50]      6.38      1.25      6.32      4.40      8.36   1327.53      1.00\n",
      " gamma[51]      9.03      1.88      8.95      5.97     11.97   1378.62      1.00\n",
      " gamma[52]      8.01      1.37      7.97      5.50      9.99    802.20      1.00\n",
      " gamma[53]      9.79      1.17      9.75      7.89     11.66    907.04      1.00\n",
      " gamma[54]      9.43      1.73      9.39      6.65     12.26    787.43      1.00\n",
      " gamma[55]      5.81      1.10      5.73      3.86      7.44   1771.77      1.00\n",
      " gamma[56]      8.74      1.87      8.64      5.78     11.71   1778.85      1.00\n",
      " gamma[57]      4.74      0.74      4.74      3.34      5.79    950.46      1.00\n",
      " gamma[58]      7.05      1.36      7.03      4.67      9.02    921.78      1.00\n",
      " gamma[59]      7.55      1.36      7.47      5.11      9.54    954.24      1.00\n",
      " gamma[60]      3.56      0.66      3.52      2.49      4.65    677.03      1.00\n",
      " gamma[61]      9.21      1.54      9.12      6.72     11.63    743.22      1.01\n",
      " gamma[62]      6.03      0.86      6.02      4.62      7.39   1007.81      1.00\n",
      " gamma[63]      5.29      1.09      5.21      3.57      7.12    716.58      1.00\n",
      " gamma[64]      8.66      1.95      8.55      5.62     11.75   1854.40      1.00\n",
      " gamma[65]      4.11      0.86      4.05      2.72      5.44    805.92      1.00\n",
      " gamma[66]      9.69      1.38      9.63      7.27     11.82   1051.87      1.00\n",
      " gamma[67]      4.83      0.70      4.80      3.74      5.96    849.11      1.00\n",
      " gamma[68]      4.98      0.76      4.97      3.84      6.32   1096.18      1.00\n",
      " gamma[69]      9.31      1.57      9.21      6.98     12.03    911.05      1.00\n",
      " gamma[70]      6.09      1.26      5.99      3.85      7.87   1351.71      1.00\n",
      " gamma[71]      3.14      0.63      3.11      2.13      4.18    923.07      1.00\n",
      " gamma[72]      7.85      1.24      7.82      5.75      9.66    686.56      1.00\n",
      " gamma[73]      4.90      0.78      4.90      3.70      6.22    749.52      1.00\n",
      " lams[0,0]      1.66      0.73      1.48      0.63      2.72    561.12      1.00\n",
      " lams[0,1]      2.13      0.74      1.98      1.09      3.31    575.62      1.00\n",
      " lams[1,0]      1.72      0.73      1.56      0.65      2.76    478.62      1.00\n",
      " lams[1,1]      2.37      0.74      2.23      1.27      3.50    512.35      1.00\n",
      " lams[2,0]      1.54      0.77      1.35      0.53      2.63    426.36      1.00\n",
      " lams[2,1]      2.23      0.77      2.05      1.16      3.33    438.07      1.00\n",
      " lams[3,0]      7.77      0.92      7.77      6.31      9.28   1131.34      1.00\n",
      " lams[3,1]      7.83      0.91      7.83      6.32      9.25   1137.96      1.00\n",
      " lams[4,0]      1.70      0.66      1.53      0.79      2.63    469.05      1.00\n",
      " lams[4,1]      1.93      0.67      1.75      1.02      2.90    484.94      1.00\n",
      " lams[5,0]      1.61      0.72      1.43      0.69      2.71    399.89      1.00\n",
      " lams[5,1]      1.98      0.72      1.81      1.00      3.06    400.19      1.00\n",
      " lams[6,0]      1.69      0.73      1.54      0.66      2.69    483.22      1.00\n",
      " lams[6,1]      1.95      0.73      1.81      0.94      3.02    479.51      1.00\n",
      " lams[7,0]      1.68      0.79      1.50      0.62      2.80    503.00      1.00\n",
      " lams[7,1]      2.13      0.79      1.96      0.99      3.22    515.97      1.00\n",
      " lams[8,0]      1.69      0.79      1.49      0.59      2.85    533.81      1.00\n",
      " lams[8,1]      2.24      0.79      2.06      1.13      3.47    536.00      1.00\n",
      " lams[9,0]      1.66      0.78      1.50      0.58      2.76    595.96      1.00\n",
      " lams[9,1]      2.22      0.78      2.06      1.04      3.34    618.48      1.00\n",
      "lams[10,0]      1.59      0.70      1.42      0.63      2.58    535.69      1.00\n",
      "lams[10,1]      2.14      0.71      1.97      1.10      3.14    545.06      1.00\n",
      "lams[11,0]      1.64      0.67      1.49      0.70      2.66    681.36      1.00\n",
      "lams[11,1]      2.05      0.67      1.92      1.00      3.03    657.85      1.00\n",
      "lams[12,0]      1.72      0.74      1.56      0.65      2.79    447.24      1.01\n",
      "lams[12,1]      2.12      0.74      1.97      1.03      3.20    451.78      1.01\n",
      "lams[13,0]      1.48      0.73      1.34      0.46      2.49    616.72      1.00\n",
      "lams[13,1]      2.20      0.74      2.07      1.15      3.29    624.81      1.00\n",
      "lams[14,0]      0.71      0.15      0.68      0.48      0.93    635.63      1.00\n",
      "lams[14,1]      1.25      0.24      1.21      0.87      1.61    529.23      1.00\n",
      "lams[15,0]      1.10      0.29      1.04      0.67      1.47    516.49      1.00\n",
      "lams[15,1]      1.78      0.40      1.71      1.23      2.38    505.09      1.00\n",
      "lams[16,0]      1.81      0.65      1.67      0.88      2.79    598.65      1.00\n",
      "lams[16,1]      2.13      0.66      2.01      1.12      3.07    598.37      1.00\n",
      "lams[17,0]     -0.18      0.08     -0.17     -0.30     -0.04    713.85      1.00\n",
      "lams[17,1]      0.75      0.07      0.74      0.64      0.86    767.18      1.00\n",
      "lams[18,0]      0.26      0.04      0.27      0.20      0.34   1260.00      1.00\n",
      "lams[18,1]      0.93      0.11      0.92      0.76      1.09    804.53      1.00\n",
      "lams[19,0]     -0.27      0.16     -0.25     -0.49     -0.00    777.30      1.00\n",
      "lams[19,1]      1.34      0.20      1.32      1.03      1.64    802.92      1.00\n",
      "lams[20,0]      1.58      0.77      1.42      0.57      2.72    459.94      1.00\n",
      "lams[20,1]      2.16      0.77      2.00      1.06      3.25    471.95      1.00\n",
      "lams[21,0]      1.49      0.62      1.34      0.62      2.50    456.60      1.00\n",
      "lams[21,1]      1.89      0.64      1.76      1.04      3.01    461.82      1.00\n",
      "lams[22,0]      0.27      0.05      0.27      0.19      0.36   2181.24      1.00\n",
      "lams[22,1]      1.15      0.18      1.13      0.86      1.40    790.31      1.00\n",
      "lams[23,0]      0.79      0.13      0.77      0.60      0.97    475.60      1.00\n",
      "lams[23,1]      1.27      0.22      1.24      0.92      1.59    468.90      1.00\n",
      "lams[24,0]      0.81      0.17      0.78      0.55      1.07    527.94      1.00\n",
      "lams[24,1]      1.36      0.26      1.31      0.95      1.75    418.54      1.00\n",
      "lams[25,0]      0.28      0.06      0.28      0.20      0.39   1315.43      1.00\n",
      "lams[25,1]      1.16      0.18      1.14      0.87      1.42    784.50      1.00\n",
      "lams[26,0]      0.86      0.16      0.83      0.63      1.10    681.54      1.00\n",
      "lams[26,1]      1.18      0.21      1.14      0.85      1.49    681.06      1.00\n",
      "lams[27,0]     -0.48      0.19     -0.45     -0.78     -0.18    558.30      1.00\n",
      "lams[27,1]      1.02      0.14      1.00      0.82      1.27    546.54      1.00\n",
      "lams[28,0]     -0.79      0.24     -0.75     -1.21     -0.44    538.10      1.00\n",
      "lams[28,1]      1.70      0.27      1.66      1.27      2.14    541.46      1.00\n",
      "lams[29,0]      0.02      0.07      0.03     -0.10      0.14    871.99      1.00\n",
      "lams[29,1]      0.88      0.09      0.87      0.74      1.04    677.53      1.00\n",
      "lams[30,0]     -0.09      0.12     -0.06     -0.27      0.10    606.19      1.00\n",
      "lams[30,1]      1.22      0.19      1.19      0.94      1.53    546.93      1.00\n",
      "lams[31,0]      0.58      0.07      0.57      0.48      0.68    710.44      1.00\n",
      "lams[31,1]      1.05      0.15      1.02      0.81      1.28    601.48      1.00\n",
      "lams[32,0]     -0.54      0.22     -0.51     -0.88     -0.21    666.07      1.01\n",
      "lams[32,1]      0.82      0.12      0.81      0.63      1.01    656.07      1.01\n",
      "lams[33,0]      1.99      0.83      1.80      0.83      3.18    485.41      1.00\n",
      "lams[33,1]      2.20      0.83      2.03      1.02      3.40    485.40      1.00\n",
      "lams[34,0]      0.24      0.06      0.24      0.15      0.34   1256.89      1.00\n",
      "lams[34,1]      1.24      0.18      1.21      0.92      1.49    740.50      1.00\n",
      "lams[35,0]      0.42      0.04      0.42      0.37      0.49   1248.42      1.00\n",
      "lams[35,1]      0.87      0.10      0.86      0.71      1.02    651.99      1.01\n",
      "lams[36,0]     -0.42      0.21     -0.39     -0.74     -0.10    427.28      1.00\n",
      "lams[36,1]      1.21      0.20      1.18      0.91      1.48    418.34      1.00\n",
      "lams[37,0]     -0.33      0.20     -0.30     -0.61     -0.01    690.77      1.00\n",
      "lams[37,1]      1.36      0.22      1.32      1.00      1.68    670.74      1.00\n",
      "lams[38,0]      0.04      0.08      0.05     -0.08      0.18    693.71      1.00\n",
      "lams[38,1]      1.16      0.16      1.14      0.89      1.39    588.96      1.01\n",
      "lams[39,0]     -0.32      0.19     -0.30     -0.60     -0.01    745.25      1.00\n",
      "lams[39,1]      1.97      0.37      1.91      1.44      2.57    742.73      1.00\n",
      "lams[40,0]      0.16      0.06      0.16      0.06      0.25   1569.45      1.00\n",
      "lams[40,1]      0.98      0.13      0.96      0.79      1.19   1071.88      1.00\n",
      "lams[41,0]      1.63      0.67      1.50      0.66      2.56    591.26      1.00\n",
      "lams[41,1]      2.19      0.69      2.08      1.17      3.21    603.15      1.00\n",
      "lams[42,0]      1.51      0.64      1.34      0.64      2.42    361.02      1.01\n",
      "lams[42,1]      2.16      0.67      2.02      1.17      3.10    357.99      1.00\n",
      "lams[43,0]      1.78      0.82      1.63      0.63      2.98    531.79      1.00\n",
      "lams[43,1]      2.71      0.81      2.58      1.57      3.96    528.23      1.00\n",
      "lams[44,0]      1.60      0.70      1.48      0.67      2.68    575.39      1.00\n",
      "lams[44,1]      2.62      0.71      2.52      1.50      3.67    580.74      1.00\n",
      "lams[45,0]      1.74      0.73      1.57      0.77      2.81    645.63      1.00\n",
      "lams[45,1]      2.06      0.73      1.89      0.97      3.07    668.59      1.00\n",
      "lams[46,0]      1.98      0.61      1.88      1.04      2.87   1151.30      1.01\n",
      "lams[46,1]      2.63      0.63      2.56      1.65      3.58   1176.93      1.01\n",
      "lams[47,0]      2.14      0.60      2.05      1.20      3.02    895.98      1.00\n",
      "lams[47,1]      2.68      0.62      2.59      1.69      3.58    889.26      1.00\n",
      "lams[48,0]      1.60      0.65      1.49      0.69      2.56    506.08      1.00\n",
      "lams[48,1]      2.15      0.67      2.03      1.14      3.10    485.10      1.00\n",
      "lams[49,0]      1.49      0.66      1.35      0.65      2.45    547.76      1.00\n",
      "lams[49,1]      2.35      0.68      2.25      1.36      3.34    560.63      1.00\n",
      "lams[50,0]      1.70      0.59      1.59      0.88      2.61    510.34      1.00\n",
      "lams[50,1]      2.37      0.63      2.27      1.43      3.36    508.13      1.00\n",
      "lams[51,0]      1.45      0.72      1.28      0.48      2.40    472.43      1.00\n",
      "lams[51,1]      2.38      0.74      2.23      1.36      3.46    474.76      1.00\n",
      "lams[52,0]     -0.31      0.17     -0.29     -0.59     -0.04    718.93      1.00\n",
      "lams[52,1]      1.23      0.17      1.21      0.99      1.53    658.92      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.27    933.46      1.00\n",
      "lams[53,1]      0.69      0.05      0.69      0.61      0.75    996.89      1.00\n",
      "lams[54,0]      0.07      0.09      0.08     -0.07      0.20    683.54      1.00\n",
      "lams[54,1]      1.07      0.14      1.05      0.86      1.26    650.83      1.00\n",
      "lams[55,0]      1.74      0.68      1.60      0.79      2.72    456.94      1.00\n",
      "lams[55,1]      2.69      0.72      2.58      1.64      3.79    509.66      1.01\n",
      "lams[56,0]      1.47      0.70      1.31      0.54      2.40    553.49      1.00\n",
      "lams[56,1]      2.31      0.72      2.18      1.33      3.33    529.55      1.00\n",
      "lams[57,0]      0.19      0.05      0.19      0.10      0.26   1136.26      1.00\n",
      "lams[57,1]      1.05      0.12      1.04      0.86      1.25    765.16      1.00\n",
      "lams[58,0]      0.52      0.05      0.51      0.45      0.60    954.99      1.00\n",
      "lams[58,1]      1.14      0.16      1.12      0.92      1.39    674.56      1.00\n",
      "lams[59,0]      0.27      0.05      0.27      0.18      0.35    891.44      1.00\n",
      "lams[59,1]      1.15      0.15      1.13      0.92      1.36    747.38      1.00\n",
      "lams[60,0]     -0.91      0.31     -0.88     -1.37     -0.40    506.41      1.00\n",
      "lams[60,1]      1.83      0.32      1.79      1.32      2.31    504.30      1.00\n",
      "lams[61,0]     -0.40      0.21     -0.38     -0.73     -0.10    539.52      1.00\n",
      "lams[61,1]      1.11      0.16      1.09      0.86      1.35    581.65      1.00\n",
      "lams[62,0]     -0.27      0.12     -0.26     -0.47     -0.09    870.73      1.00\n",
      "lams[62,1]      0.93      0.10      0.92      0.76      1.08    808.11      1.00\n",
      "lams[63,0]     -0.64      0.29     -0.60     -1.03     -0.17    605.94      1.00\n",
      "lams[63,1]      1.75      0.31      1.71      1.21      2.16    600.08      1.00\n",
      "lams[64,0]      1.65      0.65      1.50      0.69      2.55    514.16      1.00\n",
      "lams[64,1]      2.18      0.67      2.08      1.21      3.27    531.26      1.00\n",
      "lams[65,0]     -0.03      0.13     -0.01     -0.21      0.19    761.53      1.00\n",
      "lams[65,1]      1.75      0.32      1.70      1.29      2.23    651.05      1.00\n",
      "lams[66,0]      0.08      0.07      0.08     -0.03      0.19   1004.94      1.00\n",
      "lams[66,1]      0.86      0.08      0.85      0.74      0.99    902.39      1.00\n",
      "lams[67,0]     -0.31      0.12     -0.29     -0.49     -0.10    739.74      1.00\n",
      "lams[67,1]      0.96      0.11      0.95      0.80      1.15    727.57      1.00\n",
      "lams[68,0]      0.09      0.06      0.10      0.01      0.19    917.59      1.00\n",
      "lams[68,1]      0.88      0.09      0.87      0.72      1.02    901.71      1.00\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.41      0.04    567.40      1.00\n",
      "lams[69,1]      1.11      0.14      1.10      0.89      1.33    594.82      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.25      0.51   1579.52      1.00\n",
      "lams[70,1]      1.58      0.26      1.54      1.18      1.97    939.98      1.00\n",
      "lams[71,0]     -0.85      0.30     -0.81     -1.30     -0.36    786.37      1.00\n",
      "lams[71,1]      1.85      0.33      1.80      1.31      2.34    745.22      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.04      0.24    590.37      1.00\n",
      "lams[72,1]      0.99      0.11      0.98      0.84      1.17    568.04      1.00\n",
      "lams[73,0]     -1.08      0.32     -1.03     -1.55     -0.56    575.09      1.00\n",
      "lams[73,1]      1.23      0.19      1.20      0.94      1.54    580.35      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [59:58<00:00,  1.44s/it, 511 steps of size 1.21e-02. acc. prob=0.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -25908.71\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]      9.97      2.20      9.87      6.56     13.50   1668.00      1.00\n",
      "  gamma[1]      7.94      1.61      7.82      5.25     10.49   1791.53      1.00\n",
      "  gamma[2]      9.95      2.26      9.77      6.48     13.52   1791.98      1.00\n",
      "  gamma[3]     15.61      2.11     15.54     12.53     19.35   1601.95      1.00\n",
      "  gamma[4]      9.05      1.91      8.88      5.96     12.09   1517.95      1.00\n",
      "  gamma[5]      9.50      2.05      9.32      6.27     12.96   2352.22      1.00\n",
      "  gamma[6]     11.21      2.33     11.10      7.38     14.86   1237.04      1.00\n",
      "  gamma[7]     11.22      2.51     10.98      7.31     14.99   1831.06      1.00\n",
      "  gamma[8]     10.31      2.31     10.19      6.40     13.72   2013.53      1.00\n",
      "  gamma[9]     10.41      2.34     10.25      6.58     14.21   1659.10      1.00\n",
      " gamma[10]      8.37      1.68      8.27      5.56     10.82   1351.57      1.00\n",
      " gamma[11]      9.45      2.03      9.37      6.24     12.86   1483.58      1.00\n",
      " gamma[12]      9.53      2.04      9.40      6.41     13.00   2653.90      1.00\n",
      " gamma[13]     10.19      2.18     10.03      6.55     13.50   1601.14      1.00\n",
      " gamma[14]     10.59      2.38     10.36      6.55     14.20   1204.63      1.00\n",
      " gamma[15]      5.82      1.22      5.78      3.67      7.58   1512.81      1.00\n",
      " gamma[16]      7.24      1.43      7.16      5.07      9.77   1647.57      1.00\n",
      " gamma[17]      6.48      0.75      6.45      5.20      7.67   1230.29      1.00\n",
      " gamma[18]      6.27      1.08      6.24      4.59      8.03   1656.61      1.00\n",
      " gamma[19]      4.13      0.74      4.06      3.00      5.33   1334.52      1.00\n",
      " gamma[20]     10.12      2.27      9.94      6.54     13.68   1998.53      1.00\n",
      " gamma[21]      9.48      2.05      9.38      6.05     12.61   1481.88      1.00\n",
      " gamma[22]      3.65      0.67      3.62      2.57      4.78   1204.00      1.00\n",
      " gamma[23]      6.82      1.48      6.70      4.56      9.32   1178.06      1.00\n",
      " gamma[24]      8.85      1.83      8.75      6.27     12.15   1429.34      1.00\n",
      " gamma[25]      4.30      0.82      4.25      2.90      5.59   1433.48      1.00\n",
      " gamma[26]      9.53      2.12      9.44      5.77     12.59   1217.63      1.00\n",
      " gamma[27]      5.88      0.93      5.85      4.12      7.22   1232.98      1.00\n",
      " gamma[28]      2.67      0.50      2.63      1.94      3.60   1140.28      1.00\n",
      " gamma[29]      6.74      0.98      6.69      5.15      8.35   1503.54      1.00\n",
      " gamma[30]      5.23      0.91      5.17      3.62      6.56   1322.43      1.00\n",
      " gamma[31]      6.58      1.26      6.52      4.66      8.71   1256.53      1.00\n",
      " gamma[32]      7.09      1.13      7.07      5.12      8.81    903.37      1.00\n",
      " gamma[33]      9.36      1.96      9.26      6.30     12.62   1678.55      1.00\n",
      " gamma[34]      3.44      0.66      3.39      2.38      4.54   1440.17      1.00\n",
      " gamma[35]      5.70      0.90      5.64      4.21      7.13   1188.80      1.00\n",
      " gamma[36]      5.65      0.99      5.62      4.01      7.22   1225.16      1.00\n",
      " gamma[37]      6.11      1.19      6.08      4.12      7.99   1054.03      1.00\n",
      " gamma[38]      3.64      0.64      3.60      2.50      4.55   1530.35      1.00\n",
      " gamma[39]      2.12      0.43      2.10      1.38      2.77   1064.03      1.00\n",
      " gamma[40]      3.96      0.70      3.91      2.94      5.13   1142.68      1.00\n",
      " gamma[41]      8.27      1.75      8.17      5.09     10.74   2008.22      1.00\n",
      " gamma[42]      8.45      1.66      8.39      5.87     11.28   1833.33      1.00\n",
      " gamma[43]      7.64      1.70      7.46      4.91     10.25   2143.03      1.00\n",
      " gamma[44]      7.26      1.49      7.19      4.83      9.54   2063.85      1.00\n",
      " gamma[45]     10.08      2.27      9.85      6.60     13.79   2108.29      1.00\n",
      " gamma[46]      4.88      0.89      4.86      3.49      6.31   1677.00      1.00\n",
      " gamma[47]      4.57      0.77      4.53      3.32      5.76   1828.54      1.00\n",
      " gamma[48]      8.47      1.70      8.40      5.79     11.29   1564.16      1.00\n",
      " gamma[49]      7.84      1.59      7.75      5.38     10.33   1526.92      1.00\n",
      " gamma[50]      6.45      1.14      6.38      4.51      8.19   1563.92      1.00\n",
      " gamma[51]      9.17      1.83      9.04      6.32     12.33   1787.00      1.00\n",
      " gamma[52]      8.07      1.37      8.04      5.96     10.37   1348.87      1.00\n",
      " gamma[53]      9.74      1.14      9.75      7.76     11.54   1638.05      1.00\n",
      " gamma[54]      9.37      1.77      9.26      6.40     12.18   1165.40      1.00\n",
      " gamma[55]      5.83      1.11      5.80      3.78      7.46   2228.03      1.00\n",
      " gamma[56]      8.82      1.83      8.71      6.08     11.89   1788.15      1.00\n",
      " gamma[57]      4.72      0.74      4.72      3.51      5.93    802.77      1.01\n",
      " gamma[58]      7.02      1.37      6.97      4.89      9.39   1492.85      1.00\n",
      " gamma[59]      7.70      1.39      7.63      5.64     10.16   1113.55      1.00\n",
      " gamma[60]      3.55      0.68      3.53      2.44      4.61   1004.68      1.00\n",
      " gamma[61]      9.35      1.53      9.29      6.80     11.70   1350.68      1.00\n",
      " gamma[62]      5.97      0.88      5.93      4.58      7.35    911.56      1.00\n",
      " gamma[63]      5.33      1.06      5.26      3.65      7.05    962.82      1.00\n",
      " gamma[64]      8.62      1.93      8.42      5.45     11.55   1435.45      1.00\n",
      " gamma[65]      4.12      0.87      4.05      2.81      5.62    940.58      1.00\n",
      " gamma[66]      9.66      1.41      9.60      7.17     11.73   1009.35      1.00\n",
      " gamma[67]      4.82      0.72      4.80      3.59      5.98    949.39      1.00\n",
      " gamma[68]      4.99      0.74      4.95      3.71      6.13   1784.27      1.00\n",
      " gamma[69]      9.32      1.60      9.28      6.78     11.91    991.83      1.00\n",
      " gamma[70]      6.08      1.25      6.06      4.12      8.15   1544.86      1.00\n",
      " gamma[71]      3.17      0.61      3.14      2.14      4.12   1145.09      1.00\n",
      " gamma[72]      7.89      1.20      7.86      6.07      9.98   1295.05      1.00\n",
      " gamma[73]      4.91      0.84      4.89      3.54      6.24    925.73      1.00\n",
      " lams[0,0]      1.62      0.67      1.47      0.68      2.62    704.61      1.01\n",
      " lams[0,1]      2.07      0.68      1.96      1.00      3.05    749.14      1.00\n",
      " lams[1,0]      1.75      0.80      1.55      0.69      2.99    678.75      1.00\n",
      " lams[1,1]      2.41      0.80      2.24      1.22      3.68    715.82      1.00\n",
      " lams[2,0]      1.50      0.71      1.33      0.57      2.57    635.83      1.00\n",
      " lams[2,1]      2.17      0.72      2.01      1.11      3.24    680.92      1.00\n",
      " lams[3,0]      7.71      0.95      7.71      6.24      9.39   1191.41      1.00\n",
      " lams[3,1]      7.77      0.95      7.77      6.10      9.23   1199.43      1.00\n",
      " lams[4,0]      1.72      0.65      1.57      0.78      2.62    541.05      1.00\n",
      " lams[4,1]      1.95      0.66      1.81      1.05      2.91    548.19      1.00\n",
      " lams[5,0]      1.67      0.75      1.49      0.70      2.86    447.21      1.00\n",
      " lams[5,1]      2.03      0.75      1.85      1.05      3.24    441.66      1.00\n",
      " lams[6,0]      1.72      0.72      1.59      0.69      2.74    588.87      1.00\n",
      " lams[6,1]      1.98      0.72      1.85      0.99      3.08    605.95      1.00\n",
      " lams[7,0]      1.58      0.72      1.39      0.61      2.60    710.03      1.00\n",
      " lams[7,1]      2.02      0.73      1.83      1.01      3.12    741.43      1.00\n",
      " lams[8,0]      1.71      0.79      1.57      0.64      2.91    659.45      1.00\n",
      " lams[8,1]      2.25      0.79      2.10      1.12      3.50    706.96      1.00\n",
      " lams[9,0]      1.69      0.80      1.49      0.63      2.87    542.93      1.00\n",
      " lams[9,1]      2.22      0.79      2.03      1.16      3.44    545.46      1.00\n",
      "lams[10,0]      1.57      0.67      1.42      0.63      2.46    633.91      1.00\n",
      "lams[10,1]      2.11      0.68      1.98      1.12      3.08    642.96      1.00\n",
      "lams[11,0]      1.68      0.69      1.53      0.71      2.71    559.10      1.00\n",
      "lams[11,1]      2.08      0.69      1.95      1.03      3.07    560.78      1.00\n",
      "lams[12,0]      1.71      0.70      1.60      0.75      2.79    725.99      1.00\n",
      "lams[12,1]      2.11      0.70      2.03      1.08      3.18    763.24      1.00\n",
      "lams[13,0]      1.48      0.73      1.31      0.51      2.50    661.48      1.00\n",
      "lams[13,1]      2.17      0.73      2.03      1.16      3.23    708.45      1.00\n",
      "lams[14,0]      0.71      0.15      0.69      0.49      0.93    769.50      1.00\n",
      "lams[14,1]      1.24      0.23      1.20      0.88      1.56    730.61      1.00\n",
      "lams[15,0]      1.10      0.26      1.05      0.69      1.45    789.37      1.00\n",
      "lams[15,1]      1.77      0.37      1.72      1.21      2.29    838.34      1.00\n",
      "lams[16,0]      1.76      0.61      1.63      0.91      2.63    574.39      1.00\n",
      "lams[16,1]      2.09      0.62      1.97      1.22      3.07    549.17      1.00\n",
      "lams[17,0]     -0.20      0.08     -0.19     -0.32     -0.06   1107.43      1.00\n",
      "lams[17,1]      0.77      0.07      0.76      0.66      0.88   1158.22      1.00\n",
      "lams[18,0]      0.26      0.05      0.27      0.19      0.34   1325.03      1.00\n",
      "lams[18,1]      0.95      0.11      0.94      0.77      1.11   1195.82      1.00\n",
      "lams[19,0]     -0.28      0.16     -0.26     -0.50     -0.03    877.47      1.00\n",
      "lams[19,1]      1.35      0.21      1.34      1.04      1.67    895.04      1.00\n",
      "lams[20,0]      1.57      0.74      1.41      0.56      2.59    455.50      1.00\n",
      "lams[20,1]      2.14      0.75      1.98      1.03      3.19    508.77      1.00\n",
      "lams[21,0]      1.47      0.57      1.35      0.65      2.25    606.59      1.00\n",
      "lams[21,1]      1.86      0.59      1.75      0.98      2.66    611.98      1.00\n",
      "lams[22,0]      0.27      0.05      0.28      0.19      0.37   2018.96      1.00\n",
      "lams[22,1]      1.17      0.18      1.14      0.88      1.42   1001.65      1.00\n",
      "lams[23,0]      0.79      0.13      0.77      0.60      0.99    790.19      1.00\n",
      "lams[23,1]      1.27      0.22      1.23      0.94      1.60    793.96      1.00\n",
      "lams[24,0]      0.81      0.16      0.79      0.57      1.04    834.48      1.00\n",
      "lams[24,1]      1.34      0.24      1.31      0.97      1.70    865.99      1.00\n",
      "lams[25,0]      0.29      0.05      0.29      0.20      0.37   2287.36      1.00\n",
      "lams[25,1]      1.17      0.18      1.15      0.84      1.41   1094.45      1.00\n",
      "lams[26,0]      0.86      0.17      0.83      0.62      1.12    695.59      1.00\n",
      "lams[26,1]      1.18      0.23      1.14      0.84      1.52    743.79      1.00\n",
      "lams[27,0]     -0.49      0.20     -0.46     -0.77     -0.16   1124.50      1.00\n",
      "lams[27,1]      1.04      0.15      1.02      0.81      1.26   1132.29      1.00\n",
      "lams[28,0]     -0.80      0.27     -0.76     -1.20     -0.38    909.72      1.00\n",
      "lams[28,1]      1.73      0.30      1.70      1.27      2.18    911.56      1.00\n",
      "lams[29,0]      0.02      0.07      0.03     -0.09      0.13   1240.09      1.00\n",
      "lams[29,1]      0.89      0.09      0.88      0.74      1.03   1280.55      1.00\n",
      "lams[30,0]     -0.08      0.11     -0.06     -0.24      0.09   1005.57      1.00\n",
      "lams[30,1]      1.22      0.17      1.20      0.93      1.45   1038.60      1.00\n",
      "lams[31,0]      0.58      0.06      0.57      0.49      0.69    645.45      1.00\n",
      "lams[31,1]      1.05      0.15      1.03      0.82      1.27    649.67      1.00\n",
      "lams[32,0]     -0.63      0.25     -0.58     -0.99     -0.25    609.28      1.00\n",
      "lams[32,1]      0.88      0.14      0.86      0.67      1.09    600.62      1.00\n",
      "lams[33,0]      1.94      0.78      1.82      0.78      3.03    958.39      1.00\n",
      "lams[33,1]      2.16      0.78      2.02      0.96      3.25    977.32      1.00\n",
      "lams[34,0]      0.25      0.06      0.25      0.13      0.34   1890.22      1.00\n",
      "lams[34,1]      1.25      0.20      1.22      0.93      1.52    862.08      1.00\n",
      "lams[35,0]      0.43      0.04      0.42      0.37      0.49   1659.03      1.00\n",
      "lams[35,1]      0.87      0.09      0.86      0.72      1.02   1017.77      1.00\n",
      "lams[36,0]     -0.41      0.20     -0.38     -0.71     -0.09    863.87      1.00\n",
      "lams[36,1]      1.22      0.19      1.19      0.93      1.50    838.53      1.00\n",
      "lams[37,0]     -0.30      0.19     -0.28     -0.61     -0.03    860.34      1.01\n",
      "lams[37,1]      1.35      0.22      1.31      1.02      1.71    791.01      1.01\n",
      "lams[38,0]      0.04      0.09      0.05     -0.09      0.18   1450.89      1.00\n",
      "lams[38,1]      1.18      0.17      1.16      0.93      1.45   1060.65      1.00\n",
      "lams[39,0]     -0.30      0.18     -0.27     -0.57     -0.02    722.49      1.00\n",
      "lams[39,1]      1.98      0.38      1.93      1.38      2.52    800.67      1.00\n",
      "lams[40,0]      0.16      0.06      0.16      0.05      0.25    871.64      1.00\n",
      "lams[40,1]      0.97      0.14      0.95      0.77      1.16    686.53      1.00\n",
      "lams[41,0]      1.66      0.67      1.52      0.72      2.62    740.92      1.00\n",
      "lams[41,1]      2.23      0.68      2.10      1.18      3.23    745.99      1.00\n",
      "lams[42,0]      1.49      0.61      1.34      0.67      2.37    627.00      1.00\n",
      "lams[42,1]      2.14      0.64      2.03      1.21      3.05    647.46      1.00\n",
      "lams[43,0]      1.82      0.84      1.66      0.62      3.11    727.41      1.00\n",
      "lams[43,1]      2.78      0.83      2.65      1.52      4.08    787.82      1.00\n",
      "lams[44,0]      1.64      0.73      1.49      0.58      2.61    762.78      1.00\n",
      "lams[44,1]      2.65      0.75      2.52      1.60      3.88    848.30      1.00\n",
      "lams[45,0]      1.78      0.71      1.62      0.74      2.81    693.62      1.00\n",
      "lams[45,1]      2.10      0.72      1.94      1.04      3.19    703.13      1.00\n",
      "lams[46,0]      2.02      0.65      1.92      1.10      3.01    811.56      1.00\n",
      "lams[46,1]      2.68      0.67      2.58      1.68      3.76    800.05      1.00\n",
      "lams[47,0]      2.14      0.68      2.02      1.11      3.12    604.40      1.01\n",
      "lams[47,1]      2.67      0.70      2.57      1.73      3.84    651.19      1.01\n",
      "lams[48,0]      1.57      0.62      1.43      0.70      2.52    713.60      1.00\n",
      "lams[48,1]      2.12      0.63      2.00      1.22      3.15    760.03      1.00\n",
      "lams[49,0]      1.50      0.67      1.34      0.60      2.48    515.53      1.00\n",
      "lams[49,1]      2.36      0.71      2.23      1.30      3.35    509.17      1.00\n",
      "lams[50,0]      1.73      0.63      1.60      0.87      2.67    609.03      1.00\n",
      "lams[50,1]      2.37      0.65      2.24      1.41      3.31    637.52      1.00\n",
      "lams[51,0]      1.46      0.71      1.30      0.46      2.47    711.71      1.00\n",
      "lams[51,1]      2.37      0.72      2.23      1.27      3.39    666.69      1.00\n",
      "lams[52,0]     -0.31      0.17     -0.28     -0.57     -0.04   1042.84      1.00\n",
      "lams[52,1]      1.23      0.17      1.20      0.96      1.48   1007.78      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.26   1339.66      1.00\n",
      "lams[53,1]      0.69      0.05      0.69      0.61      0.76   1343.72      1.00\n",
      "lams[54,0]      0.07      0.09      0.08     -0.06      0.22    797.86      1.00\n",
      "lams[54,1]      1.07      0.14      1.06      0.86      1.28    705.08      1.00\n",
      "lams[55,0]      1.75      0.67      1.60      0.82      2.79    810.10      1.00\n",
      "lams[55,1]      2.69      0.71      2.56      1.65      3.77    893.52      1.00\n",
      "lams[56,0]      1.44      0.65      1.30      0.55      2.33    404.43      1.00\n",
      "lams[56,1]      2.27      0.68      2.13      1.23      3.18    448.64      1.00\n",
      "lams[57,0]      0.19      0.05      0.19      0.11      0.28    844.93      1.00\n",
      "lams[57,1]      1.05      0.13      1.03      0.84      1.24    661.46      1.01\n",
      "lams[58,0]      0.52      0.05      0.52      0.43      0.59   1499.30      1.00\n",
      "lams[58,1]      1.15      0.16      1.12      0.89      1.38   1111.44      1.00\n",
      "lams[59,0]      0.27      0.06      0.28      0.20      0.36    847.67      1.00\n",
      "lams[59,1]      1.13      0.15      1.11      0.90      1.35    798.03      1.00\n",
      "lams[60,0]     -0.92      0.32     -0.87     -1.39     -0.43    771.81      1.00\n",
      "lams[60,1]      1.83      0.32      1.78      1.36      2.34    780.10      1.00\n",
      "lams[61,0]     -0.39      0.19     -0.35     -0.70     -0.10   1070.20      1.00\n",
      "lams[61,1]      1.10      0.15      1.08      0.88      1.36   1020.05      1.00\n",
      "lams[62,0]     -0.28      0.13     -0.27     -0.48     -0.10    733.85      1.00\n",
      "lams[62,1]      0.94      0.11      0.93      0.76      1.09    702.91      1.00\n",
      "lams[63,0]     -0.63      0.28     -0.59     -1.04     -0.20    619.31      1.00\n",
      "lams[63,1]      1.73      0.31      1.69      1.28      2.19    584.49      1.00\n",
      "lams[64,0]      1.64      0.64      1.49      0.76      2.59    654.27      1.00\n",
      "lams[64,1]      2.18      0.66      2.05      1.18      3.11    682.67      1.00\n",
      "lams[65,0]     -0.03      0.13     -0.01     -0.24      0.16    970.61      1.00\n",
      "lams[65,1]      1.74      0.31      1.70      1.29      2.22    765.62      1.00\n",
      "lams[66,0]      0.08      0.07      0.08     -0.03      0.18   1026.08      1.00\n",
      "lams[66,1]      0.86      0.08      0.85      0.75      0.99    930.55      1.00\n",
      "lams[67,0]     -0.32      0.13     -0.30     -0.51     -0.11    759.35      1.00\n",
      "lams[67,1]      0.96      0.12      0.95      0.77      1.14    767.10      1.00\n",
      "lams[68,0]      0.10      0.06      0.10      0.01      0.19   1484.41      1.00\n",
      "lams[68,1]      0.88      0.10      0.87      0.72      1.02   1529.16      1.00\n",
      "lams[69,0]     -0.19      0.15     -0.17     -0.43      0.04    788.52      1.00\n",
      "lams[69,1]      1.11      0.15      1.09      0.88      1.34    705.50      1.00\n",
      "lams[70,0]      0.36      0.08      0.37      0.23      0.47   2180.11      1.00\n",
      "lams[70,1]      1.58      0.27      1.53      1.20      2.01   1024.17      1.00\n",
      "lams[71,0]     -0.84      0.29     -0.80     -1.27     -0.38    797.70      1.00\n",
      "lams[71,1]      1.84      0.32      1.79      1.39      2.39    838.80      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.06      0.24    946.79      1.00\n",
      "lams[72,1]      0.99      0.11      0.97      0.82      1.13    625.60      1.00\n",
      "lams[73,0]     -1.07      0.34     -1.03     -1.61     -0.58    757.86      1.00\n",
      "lams[73,1]      1.23      0.21      1.20      0.94      1.57    740.07      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2500/2500 [1:02:23<00:00,  1.50s/it, 511 steps of size 9.02e-03. acc. prob=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected log joint density: -26056.21\n",
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  gamma[0]     10.02      2.20      9.87      6.56     13.47   2234.43      1.00\n",
      "  gamma[1]      7.82      1.69      7.78      5.22     10.63   1890.38      1.00\n",
      "  gamma[2]      9.99      2.26      9.75      6.42     13.47   3114.33      1.00\n",
      "  gamma[3]     15.67      2.04     15.63     12.39     19.02   1732.14      1.00\n",
      "  gamma[4]      9.11      1.90      9.00      5.84     11.82   2093.53      1.00\n",
      "  gamma[5]      9.72      2.11      9.51      6.00     12.83   1796.29      1.00\n",
      "  gamma[6]     11.23      2.41     11.09      7.03     14.68   1865.71      1.00\n",
      "  gamma[7]     11.23      2.31     11.14      7.58     15.01   1808.03      1.00\n",
      "  gamma[8]     10.34      2.27     10.17      6.52     13.81   1664.79      1.00\n",
      "  gamma[9]     10.24      2.21     10.04      6.64     13.85   1572.25      1.00\n",
      " gamma[10]      8.39      1.76      8.26      5.55     11.15   1465.25      1.00\n",
      " gamma[11]      9.42      2.04      9.25      6.12     12.60   1638.14      1.00\n",
      " gamma[12]      9.47      2.06      9.22      6.12     12.59   1271.17      1.00\n",
      " gamma[13]     10.42      2.32     10.28      6.53     14.14   2164.69      1.00\n",
      " gamma[14]     10.75      2.36     10.66      6.68     14.28    889.14      1.00\n",
      " gamma[15]      5.74      1.17      5.60      3.81      7.50   1014.34      1.00\n",
      " gamma[16]      7.23      1.43      7.16      5.05      9.53   1188.22      1.00\n",
      " gamma[17]      6.18      0.75      6.18      4.97      7.42    729.75      1.00\n",
      " gamma[18]      6.05      1.04      6.04      4.25      7.61    836.59      1.00\n",
      " gamma[19]      4.03      0.71      4.01      2.88      5.15    594.59      1.00\n",
      " gamma[20]     10.16      2.29      9.99      6.78     14.22   1481.67      1.00\n",
      " gamma[21]      9.52      2.08      9.34      5.95     12.61   1526.09      1.00\n",
      " gamma[22]      3.59      0.67      3.55      2.50      4.63   1169.57      1.00\n",
      " gamma[23]      6.84      1.49      6.75      4.56      9.35    726.11      1.00\n",
      " gamma[24]      8.86      1.90      8.73      5.88     12.08    742.87      1.00\n",
      " gamma[25]      4.32      0.81      4.29      2.97      5.62    883.98      1.00\n",
      " gamma[26]      9.45      2.12      9.31      5.95     12.77    734.76      1.00\n",
      " gamma[27]      5.57      0.96      5.50      4.01      7.10    563.68      1.00\n",
      " gamma[28]      2.67      0.51      2.65      1.90      3.52    612.48      1.00\n",
      " gamma[29]      6.49      0.95      6.45      4.95      8.04    656.84      1.00\n",
      " gamma[30]      5.04      0.95      4.98      3.39      6.45    819.19      1.00\n",
      " gamma[31]      6.45      1.22      6.40      4.42      8.28    697.03      1.00\n",
      " gamma[32]      6.55      1.05      6.52      4.77      8.21    953.03      1.00\n",
      " gamma[33]      9.25      1.91      9.11      6.57     12.63   1734.58      1.00\n",
      " gamma[34]      3.41      0.63      3.40      2.34      4.38    870.77      1.00\n",
      " gamma[35]      5.69      0.86      5.69      4.14      6.96   1066.45      1.00\n",
      " gamma[36]      5.45      0.95      5.40      3.69      6.85    687.07      1.00\n",
      " gamma[37]      5.96      1.15      5.88      3.92      7.69    579.88      1.00\n",
      " gamma[38]      3.52      0.63      3.49      2.49      4.55    639.28      1.00\n",
      " gamma[39]      2.15      0.39      2.13      1.56      2.83    686.03      1.00\n",
      " gamma[40]      4.07      0.66      4.03      3.04      5.19    884.00      1.00\n",
      " gamma[41]      8.35      1.69      8.30      5.73     11.07   2035.53      1.00\n",
      " gamma[42]      8.35      1.78      8.22      5.57     11.23   1649.43      1.00\n",
      " gamma[43]      7.57      1.70      7.45      5.16     10.58   2052.34      1.00\n",
      " gamma[44]      7.20      1.51      7.08      4.57      9.37   1928.82      1.00\n",
      " gamma[45]     10.07      2.12      9.97      6.77     13.54   1595.71      1.00\n",
      " gamma[46]      4.88      0.86      4.83      3.41      6.20   1499.68      1.00\n",
      " gamma[47]      4.54      0.75      4.51      3.23      5.69   1693.87      1.00\n",
      " gamma[48]      8.44      1.81      8.27      5.36     11.17   2013.04      1.00\n",
      " gamma[49]      7.85      1.56      7.77      5.17     10.21   1493.18      1.00\n",
      " gamma[50]      6.41      1.21      6.37      4.48      8.30   1582.25      1.00\n",
      " gamma[51]      9.15      1.75      9.13      6.37     11.92   1726.98      1.00\n",
      " gamma[52]      8.04      1.40      7.95      5.94     10.60    712.43      1.00\n",
      " gamma[53]      9.76      1.18      9.69      7.91     11.77    780.19      1.00\n",
      " gamma[54]      9.26      1.78      9.20      6.54     12.29    431.39      1.00\n",
      " gamma[55]      5.80      1.15      5.75      3.76      7.50   1868.61      1.00\n",
      " gamma[56]      8.74      1.93      8.55      5.71     12.08   1346.09      1.00\n",
      " gamma[57]      4.74      0.78      4.72      3.31      5.85    779.85      1.00\n",
      " gamma[58]      6.87      1.47      6.76      4.56      9.28    742.48      1.00\n",
      " gamma[59]      7.53      1.45      7.49      5.23      9.83    614.38      1.00\n",
      " gamma[60]      3.57      0.69      3.54      2.57      4.77    617.20      1.00\n",
      " gamma[61]      9.18      1.59      9.08      6.67     11.79    781.60      1.00\n",
      " gamma[62]      5.95      0.83      5.90      4.61      7.30    770.96      1.00\n",
      " gamma[63]      5.32      1.09      5.23      3.54      7.04    712.21      1.00\n",
      " gamma[64]      8.76      1.92      8.64      5.84     12.01   1909.88      1.00\n",
      " gamma[65]      4.07      0.87      3.97      2.66      5.47    687.64      1.00\n",
      " gamma[66]      9.70      1.39      9.63      7.44     11.95    690.68      1.00\n",
      " gamma[67]      4.79      0.69      4.78      3.66      5.88    509.09      1.00\n",
      " gamma[68]      4.98      0.78      5.00      3.73      6.23    500.77      1.01\n",
      " gamma[69]      9.23      1.64      9.14      6.28     11.71    658.28      1.00\n",
      " gamma[70]      6.08      1.33      6.00      3.87      8.17    904.80      1.00\n",
      " gamma[71]      3.13      0.64      3.11      1.98      4.02    788.47      1.00\n",
      " gamma[72]      7.86      1.26      7.79      5.86      9.86    879.17      1.00\n",
      " gamma[73]      4.91      0.85      4.92      3.48      6.28    788.43      1.00\n",
      " lams[0,0]      1.65      0.75      1.48      0.65      2.69    437.97      1.00\n",
      " lams[0,1]      2.09      0.75      1.95      1.01      3.14    462.82      1.00\n",
      " lams[1,0]      1.75      0.76      1.60      0.64      2.81    625.94      1.00\n",
      " lams[1,1]      2.41      0.76      2.29      1.30      3.63    617.03      1.00\n",
      " lams[2,0]      1.61      0.81      1.40      0.53      2.81    501.10      1.00\n",
      " lams[2,1]      2.26      0.81      2.10      1.12      3.43    521.19      1.00\n",
      " lams[3,0]      7.73      0.90      7.71      6.28      9.19   1053.27      1.00\n",
      " lams[3,1]      7.78      0.89      7.76      6.34      9.21   1049.40      1.00\n",
      " lams[4,0]      1.67      0.64      1.53      0.79      2.57    544.48      1.00\n",
      " lams[4,1]      1.90      0.65      1.77      0.95      2.81    550.95      1.00\n",
      " lams[5,0]      1.62      0.68      1.45      0.70      2.64    534.13      1.00\n",
      " lams[5,1]      1.97      0.69      1.80      0.96      2.97    533.72      1.00\n",
      " lams[6,0]      1.68      0.74      1.49      0.71      2.79    414.63      1.00\n",
      " lams[6,1]      1.94      0.74      1.75      0.99      3.07    411.64      1.00\n",
      " lams[7,0]      1.61      0.75      1.41      0.63      2.75    584.46      1.00\n",
      " lams[7,1]      2.04      0.75      1.86      1.00      3.11    587.21      1.00\n",
      " lams[8,0]      1.66      0.78      1.47      0.61      2.72    439.91      1.00\n",
      " lams[8,1]      2.18      0.77      2.00      1.13      3.30    459.63      1.00\n",
      " lams[9,0]      1.66      0.75      1.50      0.56      2.70    554.77      1.00\n",
      " lams[9,1]      2.18      0.75      2.05      1.18      3.36    584.35      1.00\n",
      "lams[10,0]      1.58      0.66      1.40      0.71      2.63    610.55      1.00\n",
      "lams[10,1]      2.11      0.68      1.96      1.05      3.08    595.90      1.00\n",
      "lams[11,0]      1.76      0.76      1.60      0.71      2.84    531.85      1.00\n",
      "lams[11,1]      2.15      0.77      2.00      1.07      3.28    525.76      1.00\n",
      "lams[12,0]      1.67      0.71      1.50      0.72      2.66    447.53      1.00\n",
      "lams[12,1]      2.07      0.70      1.91      1.14      3.12    444.67      1.00\n",
      "lams[13,0]      1.54      0.75      1.37      0.49      2.57    604.88      1.00\n",
      "lams[13,1]      2.19      0.75      2.05      1.11      3.27    625.72      1.00\n",
      "lams[14,0]      0.70      0.15      0.67      0.50      0.94    486.82      1.00\n",
      "lams[14,1]      1.22      0.23      1.17      0.90      1.59    448.00      1.00\n",
      "lams[15,0]      1.11      0.27      1.06      0.69      1.50    752.07      1.00\n",
      "lams[15,1]      1.78      0.36      1.73      1.25      2.37    674.74      1.00\n",
      "lams[16,0]      1.82      0.65      1.68      0.91      2.74    530.73      1.01\n",
      "lams[16,1]      2.14      0.67      2.03      1.18      3.11    520.76      1.01\n",
      "lams[17,0]     -0.22      0.09     -0.21     -0.36     -0.08    643.87      1.00\n",
      "lams[17,1]      0.80      0.07      0.79      0.68      0.92    635.05      1.00\n",
      "lams[18,0]      0.26      0.05      0.27      0.19      0.34    941.53      1.00\n",
      "lams[18,1]      0.97      0.12      0.95      0.79      1.15    631.76      1.00\n",
      "lams[19,0]     -0.27      0.15     -0.26     -0.49     -0.03    533.71      1.00\n",
      "lams[19,1]      1.37      0.20      1.34      1.06      1.68    488.86      1.00\n",
      "lams[20,0]      1.58      0.74      1.40      0.59      2.68    542.04      1.01\n",
      "lams[20,1]      2.14      0.75      1.97      1.06      3.33    565.29      1.01\n",
      "lams[21,0]      1.47      0.57      1.35      0.67      2.28    489.02      1.00\n",
      "lams[21,1]      1.86      0.59      1.76      0.95      2.69    487.77      1.00\n",
      "lams[22,0]      0.27      0.05      0.28      0.18      0.35   1696.80      1.00\n",
      "lams[22,1]      1.18      0.18      1.15      0.93      1.48   1014.95      1.00\n",
      "lams[23,0]      0.79      0.13      0.77      0.60      0.98    562.41      1.00\n",
      "lams[23,1]      1.26      0.22      1.23      0.91      1.59    517.47      1.00\n",
      "lams[24,0]      0.82      0.18      0.79      0.54      1.06    392.33      1.00\n",
      "lams[24,1]      1.34      0.27      1.30      0.94      1.72    380.36      1.00\n",
      "lams[25,0]      0.29      0.05      0.29      0.20      0.37   2032.11      1.00\n",
      "lams[25,1]      1.16      0.18      1.14      0.90      1.45    693.95      1.00\n",
      "lams[26,0]      0.88      0.19      0.85      0.60      1.12    344.97      1.00\n",
      "lams[26,1]      1.20      0.25      1.16      0.84      1.54    340.56      1.00\n",
      "lams[27,0]     -0.53      0.22     -0.50     -0.88     -0.21    458.24      1.00\n",
      "lams[27,1]      1.08      0.17      1.06      0.80      1.32    380.95      1.00\n",
      "lams[28,0]     -0.78      0.27     -0.75     -1.16     -0.36    400.20      1.00\n",
      "lams[28,1]      1.72      0.31      1.68      1.22      2.15    441.04      1.00\n",
      "lams[29,0]      0.01      0.07      0.02     -0.10      0.13    709.48      1.00\n",
      "lams[29,1]      0.91      0.10      0.90      0.76      1.05    591.12      1.00\n",
      "lams[30,0]     -0.09      0.12     -0.07     -0.26      0.10    589.22      1.01\n",
      "lams[30,1]      1.25      0.19      1.23      0.94      1.52    619.46      1.00\n",
      "lams[31,0]      0.59      0.06      0.58      0.48      0.68    685.47      1.00\n",
      "lams[31,1]      1.06      0.15      1.04      0.85      1.30    581.12      1.00\n",
      "lams[32,0]     -0.72      0.26     -0.67     -1.12     -0.31    641.17      1.00\n",
      "lams[32,1]      0.94      0.15      0.92      0.73      1.20    639.84      1.00\n",
      "lams[33,0]      1.96      0.82      1.77      0.84      3.16    376.18      1.01\n",
      "lams[33,1]      2.17      0.82      2.01      1.01      3.35    384.40      1.01\n",
      "lams[34,0]      0.25      0.06      0.25      0.15      0.35   1289.09      1.00\n",
      "lams[34,1]      1.25      0.19      1.22      0.95      1.52    639.79      1.00\n",
      "lams[35,0]      0.43      0.04      0.42      0.37      0.48   1277.35      1.00\n",
      "lams[35,1]      0.87      0.09      0.85      0.73      1.02    791.55      1.00\n",
      "lams[36,0]     -0.42      0.19     -0.40     -0.75     -0.14    627.40      1.00\n",
      "lams[36,1]      1.24      0.19      1.21      0.95      1.53    594.74      1.00\n",
      "lams[37,0]     -0.29      0.19     -0.27     -0.56      0.00    451.68      1.00\n",
      "lams[37,1]      1.36      0.22      1.32      1.00      1.70    465.20      1.00\n",
      "lams[38,0]      0.03      0.09      0.04     -0.10      0.17    557.21      1.00\n",
      "lams[38,1]      1.21      0.19      1.19      0.89      1.44    495.57      1.01\n",
      "lams[39,0]     -0.26      0.16     -0.25     -0.51     -0.03    791.69      1.00\n",
      "lams[39,1]      1.94      0.32      1.90      1.46      2.43    618.25      1.00\n",
      "lams[40,0]      0.16      0.06      0.17      0.07      0.25    813.57      1.00\n",
      "lams[40,1]      0.95      0.12      0.94      0.76      1.12    643.97      1.00\n",
      "lams[41,0]      1.63      0.60      1.52      0.79      2.57    581.93      1.00\n",
      "lams[41,1]      2.19      0.62      2.08      1.19      3.09    599.40      1.00\n",
      "lams[42,0]      1.52      0.63      1.39      0.65      2.47    398.34      1.00\n",
      "lams[42,1]      2.18      0.66      2.07      1.16      3.15    393.79      1.00\n",
      "lams[43,0]      1.79      0.83      1.64      0.52      3.03    490.39      1.00\n",
      "lams[43,1]      2.76      0.84      2.65      1.49      4.06    463.13      1.00\n",
      "lams[44,0]      1.65      0.79      1.50      0.57      2.71    565.24      1.00\n",
      "lams[44,1]      2.66      0.80      2.55      1.48      3.77    557.66      1.00\n",
      "lams[45,0]      1.74      0.69      1.56      0.83      2.73    504.14      1.00\n",
      "lams[45,1]      2.06      0.70      1.90      1.09      3.08    517.76      1.00\n",
      "lams[46,0]      1.97      0.62      1.85      1.08      2.88    369.47      1.00\n",
      "lams[46,1]      2.62      0.65      2.52      1.64      3.60    372.98      1.00\n",
      "lams[47,0]      2.14      0.63      2.01      1.19      3.02    386.04      1.00\n",
      "lams[47,1]      2.68      0.65      2.57      1.69      3.64    399.71      1.00\n",
      "lams[48,0]      1.54      0.60      1.40      0.70      2.33    506.28      1.00\n",
      "lams[48,1]      2.09      0.62      1.95      1.14      2.95    509.90      1.00\n",
      "lams[49,0]      1.47      0.62      1.31      0.66      2.40    639.57      1.00\n",
      "lams[49,1]      2.33      0.66      2.18      1.44      3.31    665.26      1.00\n",
      "lams[50,0]      1.76      0.64      1.63      0.81      2.68    534.06      1.00\n",
      "lams[50,1]      2.42      0.67      2.30      1.42      3.43    494.78      1.00\n",
      "lams[51,0]      1.45      0.73      1.27      0.47      2.59    579.79      1.00\n",
      "lams[51,1]      2.37      0.75      2.19      1.29      3.50    573.27      1.00\n",
      "lams[52,0]     -0.31      0.18     -0.28     -0.60     -0.03    583.31      1.00\n",
      "lams[52,1]      1.24      0.18      1.21      0.94      1.50    570.35      1.00\n",
      "lams[53,0]      0.21      0.03      0.21      0.16      0.27    808.88      1.00\n",
      "lams[53,1]      0.69      0.05      0.69      0.61      0.76    721.71      1.00\n",
      "lams[54,0]      0.06      0.10      0.07     -0.11      0.20    298.64      1.00\n",
      "lams[54,1]      1.09      0.15      1.06      0.88      1.32    293.56      1.00\n",
      "lams[55,0]      1.76      0.69      1.64      0.82      2.69    459.50      1.00\n",
      "lams[55,1]      2.72      0.72      2.61      1.66      3.75    500.88      1.00\n",
      "lams[56,0]      1.43      0.65      1.27      0.54      2.46    431.36      1.01\n",
      "lams[56,1]      2.28      0.68      2.13      1.25      3.32    461.63      1.01\n",
      "lams[57,0]      0.19      0.05      0.19      0.10      0.27    760.56      1.00\n",
      "lams[57,1]      1.05      0.13      1.03      0.84      1.24    665.72      1.00\n",
      "lams[58,0]      0.52      0.06      0.52      0.43      0.60    701.56      1.00\n",
      "lams[58,1]      1.17      0.18      1.15      0.90      1.44    424.68      1.00\n",
      "lams[59,0]      0.27      0.06      0.27      0.17      0.35    519.18      1.00\n",
      "lams[59,1]      1.16      0.17      1.13      0.92      1.40    392.44      1.00\n",
      "lams[60,0]     -0.90      0.32     -0.87     -1.41     -0.44    508.88      1.00\n",
      "lams[60,1]      1.82      0.33      1.78      1.32      2.31    475.17      1.00\n",
      "lams[61,0]     -0.41      0.21     -0.38     -0.73     -0.09    711.03      1.00\n",
      "lams[61,1]      1.12      0.17      1.09      0.87      1.38    673.96      1.00\n",
      "lams[62,0]     -0.28      0.12     -0.27     -0.47     -0.10    678.88      1.00\n",
      "lams[62,1]      0.94      0.10      0.93      0.78      1.09    669.53      1.00\n",
      "lams[63,0]     -0.64      0.27     -0.60     -1.09     -0.22    578.68      1.00\n",
      "lams[63,1]      1.74      0.31      1.70      1.27      2.24    524.98      1.00\n",
      "lams[64,0]      1.62      0.67      1.45      0.75      2.62    459.07      1.01\n",
      "lams[64,1]      2.14      0.69      2.00      1.21      3.16    449.50      1.01\n",
      "lams[65,0]     -0.03      0.14     -0.02     -0.24      0.18    664.52      1.00\n",
      "lams[65,1]      1.76      0.31      1.73      1.28      2.26    508.71      1.00\n",
      "lams[66,0]      0.08      0.07      0.08     -0.03      0.18    591.88      1.00\n",
      "lams[66,1]      0.86      0.08      0.85      0.73      0.96    572.36      1.00\n",
      "lams[67,0]     -0.32      0.13     -0.30     -0.50     -0.12    492.11      1.00\n",
      "lams[67,1]      0.96      0.11      0.95      0.81      1.15    458.42      1.00\n",
      "lams[68,0]      0.09      0.06      0.10      0.01      0.19    349.04      1.01\n",
      "lams[68,1]      0.88      0.10      0.86      0.71      1.03    375.32      1.01\n",
      "lams[69,0]     -0.20      0.15     -0.18     -0.42      0.04    478.54      1.00\n",
      "lams[69,1]      1.12      0.15      1.11      0.88      1.34    474.32      1.00\n",
      "lams[70,0]      0.37      0.08      0.37      0.25      0.51   1436.96      1.00\n",
      "lams[70,1]      1.59      0.29      1.55      1.13      2.00    707.43      1.00\n",
      "lams[71,0]     -0.86      0.31     -0.82     -1.32     -0.38    489.30      1.00\n",
      "lams[71,1]      1.87      0.34      1.82      1.33      2.38    473.08      1.00\n",
      "lams[72,0]      0.14      0.06      0.15      0.05      0.25    839.20      1.00\n",
      "lams[72,1]      0.99      0.10      0.98      0.85      1.17    723.92      1.00\n",
      "lams[73,0]     -1.08      0.36     -1.02     -1.62     -0.54    606.59      1.00\n",
      "lams[73,1]      1.24      0.22      1.20      0.91      1.57    611.89      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=1500, num_samples=1000)\n",
    "\n",
    "rng_key = random.PRNGKey(23478)\n",
    "cutoff = 800\n",
    "samples = {}\n",
    "for nu_max in sequences:\n",
    "    rng_key, _rng_key = random.split(rng_key)\n",
    "    seq = (sequences[nu_max]['beliefs'][0][-cutoff:], \n",
    "           sequences[nu_max]['beliefs'][1][-cutoff:])\n",
    "    mcmc.run(\n",
    "        _rng_key, \n",
    "        seq, \n",
    "        agents[nu_max], \n",
    "        y=responses_data[-cutoff:], \n",
    "        mask=mask_data[-cutoff:].astype(bool), \n",
    "        extra_fields=('potential_energy',)\n",
    "    )\n",
    "    \n",
    "    pe = mcmc.get_extra_fields()['potential_energy']\n",
    "    print('Expected log joint density: {:.2f}'.format(jnp.mean(-pe)))\n",
    "    \n",
    "    mcmc.print_summary()\n",
    "    samples[nu_max] = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictive log likelihood\n",
    "res_waic = []\n",
    "for nu_max in sequences: \n",
    "    seq = (sequences[nu_max]['beliefs'][0][-cutoff:], sequences[nu_max]['beliefs'][1][-cutoff:])\n",
    "    res_waic.append( log_pred_density(model, samples[nu_max], seq, agents[nu_max], y=responses_data[-cutoff:], mask=mask_data[-cutoff:])['waic'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jax import nn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "sns.heatmap(nn.softmax(jnp.stack(res_waic, -1)[:27], -1), ax=axes[0])\n",
    "sns.heatmap(nn.softmax(jnp.stack(res_waic, -1)[27:], -1), ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.distributions.distribution import Distribution\n",
    "from numpyro.distributions import constraints\n",
    "from numpyro.distributions.util import validate_sample\n",
    "\n",
    "class CategoricalMixture(Distribution):\n",
    "    arg_constraints = {'logits': constraints.real_vector}\n",
    "    has_enumerate_support = False\n",
    "    is_discrete = True\n",
    "\n",
    "    def __init__(self, logits, weights, mask, validate_args=None):\n",
    "        if jnp.ndim(logits) < 1:\n",
    "            raise ValueError(\"`logits` parameter must be at least one-dimensional.\")\n",
    "        self.logits = logits\n",
    "        self.weights = weights\n",
    "        self.mask = mask\n",
    "        super(CategoricalMixture, self).__init__(batch_shape=jnp.shape(weights)[:-1],\n",
    "                                                validate_args=validate_args)\n",
    "    def sample(self, key, sample_shape=()):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    @validate_sample\n",
    "    def log_prob(self, value):\n",
    "        mask = jnp.expand_dims(self.mask, -1)\n",
    "        one_hot_value = jnp.eye(3)[value]\n",
    "        logits = jnp.sum(jnp.expand_dims(one_hot_value, -1) * self.logits, -2) * mask\n",
    "        log_z = logsumexp(self.logits, -2) * mask\n",
    "        log_pmf = logsumexp(jnp.sum(logits - log_z, -3) + jnp.log(self.weights), -1)\n",
    "\n",
    "        return log_pmf\n",
    "\n",
    "    @property\n",
    "    def support(self):\n",
    "        return constraints.integer_interval(0, jnp.shape(self.logits)[-2] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_model(sequences, agents, mask, y=None):\n",
    "    D = len(sequences)\n",
    "    T, N = sequences[1]['beliefs'][0].shape[:2]\n",
    "    \n",
    "    tau = npyro.sample('tau', dist.HalfCauchy(1.))\n",
    "    with npyro.plate('N', N):\n",
    "        weights = npyro.sample('weights', dist.Dirichlet(tau * jnp.ones(D)))\n",
    "        with npyro.plate('D', D):\n",
    "            gamma = npyro.sample('gamma', dist.Gamma(20., 2.))\n",
    "            prob = npyro.sample('prob', dist.Dirichlet(jnp.array([1., 5., 4.])))\n",
    "\n",
    "        def vec_func(i, nu):\n",
    "            U = jnp.log(jnp.stack([prob[i, :, 0], prob[i, :, 1], prob[i, :, 2]/2, prob[i, :, 2]/2], -1))\n",
    "            res = agents[nu].logits(\n",
    "                    sequences[nu]['beliefs'], \n",
    "                    jnp.expand_dims(gamma[i], -1), \n",
    "                    1., \n",
    "                    jnp.expand_dims(U, -2)\n",
    "                  )\n",
    "            return res\n",
    "        logits = []\n",
    "        for i, nu in enumerate(sequences):\n",
    "            logits.append(vec_func(i, nu))\n",
    "\n",
    "        obs = npyro.sample('obs', CategoricalMixture(logits, weights, mask), obs=y)\n",
    "\n",
    "# with npyro.handlers.seed(rng_seed=random.PRNGKey(0)):\n",
    "#     mixture_model(sequences, agents, mask_data, y=responses_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[[-0.7033054  -0.47175476 -0.        ]\n  [-0.21759875 -0.         -0.05265158]\n  [-0.         -0.6987022  -0.65153885]\n  ...\n  [-1.8429785  -1.487565   -0.        ]\n  [-0.03948966 -0.         -0.09108281]\n  [-0.3597215  -0.         -0.3772351 ]]\n\n [[-0.76330316 -0.64973587 -0.        ]\n  [-0.1782569  -0.08730344 -0.        ]\n  [-0.         -0.2992027  -0.18282089]\n  ...\n  [-1.8419564  -1.4853681  -0.        ]\n  [-0.03961568 -0.         -0.09135436]\n  [-0.3605111  -0.         -0.3781346 ]]\n\n [[-0.731059   -0.5259844  -0.        ]\n  [-0.19031341 -0.         -0.01414236]\n  [-0.         -0.6161661  -0.52285767]\n  ...\n  [-1.99934    -1.8792112  -0.        ]\n  [-0.02140291 -0.         -0.04218084]\n  [-0.12390733 -0.         -0.16076677]]\n\n ...\n\n [[-0.7168098  -0.5062812  -0.        ]\n  [-0.20128258 -0.         -0.03573397]\n  [-0.         -0.65709496 -0.65724784]\n  ...\n  [-1.6025221  -1.8437862  -0.        ]\n  [-0.         -0.02161165 -0.07492917]\n  [-0.         -0.2668863  -0.30240908]]\n\n [[-0.7111022  -0.49571267 -0.        ]\n  [-0.20724443 -0.         -0.04456911]\n  [-0.         -0.65824246 -0.6587337 ]\n  ...\n  [-1.506151   -1.7895727  -0.        ]\n  [-0.         -0.02653998 -0.08760779]\n  [-0.         -0.29659012 -0.33980873]]\n\n [[-0.76823443 -0.6745244  -0.        ]\n  [-0.17305663 -0.09348579 -0.        ]\n  [-0.         -0.27002108 -0.19312783]\n  ...\n  [-1.4817543  -1.7750046  -0.        ]\n  [-0.         -0.0278604  -0.09126941]\n  [-0.         -0.30286664 -0.34831122]]])>with<JVPTrace(level=2/0)>\n  with primal = DeviceArray([[[-0.7033054 , -0.47175476, -0.        ],\n                              [-0.21759875, -0.        , -0.05265158],\n                              [-0.        , -0.6987022 , -0.65153885],\n                              ...,\n                              [-1.8429785 , -1.487565  , -0.        ],\n                              [-0.03948966, -0.        , -0.09108281],\n                              [-0.3597215 , -0.        , -0.3772351 ]],\n                \n                             [[-0.76330316, -0.64973587, -0.        ],\n                              [-0.1782569 , -0.08730344, -0.        ],\n                              [-0.        , -0.2992027 , -0.18282089],\n                              ...,\n                              [-1.8419564 , -1.4853681 , -0.        ],\n                              [-0.03961568, -0.        , -0.09135436],\n                              [-0.3605111 , -0.        , -0.3781346 ]],\n                \n                             [[-0.731059  , -0.5259844 , -0.        ],\n                              [-0.19031341, -0.        , -0.01414236],\n                              [-0.        , -0.6161661 , -0.52285767],\n                              ...,\n                              [-1.99934   , -1.8792112 , -0.        ],\n                              [-0.02140291, -0.        , -0.04218084],\n                              [-0.12390733, -0.        , -0.16076677]],\n                \n                             ...,\n                \n                             [[-0.7168098 , -0.5062812 , -0.        ],\n                              [-0.20128258, -0.        , -0.03573397],\n                              [-0.        , -0.65709496, -0.65724784],\n                              ...,\n                              [-1.6025221 , -1.8437862 , -0.        ],\n                              [-0.        , -0.02161165, -0.07492917],\n                              [-0.        , -0.2668863 , -0.30240908]],\n                \n                             [[-0.7111022 , -0.49571267, -0.        ],\n                              [-0.20724443, -0.        , -0.04456911],\n                              [-0.        , -0.65824246, -0.6587337 ],\n                              ...,\n                              [-1.506151  , -1.7895727 , -0.        ],\n                              [-0.        , -0.02653998, -0.08760779],\n                              [-0.        , -0.29659012, -0.33980873]],\n                \n                             [[-0.76823443, -0.6745244 , -0.        ],\n                              [-0.17305663, -0.09348579, -0.        ],\n                              [-0.        , -0.27002108, -0.19312783],\n                              ...,\n                              [-1.4817543 , -1.7750046 , -0.        ],\n                              [-0.        , -0.0278604 , -0.09126941],\n                              [-0.        , -0.30286664, -0.34831122]]], dtype=float32)\n       tangent = Traced<ShapedArray(float32[600,50,3]):JaxprTrace(level=1/0)>.\n\nThis error can occur when a JAX Tracer object is passed to a raw numpy function, or a method on a numpy.ndarray object. You might want to check that you are using `jnp` together with `import jax.numpy as jnp` rather than using `np` via `import numpy as np`. If this error arises on a line that involves array indexing, like `x[idx]`, it may be that the array being indexed `x` is a raw numpy.ndarray while the indices `idx` are a JAX Tracer instance; in that case, you can instead write `jax.device_put(x)[idx]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFilteredStackTrace\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9d1579fba4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m mcmc.run(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0m_rng_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/mcmc.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, rng_key, extra_fields, init_params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_chains\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mstates_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/mcmc.py\u001b[0m in \u001b[0;36m_single_chain_mcmc\u001b[0;34m(self, init, args, kwargs, collect_fields)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             init_state = self.sampler.init(rng_key, self.num_warmup, init_params,\n\u001b[0m\u001b[1;32m    304\u001b[0m                                            model_args=args, model_kwargs=kwargs)\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/hmc.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, rng_key, num_warmup, init_params, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_key_init_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key_init_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_potential_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/hmc.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, rng_key, model_args, model_kwargs, init_params)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             init_params, potential_fn, postprocess_fn, model_trace = initialize_model(\n\u001b[0m\u001b[1;32m    408\u001b[0m                 \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(rng_key, model, init_strategy, dynamic_args, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0mprototype_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstrained_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     (init_params, pe, grad), is_valid = find_valid_initial_params(rng_key, model,\n\u001b[0m\u001b[1;32m    441\u001b[0m                                                                   \u001b[0minit_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mfind_valid_initial_params\u001b[0;34m(rng_key, model, init_strategy, enum, model_args, model_kwargs, prototype_params)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrng_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_valid_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexit_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36m_find_valid_params\u001b[0;34m(rng_key, exit_early)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# where we can avoid compiling body_fn in while_loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnot_jax_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mbody_fn\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mpotential_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mz_grad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel_pytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mpotential_energy\u001b[0;34m(model, model_args, model_kwargs, params, enum)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# no param is needed for log_density computation because we already substitute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mlog_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_density_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstituted_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_joint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mlog_density\u001b[0;34m(model, model_args, model_kwargs, params)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubstitute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mlog_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/handlers.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3aa8efa70d33>\u001b[0m in \u001b[0;36mmixture_model\u001b[0;34m(sequences, agents, mask, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/distributions/distribution.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-975479d90fbe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logits, weights, mask, validate_args)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3109\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFilteredStackTrace\u001b[0m: Exception: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[[-0.7033054  -0.47175476 -0.        ]\n  [-0.21759875 -0.         -0.05265158]\n  [-0.         -0.6987022  -0.65153885]\n  ...\n  [-1.8429785  -1.487565   -0.        ]\n  [-0.03948966 -0.         -0.09108281]\n  [-0.3597215  -0.         -0.3772351 ]]\n\n [[-0.76330316 -0.64973587 -0.        ]\n  [-0.1782569  -0.08730344 -0.        ]\n  [-0.         -0.2992027  -0.18282089]\n  ...\n  [-1.8419564  -1.4853681  -0.        ]\n  [-0.03961568 -0.         -0.09135436]\n  [-0.3605111  -0.         -0.3781346 ]]\n\n [[-0.731059   -0.5259844  -0.        ]\n  [-0.19031341 -0.         -0.01414236]\n  [-0.         -0.6161661  -0.52285767]\n  ...\n  [-1.99934    -1.8792112  -0.        ]\n  [-0.02140291 -0.         -0.04218084]\n  [-0.12390733 -0.         -0.16076677]]\n\n ...\n\n [[-0.7168098  -0.5062812  -0.        ]\n  [-0.20128258 -0.         -0.03573397]\n  [-0.         -0.65709496 -0.65724784]\n  ...\n  [-1.6025221  -1.8437862  -0.        ]\n  [-0.         -0.02161165 -0.07492917]\n  [-0.         -0.2668863  -0.30240908]]\n\n [[-0.7111022  -0.49571267 -0.        ]\n  [-0.20724443 -0.         -0.04456911]\n  [-0.         -0.65824246 -0.6587337 ]\n  ...\n  [-1.506151   -1.7895727  -0.        ]\n  [-0.         -0.02653998 -0.08760779]\n  [-0.         -0.29659012 -0.33980873]]\n\n [[-0.76823443 -0.6745244  -0.        ]\n  [-0.17305663 -0.09348579 -0.        ]\n  [-0.         -0.27002108 -0.19312783]\n  ...\n  [-1.4817543  -1.7750046  -0.        ]\n  [-0.         -0.0278604  -0.09126941]\n  [-0.         -0.30286664 -0.34831122]]])>with<JVPTrace(level=2/0)>\n  with primal = DeviceArray([[[-0.7033054 , -0.47175476, -0.        ],\n                              [-0.21759875, -0.        , -0.05265158],\n                              [-0.        , -0.6987022 , -0.65153885],\n                              ...,\n                              [-1.8429785 , -1.487565  , -0.        ],\n                              [-0.03948966, -0.        , -0.09108281],\n                              [-0.3597215 , -0.        , -0.3772351 ]],\n                \n                             [[-0.76330316, -0.64973587, -0.        ],\n                              [-0.1782569 , -0.08730344, -0.        ],\n                              [-0.        , -0.2992027 , -0.18282089],\n                              ...,\n                              [-1.8419564 , -1.4853681 , -0.        ],\n                              [-0.03961568, -0.        , -0.09135436],\n                              [-0.3605111 , -0.        , -0.3781346 ]],\n                \n                             [[-0.731059  , -0.5259844 , -0.        ],\n                              [-0.19031341, -0.        , -0.01414236],\n                              [-0.        , -0.6161661 , -0.52285767],\n                              ...,\n                              [-1.99934   , -1.8792112 , -0.        ],\n                              [-0.02140291, -0.        , -0.04218084],\n                              [-0.12390733, -0.        , -0.16076677]],\n                \n                             ...,\n                \n                             [[-0.7168098 , -0.5062812 , -0.        ],\n                              [-0.20128258, -0.        , -0.03573397],\n                              [-0.        , -0.65709496, -0.65724784],\n                              ...,\n                              [-1.6025221 , -1.8437862 , -0.        ],\n                              [-0.        , -0.02161165, -0.07492917],\n                              [-0.        , -0.2668863 , -0.30240908]],\n                \n                             [[-0.7111022 , -0.49571267, -0.        ],\n                              [-0.20724443, -0.        , -0.04456911],\n                              [-0.        , -0.65824246, -0.6587337 ],\n                              ...,\n                              [-1.506151  , -1.7895727 , -0.        ],\n                              [-0.        , -0.02653998, -0.08760779],\n                              [-0.        , -0.29659012, -0.33980873]],\n                \n                             [[-0.76823443, -0.6745244 , -0.        ],\n                              [-0.17305663, -0.09348579, -0.        ],\n                              [-0.        , -0.27002108, -0.19312783],\n                              ...,\n                              [-1.4817543 , -1.7750046 , -0.        ],\n                              [-0.        , -0.0278604 , -0.09126941],\n                              [-0.        , -0.30286664, -0.34831122]]], dtype=float32)\n       tangent = Traced<ShapedArray(float32[600,50,3]):JaxprTrace(level=1/0)>.\n\nThis error can occur when a JAX Tracer object is passed to a raw numpy function, or a method on a numpy.ndarray object. You might want to check that you are using `jnp` together with `import jax.numpy as jnp` rather than using `np` via `import numpy as np`. If this error arises on a line that involves array indexing, like `x[idx]`, it may be that the array being indexed `x` is a raw numpy.ndarray while the indices `idx` are a JAX Tracer instance; in that case, you can instead write `jax.device_put(x)[idx]`.\n\nThe stack trace above excludes JAX-internal frames.\nThe following is the original exception that occurred, unmodified.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9d1579fba4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         sequences[nu_max]['beliefs'][1][-cutoff:])}\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m mcmc.run(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0m_rng_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mseqs_cut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/mcmc.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, rng_key, extra_fields, init_params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mmap_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_chains\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mstates_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/mcmc.py\u001b[0m in \u001b[0;36m_single_chain_mcmc\u001b[0;34m(self, init, args, kwargs, collect_fields)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             init_state = self.sampler.init(rng_key, self.num_warmup, init_params,\n\u001b[0m\u001b[1;32m    304\u001b[0m                                            model_args=args, model_kwargs=kwargs)\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/hmc.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, rng_key, num_warmup, init_params, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_key_init_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key_init_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_potential_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             raise ValueError('Valid value of `init_params` must be provided with'\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/hmc.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, rng_key, model_args, model_kwargs, init_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             init_params, potential_fn, postprocess_fn, model_trace = initialize_model(\n\u001b[0m\u001b[1;32m    408\u001b[0m                 \u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(rng_key, model, init_strategy, dynamic_args, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0minit_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_init_to_unconstrained_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munconstrained_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0mprototype_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstrained_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     (init_params, pe, grad), is_valid = find_valid_initial_params(rng_key, model,\n\u001b[0m\u001b[1;32m    441\u001b[0m                                                                   \u001b[0minit_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                                                                   \u001b[0menum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_enumerate_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mfind_valid_initial_params\u001b[0;34m(rng_key, model, init_strategy, enum, model_args, model_kwargs, prototype_params)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Handle possible vectorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrng_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_valid_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexit_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_find_valid_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36m_find_valid_params\u001b[0;34m(rng_key, exit_early)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Early return if valid params found. This is only helpful for single chain,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# where we can avoid compiling body_fn in while_loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnot_jax_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdevice_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mbody_fn\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mpotential_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mz_grad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel_pytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mis_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_grad_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreraise_with_filtered_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_under_reraiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_input_dtype_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdyn_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   1794\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m     \u001b[0mout_primal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_vjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_primal_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_primal_pval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJaxprTrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mpotential_energy\u001b[0;34m(model, model_args, model_kwargs, params, enum)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0msubstituted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubstitute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitute_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unconstrain_reparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# no param is needed for log_density computation because we already substitute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mlog_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_density_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstituted_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_joint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/infer/util.py\u001b[0m in \u001b[0;36mlog_density\u001b[0;34m(model, model_args, model_kwargs, params)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubstitute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mlog_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/handlers.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/primitives.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3aa8efa70d33>\u001b[0m in \u001b[0;36mmixture_model\u001b[0;34m(sequences, agents, mask, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# with npyro.handlers.seed(rng_seed=random.PRNGKey(0)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/numpyro/numpyro/distributions/distribution.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-975479d90fbe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logits, weights, mask, validate_args)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/miniconda/envs/numpyro/lib/python3.8/site-packages/jax/core.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m            \u001b[0;34m\"JAX Tracer instance; in that case, you can instead write \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m            \"`jax.device_put(x)[idx]`.\")\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[[-0.7033054  -0.47175476 -0.        ]\n  [-0.21759875 -0.         -0.05265158]\n  [-0.         -0.6987022  -0.65153885]\n  ...\n  [-1.8429785  -1.487565   -0.        ]\n  [-0.03948966 -0.         -0.09108281]\n  [-0.3597215  -0.         -0.3772351 ]]\n\n [[-0.76330316 -0.64973587 -0.        ]\n  [-0.1782569  -0.08730344 -0.        ]\n  [-0.         -0.2992027  -0.18282089]\n  ...\n  [-1.8419564  -1.4853681  -0.        ]\n  [-0.03961568 -0.         -0.09135436]\n  [-0.3605111  -0.         -0.3781346 ]]\n\n [[-0.731059   -0.5259844  -0.        ]\n  [-0.19031341 -0.         -0.01414236]\n  [-0.         -0.6161661  -0.52285767]\n  ...\n  [-1.99934    -1.8792112  -0.        ]\n  [-0.02140291 -0.         -0.04218084]\n  [-0.12390733 -0.         -0.16076677]]\n\n ...\n\n [[-0.7168098  -0.5062812  -0.        ]\n  [-0.20128258 -0.         -0.03573397]\n  [-0.         -0.65709496 -0.65724784]\n  ...\n  [-1.6025221  -1.8437862  -0.        ]\n  [-0.         -0.02161165 -0.07492917]\n  [-0.         -0.2668863  -0.30240908]]\n\n [[-0.7111022  -0.49571267 -0.        ]\n  [-0.20724443 -0.         -0.04456911]\n  [-0.         -0.65824246 -0.6587337 ]\n  ...\n  [-1.506151   -1.7895727  -0.        ]\n  [-0.         -0.02653998 -0.08760779]\n  [-0.         -0.29659012 -0.33980873]]\n\n [[-0.76823443 -0.6745244  -0.        ]\n  [-0.17305663 -0.09348579 -0.        ]\n  [-0.         -0.27002108 -0.19312783]\n  ...\n  [-1.4817543  -1.7750046  -0.        ]\n  [-0.         -0.0278604  -0.09126941]\n  [-0.         -0.30286664 -0.34831122]]])>with<JVPTrace(level=2/0)>\n  with primal = DeviceArray([[[-0.7033054 , -0.47175476, -0.        ],\n                              [-0.21759875, -0.        , -0.05265158],\n                              [-0.        , -0.6987022 , -0.65153885],\n                              ...,\n                              [-1.8429785 , -1.487565  , -0.        ],\n                              [-0.03948966, -0.        , -0.09108281],\n                              [-0.3597215 , -0.        , -0.3772351 ]],\n                \n                             [[-0.76330316, -0.64973587, -0.        ],\n                              [-0.1782569 , -0.08730344, -0.        ],\n                              [-0.        , -0.2992027 , -0.18282089],\n                              ...,\n                              [-1.8419564 , -1.4853681 , -0.        ],\n                              [-0.03961568, -0.        , -0.09135436],\n                              [-0.3605111 , -0.        , -0.3781346 ]],\n                \n                             [[-0.731059  , -0.5259844 , -0.        ],\n                              [-0.19031341, -0.        , -0.01414236],\n                              [-0.        , -0.6161661 , -0.52285767],\n                              ...,\n                              [-1.99934   , -1.8792112 , -0.        ],\n                              [-0.02140291, -0.        , -0.04218084],\n                              [-0.12390733, -0.        , -0.16076677]],\n                \n                             ...,\n                \n                             [[-0.7168098 , -0.5062812 , -0.        ],\n                              [-0.20128258, -0.        , -0.03573397],\n                              [-0.        , -0.65709496, -0.65724784],\n                              ...,\n                              [-1.6025221 , -1.8437862 , -0.        ],\n                              [-0.        , -0.02161165, -0.07492917],\n                              [-0.        , -0.2668863 , -0.30240908]],\n                \n                             [[-0.7111022 , -0.49571267, -0.        ],\n                              [-0.20724443, -0.        , -0.04456911],\n                              [-0.        , -0.65824246, -0.6587337 ],\n                              ...,\n                              [-1.506151  , -1.7895727 , -0.        ],\n                              [-0.        , -0.02653998, -0.08760779],\n                              [-0.        , -0.29659012, -0.33980873]],\n                \n                             [[-0.76823443, -0.6745244 , -0.        ],\n                              [-0.17305663, -0.09348579, -0.        ],\n                              [-0.        , -0.27002108, -0.19312783],\n                              ...,\n                              [-1.4817543 , -1.7750046 , -0.        ],\n                              [-0.        , -0.0278604 , -0.09126941],\n                              [-0.        , -0.30286664, -0.34831122]]], dtype=float32)\n       tangent = Traced<ShapedArray(float32[600,50,3]):JaxprTrace(level=1/0)>.\n\nThis error can occur when a JAX Tracer object is passed to a raw numpy function, or a method on a numpy.ndarray object. You might want to check that you are using `jnp` together with `import jax.numpy as jnp` rather than using `np` via `import numpy as np`. If this error arises on a line that involves array indexing, like `x[idx]`, it may be that the array being indexed `x` is a raw numpy.ndarray while the indices `idx` are a JAX Tracer instance; in that case, you can instead write `jax.device_put(x)[idx]`."
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(mixture_model)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=2000, num_samples=2000)\n",
    "\n",
    "rng_key = random.PRNGKey(234786348)\n",
    "cutoff = 600\n",
    "seqs_cut = {}\n",
    "for nu_max in sequences:\n",
    "    rng_key, _rng_key = random.split(rng_key)\n",
    "    seqs_cut[nu_max] = {'beliefs' :(sequences[nu_max]['beliefs'][0][-cutoff:],\n",
    "                        sequences[nu_max]['beliefs'][1][-cutoff:])}\n",
    "    \n",
    "mcmc.run(\n",
    "    _rng_key, \n",
    "    seqs_cut, \n",
    "    agents,\n",
    "    mask_data[-cutoff:],\n",
    "    y=responses_data[-cutoff:], \n",
    "    extra_fields=('potential_energy',)\n",
    ")\n",
    "\n",
    "pe = mcmc.get_extra_fields()['potential_energy']\n",
    "print('Expected log joint density: {:.2f}'.format(jnp.mean(-pe)))\n",
    "\n",
    "mcmc.print_summary()\n",
    "sample = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "sns.heatmap(sample['weights'].mean(0)[:27], ax=axes[0], cmap='inferno', vmin=.05, vmax=.2);\n",
    "sns.heatmap(sample['weights'].mean(0)[27:], ax=axes[1], cmap='inferno', vmin=.05, vmax=.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
